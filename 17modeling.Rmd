# Modeling {#modeling}

```{r message = FALSE}
library(tidyverse)
library(rpart)
```

## Modeling notes

Notes on/inspired by the Machine Learning with caret in R DataCamp course. I'd like to better understand machine learning, especially as it pertains to classification problems. Most discussions begin with regression examples, so these will also be addressed in order to build a better foundation. This is the general approach of James, Witten, Hastie and Tibshirani in An Introduction to Statistical Learning with Applications in R, too. 


A regression example from the course:

```{r regression-example, cached = TRUE}
library(caret)

model <- train(
  price~., 
  diamonds,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 10,
    verboseIter = TRUE
  )
)

# Print model to console
model
```

Note that **caret** handles the work of splitting test sets and calculating RMSE. 

Another example from the DataCamp course.

```{r lm-example-5fcv, cached = TRUE}
library(MASS)

# Fit lm model using 5-fold CV: model
model <- train(
  medv~., 
  Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 5,
    verboseIter = TRUE
  )
)

# Print model to console
model
```

Cross validation can *itself* be repeated. The following is a 5-fold cross validation repeated 5 times.

```{r lm-example-5x5fold-cv, cached = TRUE}
# Fit lm model using 5 x 5-fold CV: model
model <- train(
  medv ~ ., 
  Boston,
  method = "lm",
  trControl = trainControl(
    method = "repeatedcv", 
    number = 5,
    repeats = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

predict(model, Boston)
```

## Classification

### An **rpart** classification example.

```{r rpart-classification-example, cached = TRUE}
model <- rpart(
  Species ~ .,
  data = iris,
  method = "class"
)

predicted <- predict(model, iris, type = "class")
predicted
mean(predicted == iris$Species)
```

Study what's going on in **predict()**:

```{r}
?predict.rpart
```

## From the DataCamp course

```{r mlbench-sonar-example, cached = TRUE}
library(mlbench)
data("Sonar")

# First randomize the dataset.
rows <- sample(nrow(Sonar))
Sonar <- Sonar[rows,]

# The split it into testing and training sets. 
split <- round(nrow(Sonar)*0.60)
train <- Sonar[1:split,]
test <- Sonar[(split+1):nrow(Sonar),]

nrow(train)/nrow(Sonar)

model <- glm(
  Class ~ .,
  family = binomial(link = "logit"),
  train
)

p <- predict(model, test, type = "response")
summary(p)

p_class <- as.factor(ifelse(p > 0.1, "M", "R"))

table(p_class, test[["Class"]])

confusionMatrix(p_class, test[["Class"]])
```

Receiver operator curves add a new level of complexity and usefulness. According to Zach Mayer in DataCamp, 

 `trainControl()` in **caret** can use AUC (instead of accuracy), to tune the parameters of your models. The `twoClassSummary()` convenience function allows you to do this easily.

When using `twoClassSummary()`, be sure to always include the argument `classProbs = TRUE` or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.)

```{r glm-traincontrol-example, cached = TRUE}
# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)

# Train glm with custom trainControl: model
model<-train(Class~., data=Sonar, method="glm", trControl=myControl)

# Print model to console
model
```

## Random forest with **caret**

For this set, we'll use the white wine quality dataset from UC Irvine. 

```{r wine-ranger-example, cached = TRUE}
wine <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep = ";")

glimpse(wine)
# Note that quality is an integer, not a factor. **caret** seems to take this in stride. 

# Fit random forest: model
model <- train(
  quality~.,
  tuneLength = 1,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

plot(model)
```

Let's try this again with a longer `tuneLength`. This explores more models and potentially finds a better model.

```{r wine-tunelength-example, cached = TRUE}
model <- train(
  quality~.,
  tuneLength = 10,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

plot(model)
```

## Random forest model

Custom tuning of grids can be used in **caret** by using `tuneGrid()`. While it is the most flexible method for fitting **caret** models and allows complete control over how the model is fit, it requires significant knowledge of the model and dramatically increases run time. 

```{r tunegrid-example-ranger, cached = TRUE}
tuneGrid <- data.frame(
  .mtry = c(2, 3, 7),
  .splitrule = "variance",
  .min.node.size = 5
)

# Fit random forest: model
model <- train(
  quality ~ .,
  tuneGrid = tuneGrid,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

# Plot model
plot(model)
```


