# Modeling {#modeling}

```{r message = FALSE}
library(tidyverse)
library(rpart)
```

## Classification: what I'm really interested in

Four types of classification are common in ML. See <https://machinelearningmastery.com/types-of-classification-in-machine-learning/>

## Class imbalance: a critical issue in pituitary adenoma classification

Fundamental to modeling pituitary adenoma classification is the notion of **class imbalance**, which must be acknowledged in the modeling process, even if I choose to simply ignore it. 

Resources for class imbalance:

  * https://www.svds.com/learning-imbalanced-classes/
  * https://dpmartin42.github.io/posts/r/imbalanced-classes-part-1
  * https://rstudio-pubs-static.s3.amazonaws.com/607601_57a11284917f4d79933f4c4db3d41713.html

## Modeling notes

Notes on/inspired by the Machine Learning with caret in R DataCamp course. I'd like to better understand machine learning, especially as it pertains to classification problems. Most discussions begin with regression examples, so these will also be addressed in order to build a better foundation. This is the general approach of James, Witten, Hastie and Tibshirani in An Introduction to Statistical Learning with Applications in R, too. 

But let's back up and consider the whole rationale behind modeling: according to Haley Wickham in `R4DS`,

> The goal of a model is to provide a simple low-dimensional summary of a dataset.

Modeling is a mode of `supervised learning`, which can be divided into `classification` and `regression`. 

**Root mean squared error**, `RMSE`, is an important concept in regression problems. For a review of what is meant by RMSE, see this Wikipedia page: <https://en.wikipedia.org/wiki/Root-mean-square_deviation>. The units RMSE are the same as the original data, so it is very interpretable.

It's worth taking a more careful look at RMSE, and practicing with it, to illustrate the underlying methods of regression before we get into **caret**. 

```{r}
glimpse(diamonds)

# Fit lm model: model
model<-lm(price~., diamonds)

# Predict on full data: p
p<-predict(model, diamonds)

# Compute errors: error
error<-p-diamonds$price

# Calculate RMSE
sqrt(mean(error^2))
```

Note that the RMSE in this case is $`r round(sqrt(mean(error^2)), 2)`, which is in keeping with what we know about the price of diamonds, which range from $`r range(diamonds$price)[1]` to $`r range(diamonds$price)[2]` and average $`r round(mean(diamonds$price), 2)`.

Within-sample RMSE always overestimates model accuracy--the model only "knows" what it has encountered, not what the rest of the universe holds in store. Hence, out-of-sample data is checked with the model by some means. A separate "validation set" of sample points is provided (as we did when we moved from exploration to validation in our first pituitary adenoma paper). 

Zach Mayer states this another way: 

> In-sample validation almost guarantees overfitting.

So in the wide world and blue, this is perhaps the nicest intellectual defense of study abroad, of learning the perspectives, habits, and languages of others in order to avoid overfitting of our mental models to local norms. 

Sample size cannot always grow, however: samples are expensive. **caret** simulates the process of having a validation set and permits the progressive refinement of a model. 

## Out of sample error example

This example from the DataCamp course divides `diamonds` into test and training sets. Note the assumptions that get built into the process: the use of 80% train/20% test, for instance. How does one arrive at this figure?

```{r}
# Set seed
set.seed(42)

# Shuffle row indices in case the data set is inhomogeneous: rows
rows<-sample(nrow(diamonds))

# Randomly order data
shuffled_diamonds<-diamonds[rows,]

# Determine row to split on: split
split<-round(nrow(diamonds)*0.80)

# Create train
train<-diamonds[1:split,]

# Create test
test<-diamonds[(split+1):nrow(diamonds),]

# Fit lm model on train: model
model<-lm(price~., train)

# Predict on test: p
p<-predict(model, test)

# Compute errors: error
error <- p -  test$price

# Calculate RMSE
sqrt(mean(error^2))
```

So the RMSE for the model of diamond price, as measured by dividing the set this way, is $`r round(sqrt(mean(error^2)), 2)`. 

## A regression example from the course, this time using **caret**

Note that the `train()` function has the method characteristic that can choose the type of model and that the `trainControl()` function has a method that determines cross validation. The number characteristic refers to the number of folds of cross validation. 10-fold cross validation is common, but takes more time than the use of smaller numbers. 5-fold will be used here to improve the speed of the processing. 

```{r regression-example, cached = TRUE}
library(caret)

model <- train(
  price~., 
  diamonds,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 5,
    verboseIter = TRUE
  )
)

# Print model to console
model
```

Note that **caret** handles the work of splitting test sets and calculating RMSE. 

Another example from the DataCamp course.

```{r lm-example-5fcv, cached = TRUE}
library(MASS)

# Fit lm model using 5-fold CV: model
model <- train(
  medv~., 
  Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 5,
    verboseIter = TRUE
  )
)

# Print model to console
model
```

Cross validation can *itself* be repeated. The following is a 5-fold cross validation repeated 5 times.

```{r lm-example-5x5fold-cv, cached = TRUE}
# Fit lm model using 5 x 5-fold CV: model
model <- train(
  medv ~ ., 
  Boston,
  method = "lm",
  trControl = trainControl(
    method = "repeatedcv", 
    number = 5,
    repeats = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

p <- predict(model, Boston)

error <- p - Boston$medv

sqrt(mean(error^2))
```

## Classification

### An **rpart** classification example.

```{r rpart-classification-example, cached = TRUE}
model <- rpart(
  Species ~ .,
  data = iris,
  method = "class"
)

predicted <- predict(model, iris, type = "class")
head(predicted)
```

Study what's going on in **predict()**:

```{r}
?predict.rpart
```

So **predict()** uses the model to assign a predicted value to Species based on the rest of the iris dataset data. 

The accuracy of this prediction can be tested by comparing this vector to the original Species.

```{r}
mean(predicted == iris$Species)
```

## From the DataCamp course

```{r mlbench-sonar-example, cached = TRUE}
library(mlbench)
data("Sonar")

# First randomize the dataset.
rows <- sample(nrow(Sonar))
Sonar <- Sonar[rows,]

# The split it into testing and training sets. 
split <- round(nrow(Sonar)*0.60)
train <- Sonar[1:split,]
test <- Sonar[(split+1):nrow(Sonar),]

nrow(train)/nrow(Sonar)

model <- glm(
  Class ~ .,
  family = binomial(link = "logit"),
  train
)

p <- predict(model, test, type = "response")
summary(p)
plot(p)

p_class <- as.factor(ifelse(p > 0.1, "M", "R"))

table(p_class, test[["Class"]])

confusionMatrix(p_class, test[["Class"]])
```

According to Zach Mayer in DataCamp, manually evaluating classification threshholds is hard work and arbitrary: one would need to create dozens or hundreds of confusion matrices and then manually inspect them. Receiver operator curves add a new level of complexity and usefulness. 

To illustrate, we take the predicted probability of each `Class` for each case of the `test` set, and compare it with its actual `Class`:

```{r}
library(caTools)
colAUC(p, test[["Class"]], plotROC = TRUE)
```

 `trainControl()` in **caret** can use AUC (instead of accuracy), to tune the parameters of your models. The `twoClassSummary()` convenience function allows you to do this easily.

When using `twoClassSummary()`, be sure to always include the argument `classProbs = TRUE` or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.)

```{r glm-traincontrol-example, cached = TRUE}
# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)

# Train glm with custom trainControl: model
model<-train(Class~., data=Sonar, method="glm", trControl=myControl)

# Print model to console
model
```

So `twoClassSummary` to use AUC to tune the parameters for the model generates a much more accurate model than our random assignment for `p`. 

## Random forest with **caret**

For this set, we'll use the white wine quality data set from UC Irvine. 

```{r wine-ranger-example, cached = TRUE}
wine <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep = ";")

glimpse(wine)
# Note that quality is an integer, not a factor. **caret** seems to take this in stride. 

# Fit random forest: model
model <- train(
  quality~.,
  tuneLength = 1,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

plot(model)
```

Let's try this again with a longer `tuneLength`. This explores more models and potentially finds a better model.

```{r wine-tunelength-example, cached = TRUE}
model <- train(
  quality~.,
  tuneLength = 10,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

plot(model)
```

## Random forest model and `tuneGrid()`

Custom tuning of grids can be used in **caret** by using `tuneGrid()`. While it is the most flexible method for fitting **caret** models and allows complete control over how the model is fit, it requires significant knowledge of the model and dramatically increases run time. 

For my uses, most of the time I anticipate using `tuneLength` and the default settings in **caret** to build my random forest models. 

```{r tunegrid-example-ranger, cached = TRUE}
tuneGrid <- data.frame(
  .mtry = c(2, 3, 7),
  .splitrule = "variance",
  .min.node.size = 5
)

# Fit random forest: model
model <- train(
  quality ~ .,
  tuneGrid = tuneGrid,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

# Plot model
plot(model)
```


