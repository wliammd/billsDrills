[["codebreaksrmd.html", "Chapter 18 Penalty box 18.1 glmnet and patma", " Chapter 18 Penalty box 18.1 glmnet and patma I’d like to explore Zach Meyer’s teaching on glmnet models using my own data. That means dipping into patmanDx.csv and pulling out the relative data. patma &lt;- read_csv(&quot;data/patmanDx.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Dx = col_character(), ## SurgPathNo = col_character(), ## CAM5.2Pattern = col_character(), ## newDx = col_character(), ## finDx = col_character(), ## villaDx = col_character(), ## villaCode = col_character(), ## Note = col_character(), ## manDx = col_character(), ## OrigDx = col_character(), ## Sex = col_character(), ## ClinNotes = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. df &lt;- patma %&gt;% select(SF1Median, Pit1Median, TPITMedian, PRLMedian, GHMedian, TSHMedian, LHMedian, FSHMedian, ACTHMedian, ASUMedian, GATA3Median, manDx) %&gt;% na.omit() glimpse(df) ## Rows: 147 ## Columns: 12 ## $ SF1Median &lt;dbl&gt; 0.0, 8.0, 5.0, 8.0, 0.0, 8.0, 0.0, 8.0, 8.0, 0.0, 8.0, 8.0… ## $ Pit1Median &lt;dbl&gt; 8.0, 0.0, 0.0, 0.0, 8.0, 0.0, 8.0, 0.0, 0.0, 0.0, 3.0, 0.0… ## $ TPITMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0… ## $ PRLMedian &lt;dbl&gt; 8.0, 0.0, 0.0, 0.0, 8.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0… ## $ GHMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0… ## $ TSHMedian &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ LHMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0, 1.0, 0.0, 2.5, 0.0… ## $ FSHMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0, 3.5, 0.0, 2.5, 0.0… ## $ ACTHMedian &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0… ## $ ASUMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 6.5, 7.0, 0.0, 4.0, 0.0, 6.0, 0.0… ## $ GATA3Median &lt;dbl&gt; 0.0, 7.5, 6.0, 7.0, 0.0, 7.0, 0.0, 6.0, 7.0, 7.0, 7.0, 7.0… ## $ manDx &lt;chr&gt; &quot;PRL&quot;, &quot;GON&quot;, &quot;GON&quot;, &quot;GON&quot;, &quot;PRL&quot;, &quot;GON&quot;, &quot;PIT1&quot;, &quot;GON&quot;, &quot;… df &lt;- df %&gt;% mutate(manDx = case_when( manDx == &quot;NULL&quot; ~ &quot;Null&quot;, manDx %in% c(&quot;PLUR&quot;, &quot;UNK&quot;) ~ &quot;PlurUnk&quot;, TRUE ~ manDx )) glimpse(df) ## Rows: 147 ## Columns: 12 ## $ SF1Median &lt;dbl&gt; 0.0, 8.0, 5.0, 8.0, 0.0, 8.0, 0.0, 8.0, 8.0, 0.0, 8.0, 8.0… ## $ Pit1Median &lt;dbl&gt; 8.0, 0.0, 0.0, 0.0, 8.0, 0.0, 8.0, 0.0, 0.0, 0.0, 3.0, 0.0… ## $ TPITMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0… ## $ PRLMedian &lt;dbl&gt; 8.0, 0.0, 0.0, 0.0, 8.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0… ## $ GHMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0… ## $ TSHMedian &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ LHMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0, 1.0, 0.0, 2.5, 0.0… ## $ FSHMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0, 3.5, 0.0, 2.5, 0.0… ## $ ACTHMedian &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0… ## $ ASUMedian &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 6.5, 7.0, 0.0, 4.0, 0.0, 6.0, 0.0… ## $ GATA3Median &lt;dbl&gt; 0.0, 7.5, 6.0, 7.0, 0.0, 7.0, 0.0, 6.0, 7.0, 7.0, 7.0, 7.0… ## $ manDx &lt;chr&gt; &quot;PRL&quot;, &quot;GON&quot;, &quot;GON&quot;, &quot;GON&quot;, &quot;PRL&quot;, &quot;GON&quot;, &quot;PIT1&quot;, &quot;GON&quot;, &quot;… table(df$manDx) ## ## ACTH GH GON Null PIT1 PlurUnk PRL ## 23 16 77 10 4 2 15 That seems to give me what I’m looking for. Let’s get to work: myControl &lt;- trainControl( method = &quot;cv&quot;, number = 10, summaryFunction = defaultSummary, classProbs = TRUE, #Critical to have this, per Meyers. verboseIter = TRUE ) set.seed(42) model &lt;- train( manDx ~ ., df, method = &quot;glmnet&quot;, trControl = myControl ) ## + Fold01: alpha=0.10, lambda=0.08737 ## - Fold01: alpha=0.10, lambda=0.08737 ## + Fold01: alpha=0.55, lambda=0.08737 ## - Fold01: alpha=0.55, lambda=0.08737 ## + Fold01: alpha=1.00, lambda=0.08737 ## - Fold01: alpha=1.00, lambda=0.08737 ## + Fold02: alpha=0.10, lambda=0.08737 ## - Fold02: alpha=0.10, lambda=0.08737 ## + Fold02: alpha=0.55, lambda=0.08737 ## - Fold02: alpha=0.55, lambda=0.08737 ## + Fold02: alpha=1.00, lambda=0.08737 ## - Fold02: alpha=1.00, lambda=0.08737 ## + Fold03: alpha=0.10, lambda=0.08737 ## - Fold03: alpha=0.10, lambda=0.08737 ## + Fold03: alpha=0.55, lambda=0.08737 ## - Fold03: alpha=0.55, lambda=0.08737 ## + Fold03: alpha=1.00, lambda=0.08737 ## - Fold03: alpha=1.00, lambda=0.08737 ## + Fold04: alpha=0.10, lambda=0.08737 ## model fit failed for Fold04: alpha=0.10, lambda=0.08737 Error in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : ## one multinomial or binomial class has 1 or 0 observations; not allowed ## ## - Fold04: alpha=0.10, lambda=0.08737 ## + Fold04: alpha=0.55, lambda=0.08737 ## model fit failed for Fold04: alpha=0.55, lambda=0.08737 Error in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : ## one multinomial or binomial class has 1 or 0 observations; not allowed ## ## - Fold04: alpha=0.55, lambda=0.08737 ## + Fold04: alpha=1.00, lambda=0.08737 ## model fit failed for Fold04: alpha=1.00, lambda=0.08737 Error in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : ## one multinomial or binomial class has 1 or 0 observations; not allowed ## ## - Fold04: alpha=1.00, lambda=0.08737 ## + Fold05: alpha=0.10, lambda=0.08737 ## model fit failed for Fold05: alpha=0.10, lambda=0.08737 Error in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : ## one multinomial or binomial class has 1 or 0 observations; not allowed ## ## - Fold05: alpha=0.10, lambda=0.08737 ## + Fold05: alpha=0.55, lambda=0.08737 ## model fit failed for Fold05: alpha=0.55, lambda=0.08737 Error in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : ## one multinomial or binomial class has 1 or 0 observations; not allowed ## ## - Fold05: alpha=0.55, lambda=0.08737 ## + Fold05: alpha=1.00, lambda=0.08737 ## model fit failed for Fold05: alpha=1.00, lambda=0.08737 Error in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : ## one multinomial or binomial class has 1 or 0 observations; not allowed ## ## - Fold05: alpha=1.00, lambda=0.08737 ## + Fold06: alpha=0.10, lambda=0.08737 ## - Fold06: alpha=0.10, lambda=0.08737 ## + Fold06: alpha=0.55, lambda=0.08737 ## - Fold06: alpha=0.55, lambda=0.08737 ## + Fold06: alpha=1.00, lambda=0.08737 ## - Fold06: alpha=1.00, lambda=0.08737 ## + Fold07: alpha=0.10, lambda=0.08737 ## - Fold07: alpha=0.10, lambda=0.08737 ## + Fold07: alpha=0.55, lambda=0.08737 ## - Fold07: alpha=0.55, lambda=0.08737 ## + Fold07: alpha=1.00, lambda=0.08737 ## - Fold07: alpha=1.00, lambda=0.08737 ## + Fold08: alpha=0.10, lambda=0.08737 ## - Fold08: alpha=0.10, lambda=0.08737 ## + Fold08: alpha=0.55, lambda=0.08737 ## - Fold08: alpha=0.55, lambda=0.08737 ## + Fold08: alpha=1.00, lambda=0.08737 ## - Fold08: alpha=1.00, lambda=0.08737 ## + Fold09: alpha=0.10, lambda=0.08737 ## - Fold09: alpha=0.10, lambda=0.08737 ## + Fold09: alpha=0.55, lambda=0.08737 ## - Fold09: alpha=0.55, lambda=0.08737 ## + Fold09: alpha=1.00, lambda=0.08737 ## - Fold09: alpha=1.00, lambda=0.08737 ## + Fold10: alpha=0.10, lambda=0.08737 ## - Fold10: alpha=0.10, lambda=0.08737 ## + Fold10: alpha=0.55, lambda=0.08737 ## - Fold10: alpha=0.55, lambda=0.08737 ## + Fold10: alpha=1.00, lambda=0.08737 ## - Fold10: alpha=1.00, lambda=0.08737 ## Aggregating results ## Selecting tuning parameters ## Fitting alpha = 0.1, lambda = 0.0874 on full training set model ## glmnet ## ## 147 samples ## 11 predictor ## 7 classes: &#39;ACTH&#39;, &#39;GH&#39;, &#39;GON&#39;, &#39;Null&#39;, &#39;PIT1&#39;, &#39;PlurUnk&#39;, &#39;PRL&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 133, 132, 135, 131, 130, 133, ... ## Resampling results across tuning parameters: ## ## alpha Accuracy Kappa ## 0.10 0.9377976 0.8994043 ## 0.55 0.8787202 0.8041376 ## 1.00 0.8781994 0.8018680 ## ## Tuning parameter &#39;lambda&#39; was held constant at a value of 0.08736605 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 0.1 and lambda = 0.08736605. plot(model) "]]
