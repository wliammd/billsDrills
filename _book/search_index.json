[["modeling.html", "Chapter 17 Modeling 17.1 Modeling notes 17.2 A regression example from the course: 17.3 Classification 17.4 From the DataCamp course 17.5 Random forest with caret 17.6 Random forest model", " Chapter 17 Modeling library(tidyverse) library(rpart) 17.1 Modeling notes Notes on/inspired by the Machine Learning with caret in R DataCamp course. I’d like to better understand machine learning, especially as it pertains to classification problems. Most discussions begin with regression examples, so these will also be addressed in order to build a better foundation. This is the general approach of James, Witten, Hastie and Tibshirani in An Introduction to Statistical Learning with Applications in R, too. But let’s back up and consider the whole rationale behind modeling: according to Haley Wickham in R4DS, The goal of a model is to provide a simple low-dimensional summary of a dataset. Modeling is a mode of supervised learning, which can be divided into classification and regression. Root mean squared error, RMSE, is an important concept in regression problems. It’s worth taking a more careful look at RMSE, and practicing with it, to illustrate the underlying methods of regression before we get into caret. glimpse(diamonds) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… # Fit lm model: model model&lt;-lm(price~.,diamonds) # Predict on full data: p p&lt;-predict(model, diamonds) # Compute errors: error error&lt;-p-diamonds$price # Calculate RMSE sqrt(mean(error^2)) ## [1] 1129.843 17.2 A regression example from the course: library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift model &lt;- train( price~., diamonds, method = &quot;lm&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 10, verboseIter = TRUE ) ) ## + Fold01: intercept=TRUE ## - Fold01: intercept=TRUE ## + Fold02: intercept=TRUE ## - Fold02: intercept=TRUE ## + Fold03: intercept=TRUE ## - Fold03: intercept=TRUE ## + Fold04: intercept=TRUE ## - Fold04: intercept=TRUE ## + Fold05: intercept=TRUE ## - Fold05: intercept=TRUE ## + Fold06: intercept=TRUE ## - Fold06: intercept=TRUE ## + Fold07: intercept=TRUE ## - Fold07: intercept=TRUE ## + Fold08: intercept=TRUE ## - Fold08: intercept=TRUE ## + Fold09: intercept=TRUE ## - Fold09: intercept=TRUE ## + Fold10: intercept=TRUE ## - Fold10: intercept=TRUE ## Aggregating results ## Fitting final model on full training set # Print model to console model ## Linear Regression ## ## 53940 samples ## 9 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 48546, 48547, 48546, 48545, 48547, 48546, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1130.882 0.9196667 740.4941 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE Note that caret handles the work of splitting test sets and calculating RMSE. Another example from the DataCamp course. library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select # Fit lm model using 5-fold CV: model model &lt;- train( medv~., Boston, method = &quot;lm&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: intercept=TRUE ## - Fold1: intercept=TRUE ## + Fold2: intercept=TRUE ## - Fold2: intercept=TRUE ## + Fold3: intercept=TRUE ## - Fold3: intercept=TRUE ## + Fold4: intercept=TRUE ## - Fold4: intercept=TRUE ## + Fold5: intercept=TRUE ## - Fold5: intercept=TRUE ## Aggregating results ## Fitting final model on full training set # Print model to console model ## Linear Regression ## ## 506 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 404, 404, 406, 405, 405 ## Resampling results: ## ## RMSE Rsquared MAE ## 4.847382 0.7279821 3.398158 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE Cross validation can itself be repeated. The following is a 5-fold cross validation repeated 5 times. # Fit lm model using 5 x 5-fold CV: model model &lt;- train( medv ~ ., Boston, method = &quot;lm&quot;, trControl = trainControl( method = &quot;repeatedcv&quot;, number = 5, repeats = 5, verboseIter = TRUE ) ) ## + Fold1.Rep1: intercept=TRUE ## - Fold1.Rep1: intercept=TRUE ## + Fold2.Rep1: intercept=TRUE ## - Fold2.Rep1: intercept=TRUE ## + Fold3.Rep1: intercept=TRUE ## - Fold3.Rep1: intercept=TRUE ## + Fold4.Rep1: intercept=TRUE ## - Fold4.Rep1: intercept=TRUE ## + Fold5.Rep1: intercept=TRUE ## - Fold5.Rep1: intercept=TRUE ## + Fold1.Rep2: intercept=TRUE ## - Fold1.Rep2: intercept=TRUE ## + Fold2.Rep2: intercept=TRUE ## - Fold2.Rep2: intercept=TRUE ## + Fold3.Rep2: intercept=TRUE ## - Fold3.Rep2: intercept=TRUE ## + Fold4.Rep2: intercept=TRUE ## - Fold4.Rep2: intercept=TRUE ## + Fold5.Rep2: intercept=TRUE ## - Fold5.Rep2: intercept=TRUE ## + Fold1.Rep3: intercept=TRUE ## - Fold1.Rep3: intercept=TRUE ## + Fold2.Rep3: intercept=TRUE ## - Fold2.Rep3: intercept=TRUE ## + Fold3.Rep3: intercept=TRUE ## - Fold3.Rep3: intercept=TRUE ## + Fold4.Rep3: intercept=TRUE ## - Fold4.Rep3: intercept=TRUE ## + Fold5.Rep3: intercept=TRUE ## - Fold5.Rep3: intercept=TRUE ## + Fold1.Rep4: intercept=TRUE ## - Fold1.Rep4: intercept=TRUE ## + Fold2.Rep4: intercept=TRUE ## - Fold2.Rep4: intercept=TRUE ## + Fold3.Rep4: intercept=TRUE ## - Fold3.Rep4: intercept=TRUE ## + Fold4.Rep4: intercept=TRUE ## - Fold4.Rep4: intercept=TRUE ## + Fold5.Rep4: intercept=TRUE ## - Fold5.Rep4: intercept=TRUE ## + Fold1.Rep5: intercept=TRUE ## - Fold1.Rep5: intercept=TRUE ## + Fold2.Rep5: intercept=TRUE ## - Fold2.Rep5: intercept=TRUE ## + Fold3.Rep5: intercept=TRUE ## - Fold3.Rep5: intercept=TRUE ## + Fold4.Rep5: intercept=TRUE ## - Fold4.Rep5: intercept=TRUE ## + Fold5.Rep5: intercept=TRUE ## - Fold5.Rep5: intercept=TRUE ## Aggregating results ## Fitting final model on full training set # Print model to console model ## Linear Regression ## ## 506 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 404, 405, 406, 405, 404, 405, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 4.8234 0.7285809 3.386548 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE predict(model, Boston) ## 1 2 3 4 5 6 7 ## 30.0038434 25.0255624 30.5675967 28.6070365 27.9435242 25.2562845 23.0018083 ## 8 9 10 11 12 13 14 ## 19.5359884 11.5236369 18.9202621 18.9994965 21.5867957 20.9065215 19.5529028 ## 15 16 17 18 19 20 21 ## 19.2834821 19.2974832 20.5275098 16.9114013 16.1780111 18.4061360 12.5238575 ## 22 23 24 25 26 27 28 ## 17.6710367 15.8328813 13.8062853 15.6783383 13.3866856 15.4639765 14.7084743 ## 29 30 31 32 33 34 35 ## 19.5473729 20.8764282 11.4551176 18.0592329 8.8110574 14.2827581 13.7067589 ## 36 37 38 39 40 41 42 ## 23.8146353 22.3419371 23.1089114 22.9150261 31.3576257 34.2151023 28.0205641 ## 43 44 45 46 47 48 49 ## 25.2038663 24.6097927 22.9414918 22.0966982 20.4232003 18.0365509 9.1065538 ## 50 51 52 53 54 55 56 ## 17.2060775 21.2815254 23.9722228 27.6558508 24.0490181 15.3618477 31.1526495 ## 57 58 59 60 61 62 63 ## 24.8568698 33.1091981 21.7753799 21.0849356 17.8725804 18.5111021 23.9874286 ## 64 65 66 67 68 69 70 ## 22.5540887 23.3730864 30.3614836 25.5305651 21.1133856 17.4215379 20.7848363 ## 71 72 73 74 75 76 77 ## 25.2014886 21.7426577 24.5574496 24.0429571 25.5049972 23.9669302 22.9454540 ## 78 79 80 81 82 83 84 ## 23.3569982 21.2619827 22.4281737 28.4057697 26.9948609 26.0357630 25.0587348 ## 85 86 87 88 89 90 91 ## 24.7845667 27.7904920 22.1685342 25.8927642 30.6746183 30.8311062 27.1190194 ## 92 93 94 95 96 97 98 ## 27.4126673 28.9412276 29.0810555 27.0397736 28.6245995 24.7274498 35.7815952 ## 99 100 101 102 103 104 105 ## 35.1145459 32.2510280 24.5802202 25.5941347 19.7901368 20.3116713 21.4348259 ## 106 107 108 109 110 111 112 ## 18.5399401 17.1875599 20.7504903 22.6482911 19.7720367 20.6496586 26.5258674 ## 113 114 115 116 117 118 119 ## 20.7732364 20.7154831 25.1720888 20.4302559 23.3772463 23.6904326 20.3357836 ## 120 121 122 123 124 125 126 ## 20.7918087 21.9163207 22.4710778 20.5573856 16.3666198 20.5609982 22.4817845 ## 127 128 129 130 131 132 133 ## 14.6170663 15.1787668 18.9386859 14.0557329 20.0352740 19.4101340 20.0619157 ## 134 135 136 137 138 139 140 ## 15.7580767 13.2564524 17.2627773 15.8784188 19.3616395 13.8148390 16.4488147 ## 141 142 143 144 145 146 147 ## 13.5714193 3.9888551 14.5949548 12.1488148 8.7282236 12.0358534 15.8208206 ## 148 149 150 151 152 153 154 ## 8.5149902 9.7184414 14.8045137 20.8385815 18.3010117 20.1228256 17.2860189 ## 155 156 157 158 159 160 161 ## 22.3660023 20.1037592 13.6212589 33.2598270 29.0301727 25.5675277 32.7082767 ## 162 163 164 165 166 167 168 ## 36.7746701 40.5576584 41.8472817 24.7886738 25.3788924 37.2034745 23.0874875 ## 169 170 171 172 173 174 175 ## 26.4027396 26.6538211 22.5551466 24.2908281 22.9765722 29.0719431 26.5219434 ## 176 177 178 179 180 181 182 ## 30.7220906 25.6166931 29.1374098 31.4357197 32.9223157 34.7244046 27.7655211 ## 183 184 185 186 187 188 189 ## 33.8878732 30.9923804 22.7182001 24.7664781 35.8849723 33.4247672 32.4119915 ## 190 191 192 193 194 195 196 ## 34.5150995 30.7610949 30.2893414 32.9191871 32.1126077 31.5587100 40.8455572 ## 197 198 199 200 201 202 203 ## 36.1277008 32.6692081 34.7046912 30.0934516 30.6439391 29.2871950 37.0714839 ## 204 205 206 207 208 209 210 ## 42.0319312 43.1894984 22.6903480 23.6828471 17.8544721 23.4942899 17.0058772 ## 211 212 213 214 215 216 217 ## 22.3925110 17.0604275 22.7389292 25.2194255 11.1191674 24.5104915 26.6033477 ## 218 219 220 221 222 223 224 ## 28.3551871 24.9152546 29.6865277 33.1841975 23.7745666 32.1405196 29.7458199 ## 225 226 227 228 229 230 231 ## 38.3710245 39.8146187 37.5860575 32.3995325 35.4566524 31.2341151 24.4844923 ## 232 233 234 235 236 237 238 ## 33.2883729 38.0481048 37.1632863 31.7138352 25.2670557 30.1001074 32.7198716 ## 239 240 241 242 243 244 245 ## 28.4271706 28.4294068 27.2937594 23.7426248 24.1200789 27.4020841 16.3285756 ## 246 247 248 249 250 251 252 ## 13.3989126 20.0163878 19.8618443 21.2883131 24.0798915 24.2063355 25.0421582 ## 253 254 255 256 257 258 259 ## 24.9196401 29.9456337 23.9722832 21.6958089 37.5110924 43.3023904 36.4836142 ## 260 261 262 263 264 265 266 ## 34.9898859 34.8121151 37.1663133 40.9892850 34.4463409 35.8339755 28.2457430 ## 267 268 269 270 271 272 273 ## 31.2267359 40.8395575 39.3179239 25.7081791 22.3029553 27.2034097 28.5116947 ## 274 275 276 277 278 279 280 ## 35.4767660 36.1063916 33.7966827 35.6108586 34.8399338 30.3519266 35.3098070 ## 281 282 283 284 285 286 287 ## 38.7975697 34.3312319 40.3396307 44.6730834 31.5968909 27.3565923 20.1017415 ## 288 289 290 291 292 293 294 ## 27.0420667 27.2136458 26.9139584 33.4356331 34.4034963 31.8333982 25.8178324 ## 295 296 297 298 299 300 301 ## 24.4298235 28.4576434 27.3626700 19.5392876 29.1130984 31.9105461 30.7715945 ## 302 303 304 305 306 307 308 ## 28.9427587 28.8819102 32.7988723 33.2090546 30.7683179 35.5622686 32.7090512 ## 309 310 311 312 313 314 315 ## 28.6424424 23.5896583 18.5426690 26.8788984 23.2813398 25.5458025 25.4812006 ## 316 317 318 319 320 321 322 ## 20.5390990 17.6157257 18.3758169 24.2907028 21.3252904 24.8868224 24.8693728 ## 323 324 325 326 327 328 329 ## 22.8695245 19.4512379 25.1178340 24.6678691 23.6807618 19.3408962 21.1741811 ## 330 331 332 333 334 335 336 ## 24.2524907 21.5926089 19.9844661 23.3388800 22.1406069 21.5550993 20.6187291 ## 337 338 339 340 341 342 343 ## 20.1609718 19.2849039 22.1667232 21.2496577 21.4293931 30.3278880 22.0473498 ## 344 345 346 347 348 349 350 ## 27.7064791 28.5479412 16.5450112 14.7835964 25.2738008 27.5420512 22.1483756 ## 351 352 353 354 355 356 357 ## 20.4594409 20.5460542 16.8806383 25.4025351 14.3248663 16.5948846 19.6370469 ## 358 359 360 361 362 363 364 ## 22.7180661 22.2021889 19.2054806 22.6661611 18.9319262 18.2284680 20.2315081 ## 365 366 367 368 369 370 371 ## 37.4944739 14.2819073 15.5428625 10.8316232 23.8007290 32.6440736 34.6068404 ## 372 373 374 375 376 377 378 ## 24.9433133 25.9998091 6.1263250 0.7777981 25.3071306 17.7406106 20.2327441 ## 379 380 381 382 383 384 385 ## 15.8333130 16.8351259 14.3699483 18.4768283 13.4276828 13.0617751 3.2791812 ## 386 387 388 389 390 391 392 ## 8.0602217 6.1284220 5.6186481 6.4519857 14.2076474 17.2122518 17.2988727 ## 393 394 395 396 397 398 399 ## 9.8911664 20.2212419 17.9418118 20.3044578 19.2955908 16.3363278 6.5516232 ## 400 401 402 403 404 405 406 ## 10.8901678 11.8814587 17.8117451 18.2612659 12.9794878 7.3781636 8.2111586 ## 407 408 409 410 411 412 413 ## 8.0662619 19.9829479 13.7075637 19.8526845 15.2230830 16.9607198 1.7185181 ## 414 415 416 417 418 419 420 ## 11.8057839 -4.2813107 9.5837674 13.3666081 6.8956236 6.1477985 14.6066179 ## 421 422 423 424 425 426 427 ## 19.6000267 18.1242748 18.5217713 13.1752861 14.6261762 9.9237498 16.3459065 ## 428 429 430 431 432 433 434 ## 14.0751943 14.2575624 13.0423479 18.1595569 18.6955435 21.5272830 17.0314186 ## 435 436 437 438 439 440 441 ## 15.9609044 13.3614161 14.5207938 8.8197601 4.8675110 13.0659131 12.7060970 ## 442 443 444 445 446 447 448 ## 17.2955806 18.7404850 18.0590103 11.5147468 11.9740036 17.6834462 18.1269524 ## 449 450 451 452 453 454 455 ## 17.5183465 17.2274251 16.5227163 19.4129110 18.5821524 22.4894479 15.2800013 ## 456 457 458 459 460 461 462 ## 15.8208934 12.6872558 12.8763379 17.1866853 18.5124761 19.0486053 20.1720893 ## 463 464 465 466 467 468 469 ## 19.7740732 22.4294077 20.3191185 17.8861625 14.3747852 16.9477685 16.9840576 ## 470 471 472 473 474 475 476 ## 18.5883840 20.1671944 22.9771803 22.4558073 25.5782463 16.3914763 16.1114628 ## 477 478 479 480 481 482 483 ## 20.5348160 11.5427274 19.2049630 21.8627639 23.4687887 27.0988732 28.5699430 ## 484 485 486 487 488 489 490 ## 21.0839878 19.4551620 22.2222591 19.6559196 21.3253610 11.8558372 8.2238669 ## 491 492 493 494 495 496 497 ## 3.6639967 13.7590854 15.9311855 20.6266205 20.6124941 16.8854196 14.0132079 ## 498 499 500 501 502 503 504 ## 19.1085414 21.2980517 18.4549884 20.4687085 23.5333405 22.3757189 27.6274261 ## 505 506 ## 26.1279668 22.3442123 17.3 Classification 17.3.1 An rpart classification example. model &lt;- rpart( Species ~ ., data = iris, method = &quot;class&quot; ) predicted &lt;- predict(model, iris, type = &quot;class&quot;) predicted ## 1 2 3 4 5 6 7 ## setosa setosa setosa setosa setosa setosa setosa ## 8 9 10 11 12 13 14 ## setosa setosa setosa setosa setosa setosa setosa ## 15 16 17 18 19 20 21 ## setosa setosa setosa setosa setosa setosa setosa ## 22 23 24 25 26 27 28 ## setosa setosa setosa setosa setosa setosa setosa ## 29 30 31 32 33 34 35 ## setosa setosa setosa setosa setosa setosa setosa ## 36 37 38 39 40 41 42 ## setosa setosa setosa setosa setosa setosa setosa ## 43 44 45 46 47 48 49 ## setosa setosa setosa setosa setosa setosa setosa ## 50 51 52 53 54 55 56 ## setosa versicolor versicolor versicolor versicolor versicolor versicolor ## 57 58 59 60 61 62 63 ## versicolor versicolor versicolor versicolor versicolor versicolor versicolor ## 64 65 66 67 68 69 70 ## versicolor versicolor versicolor versicolor versicolor versicolor versicolor ## 71 72 73 74 75 76 77 ## virginica versicolor versicolor versicolor versicolor versicolor versicolor ## 78 79 80 81 82 83 84 ## versicolor versicolor versicolor versicolor versicolor versicolor versicolor ## 85 86 87 88 89 90 91 ## versicolor versicolor versicolor versicolor versicolor versicolor versicolor ## 92 93 94 95 96 97 98 ## versicolor versicolor versicolor versicolor versicolor versicolor versicolor ## 99 100 101 102 103 104 105 ## versicolor versicolor virginica virginica virginica virginica virginica ## 106 107 108 109 110 111 112 ## virginica versicolor virginica virginica virginica virginica virginica ## 113 114 115 116 117 118 119 ## virginica virginica virginica virginica virginica virginica virginica ## 120 121 122 123 124 125 126 ## versicolor virginica virginica virginica virginica virginica virginica ## 127 128 129 130 131 132 133 ## virginica virginica virginica versicolor virginica virginica virginica ## 134 135 136 137 138 139 140 ## versicolor versicolor virginica virginica virginica virginica virginica ## 141 142 143 144 145 146 147 ## virginica virginica virginica virginica virginica virginica virginica ## 148 149 150 ## virginica virginica virginica ## Levels: setosa versicolor virginica mean(predicted == iris$Species) ## [1] 0.96 Study what’s going on in predict(): ?predict.rpart 17.4 From the DataCamp course library(mlbench) data(&quot;Sonar&quot;) # First randomize the dataset. rows &lt;- sample(nrow(Sonar)) Sonar &lt;- Sonar[rows,] # The split it into testing and training sets. split &lt;- round(nrow(Sonar)*0.60) train &lt;- Sonar[1:split,] test &lt;- Sonar[(split+1):nrow(Sonar),] nrow(train)/nrow(Sonar) ## [1] 0.6009615 model &lt;- glm( Class ~ ., family = binomial(link = &quot;logit&quot;), train ) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred p &lt;- predict(model, test, type = &quot;response&quot;) summary(p) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00000 0.00000 0.06159 0.48052 1.00000 1.00000 p_class &lt;- as.factor(ifelse(p &gt; 0.1, &quot;M&quot;, &quot;R&quot;)) table(p_class, test[[&quot;Class&quot;]]) ## ## p_class M R ## M 18 23 ## R 29 13 confusionMatrix(p_class, test[[&quot;Class&quot;]]) ## Confusion Matrix and Statistics ## ## Reference ## Prediction M R ## M 18 23 ## R 29 13 ## ## Accuracy : 0.3735 ## 95% CI : (0.2697, 0.4866) ## No Information Rate : 0.5663 ## P-Value [Acc &gt; NIR] : 0.9999 ## ## Kappa : -0.251 ## ## Mcnemar&#39;s Test P-Value : 0.4881 ## ## Sensitivity : 0.3830 ## Specificity : 0.3611 ## Pos Pred Value : 0.4390 ## Neg Pred Value : 0.3095 ## Prevalence : 0.5663 ## Detection Rate : 0.2169 ## Detection Prevalence : 0.4940 ## Balanced Accuracy : 0.3720 ## ## &#39;Positive&#39; Class : M ## Receiver operator curves add a new level of complexity and usefulness. According to Zach Mayer in DataCamp, trainControl() in caret can use AUC (instead of accuracy), to tune the parameters of your models. The twoClassSummary() convenience function allows you to do this easily. When using twoClassSummary(), be sure to always include the argument classProbs = TRUE or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.) # Create trainControl object: myControl myControl &lt;- trainControl( method = &quot;cv&quot;, number = 10, summaryFunction = twoClassSummary, classProbs = TRUE, # IMPORTANT! verboseIter = TRUE ) # Train glm with custom trainControl: model model&lt;-train(Class~., data=Sonar, method=&quot;glm&quot;, trControl=myControl) ## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was not ## in the result set. ROC will be used instead. ## + Fold01: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold01: parameter=none ## + Fold02: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold02: parameter=none ## + Fold03: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold03: parameter=none ## + Fold04: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold04: parameter=none ## + Fold05: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold05: parameter=none ## + Fold06: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold06: parameter=none ## + Fold07: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold07: parameter=none ## + Fold08: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold08: parameter=none ## + Fold09: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold09: parameter=none ## + Fold10: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold10: parameter=none ## Aggregating results ## Fitting final model on full training set ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred # Print model to console model ## Generalized Linear Model ## ## 208 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 187, 187, 187, 187, 188, 187, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7647475 0.7825758 0.7444444 17.5 Random forest with caret For this set, we’ll use the white wine quality dataset from UC Irvine. wine &lt;- read.csv(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&quot;, sep = &quot;;&quot;) glimpse(wine) ## Rows: 4,898 ## Columns: 12 ## $ fixed.acidity &lt;dbl&gt; 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0, 6.3, 8.1,… ## $ volatile.acidity &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.32, 0.27, 0… ## $ citric.acid &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.16, 0.36, 0… ## $ residual.sugar &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.00, 20.70,… ## $ chlorides &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.058, 0.050, 0.045, … ## $ free.sulfur.dioxide &lt;dbl&gt; 45, 14, 30, 47, 47, 30, 30, 45, 14, 28, 11, 17, 1… ## $ total.sulfur.dioxide &lt;dbl&gt; 170, 132, 97, 186, 186, 97, 136, 170, 132, 129, 6… ## $ density &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9956, 0.9951, 0… ## $ pH &lt;dbl&gt; 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.18, 3.00, 3… ## $ sulphates &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.47, 0.45, 0… ## $ alcohol &lt;dbl&gt; 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.8, 9.5, 11… ## $ quality &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6… # Note that quality is an integer, not a factor. **caret** seems to take this in stride. # Fit random forest: model model &lt;- train( quality~., tuneLength = 1, data = wine, method = &quot;ranger&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: mtry=3, min.node.size=5, splitrule=variance ## - Fold1: mtry=3, min.node.size=5, splitrule=variance ## + Fold1: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold1: mtry=3, min.node.size=5, splitrule=extratrees ## + Fold2: mtry=3, min.node.size=5, splitrule=variance ## - Fold2: mtry=3, min.node.size=5, splitrule=variance ## + Fold2: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold2: mtry=3, min.node.size=5, splitrule=extratrees ## + Fold3: mtry=3, min.node.size=5, splitrule=variance ## - Fold3: mtry=3, min.node.size=5, splitrule=variance ## + Fold3: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold3: mtry=3, min.node.size=5, splitrule=extratrees ## + Fold4: mtry=3, min.node.size=5, splitrule=variance ## - Fold4: mtry=3, min.node.size=5, splitrule=variance ## + Fold4: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold4: mtry=3, min.node.size=5, splitrule=extratrees ## + Fold5: mtry=3, min.node.size=5, splitrule=variance ## - Fold5: mtry=3, min.node.size=5, splitrule=variance ## + Fold5: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold5: mtry=3, min.node.size=5, splitrule=extratrees ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 3, splitrule = variance, min.node.size = 5 on full training set # Print model to console model ## Random Forest ## ## 4898 samples ## 11 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3919, 3918, 3918, 3919, 3918 ## Resampling results across tuning parameters: ## ## splitrule RMSE Rsquared MAE ## variance 0.6096474 0.5346825 0.4423644 ## extratrees 0.6199581 0.5292561 0.4607174 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 3 ## Tuning ## parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 3, splitrule = variance ## and min.node.size = 5. plot(model) Let’s try this again with a longer tuneLength. This explores more models and potentially finds a better model. model &lt;- train( quality~., tuneLength = 10, data = wine, method = &quot;ranger&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: mtry= 2, min.node.size=5, splitrule=variance ## - Fold1: mtry= 2, min.node.size=5, splitrule=variance ## + Fold1: mtry= 3, min.node.size=5, splitrule=variance ## - Fold1: mtry= 3, min.node.size=5, splitrule=variance ## + Fold1: mtry= 4, min.node.size=5, splitrule=variance ## - Fold1: mtry= 4, min.node.size=5, splitrule=variance ## + Fold1: mtry= 5, min.node.size=5, splitrule=variance ## - Fold1: mtry= 5, min.node.size=5, splitrule=variance ## + Fold1: mtry= 6, min.node.size=5, splitrule=variance ## - Fold1: mtry= 6, min.node.size=5, splitrule=variance ## + Fold1: mtry= 7, min.node.size=5, splitrule=variance ## - Fold1: mtry= 7, min.node.size=5, splitrule=variance ## + Fold1: mtry= 8, min.node.size=5, splitrule=variance ## - Fold1: mtry= 8, min.node.size=5, splitrule=variance ## + Fold1: mtry= 9, min.node.size=5, splitrule=variance ## - Fold1: mtry= 9, min.node.size=5, splitrule=variance ## + Fold1: mtry=10, min.node.size=5, splitrule=variance ## - Fold1: mtry=10, min.node.size=5, splitrule=variance ## + Fold1: mtry=11, min.node.size=5, splitrule=variance ## - Fold1: mtry=11, min.node.size=5, splitrule=variance ## + Fold1: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold1: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold1: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold1: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold1: mtry=11, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 2, min.node.size=5, splitrule=variance ## - Fold2: mtry= 2, min.node.size=5, splitrule=variance ## + Fold2: mtry= 3, min.node.size=5, splitrule=variance ## - Fold2: mtry= 3, min.node.size=5, splitrule=variance ## + Fold2: mtry= 4, min.node.size=5, splitrule=variance ## - Fold2: mtry= 4, min.node.size=5, splitrule=variance ## + Fold2: mtry= 5, min.node.size=5, splitrule=variance ## - Fold2: mtry= 5, min.node.size=5, splitrule=variance ## + Fold2: mtry= 6, min.node.size=5, splitrule=variance ## - Fold2: mtry= 6, min.node.size=5, splitrule=variance ## + Fold2: mtry= 7, min.node.size=5, splitrule=variance ## - Fold2: mtry= 7, min.node.size=5, splitrule=variance ## + Fold2: mtry= 8, min.node.size=5, splitrule=variance ## - Fold2: mtry= 8, min.node.size=5, splitrule=variance ## + Fold2: mtry= 9, min.node.size=5, splitrule=variance ## - Fold2: mtry= 9, min.node.size=5, splitrule=variance ## + Fold2: mtry=10, min.node.size=5, splitrule=variance ## - Fold2: mtry=10, min.node.size=5, splitrule=variance ## + Fold2: mtry=11, min.node.size=5, splitrule=variance ## - Fold2: mtry=11, min.node.size=5, splitrule=variance ## + Fold2: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold2: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold2: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold2: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold2: mtry=11, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 2, min.node.size=5, splitrule=variance ## - Fold3: mtry= 2, min.node.size=5, splitrule=variance ## + Fold3: mtry= 3, min.node.size=5, splitrule=variance ## - Fold3: mtry= 3, min.node.size=5, splitrule=variance ## + Fold3: mtry= 4, min.node.size=5, splitrule=variance ## - Fold3: mtry= 4, min.node.size=5, splitrule=variance ## + Fold3: mtry= 5, min.node.size=5, splitrule=variance ## - Fold3: mtry= 5, min.node.size=5, splitrule=variance ## + Fold3: mtry= 6, min.node.size=5, splitrule=variance ## - Fold3: mtry= 6, min.node.size=5, splitrule=variance ## + Fold3: mtry= 7, min.node.size=5, splitrule=variance ## - Fold3: mtry= 7, min.node.size=5, splitrule=variance ## + Fold3: mtry= 8, min.node.size=5, splitrule=variance ## - Fold3: mtry= 8, min.node.size=5, splitrule=variance ## + Fold3: mtry= 9, min.node.size=5, splitrule=variance ## - Fold3: mtry= 9, min.node.size=5, splitrule=variance ## + Fold3: mtry=10, min.node.size=5, splitrule=variance ## - Fold3: mtry=10, min.node.size=5, splitrule=variance ## + Fold3: mtry=11, min.node.size=5, splitrule=variance ## - Fold3: mtry=11, min.node.size=5, splitrule=variance ## + Fold3: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold3: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold3: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold3: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold3: mtry=11, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 2, min.node.size=5, splitrule=variance ## - Fold4: mtry= 2, min.node.size=5, splitrule=variance ## + Fold4: mtry= 3, min.node.size=5, splitrule=variance ## - Fold4: mtry= 3, min.node.size=5, splitrule=variance ## + Fold4: mtry= 4, min.node.size=5, splitrule=variance ## - Fold4: mtry= 4, min.node.size=5, splitrule=variance ## + Fold4: mtry= 5, min.node.size=5, splitrule=variance ## - Fold4: mtry= 5, min.node.size=5, splitrule=variance ## + Fold4: mtry= 6, min.node.size=5, splitrule=variance ## - Fold4: mtry= 6, min.node.size=5, splitrule=variance ## + Fold4: mtry= 7, min.node.size=5, splitrule=variance ## - Fold4: mtry= 7, min.node.size=5, splitrule=variance ## + Fold4: mtry= 8, min.node.size=5, splitrule=variance ## - Fold4: mtry= 8, min.node.size=5, splitrule=variance ## + Fold4: mtry= 9, min.node.size=5, splitrule=variance ## - Fold4: mtry= 9, min.node.size=5, splitrule=variance ## + Fold4: mtry=10, min.node.size=5, splitrule=variance ## - Fold4: mtry=10, min.node.size=5, splitrule=variance ## + Fold4: mtry=11, min.node.size=5, splitrule=variance ## - Fold4: mtry=11, min.node.size=5, splitrule=variance ## + Fold4: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold4: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold4: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold4: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold4: mtry=11, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 2, min.node.size=5, splitrule=variance ## - Fold5: mtry= 2, min.node.size=5, splitrule=variance ## + Fold5: mtry= 3, min.node.size=5, splitrule=variance ## - Fold5: mtry= 3, min.node.size=5, splitrule=variance ## + Fold5: mtry= 4, min.node.size=5, splitrule=variance ## - Fold5: mtry= 4, min.node.size=5, splitrule=variance ## + Fold5: mtry= 5, min.node.size=5, splitrule=variance ## - Fold5: mtry= 5, min.node.size=5, splitrule=variance ## + Fold5: mtry= 6, min.node.size=5, splitrule=variance ## - Fold5: mtry= 6, min.node.size=5, splitrule=variance ## + Fold5: mtry= 7, min.node.size=5, splitrule=variance ## - Fold5: mtry= 7, min.node.size=5, splitrule=variance ## + Fold5: mtry= 8, min.node.size=5, splitrule=variance ## - Fold5: mtry= 8, min.node.size=5, splitrule=variance ## + Fold5: mtry= 9, min.node.size=5, splitrule=variance ## - Fold5: mtry= 9, min.node.size=5, splitrule=variance ## + Fold5: mtry=10, min.node.size=5, splitrule=variance ## - Fold5: mtry=10, min.node.size=5, splitrule=variance ## + Fold5: mtry=11, min.node.size=5, splitrule=variance ## - Fold5: mtry=11, min.node.size=5, splitrule=variance ## + Fold5: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold5: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold5: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold5: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold5: mtry=11, min.node.size=5, splitrule=extratrees ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 4, splitrule = variance, min.node.size = 5 on full training set # Print model to console model ## Random Forest ## ## 4898 samples ## 11 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3918, 3918, 3919, 3919, 3918 ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 0.6026945 0.5487951 0.4379427 ## 2 extratrees 0.6199347 0.5365639 0.4631564 ## 3 variance 0.6006600 0.5486864 0.4345399 ## 3 extratrees 0.6122306 0.5422335 0.4539915 ## 4 variance 0.6004502 0.5470348 0.4332787 ## 4 extratrees 0.6089778 0.5433863 0.4502413 ## 5 variance 0.6012945 0.5446996 0.4334448 ## 5 extratrees 0.6071504 0.5442645 0.4470516 ## 6 variance 0.6011134 0.5443868 0.4321485 ## 6 extratrees 0.6053141 0.5452688 0.4438773 ## 7 variance 0.6019421 0.5424081 0.4330856 ## 7 extratrees 0.6050430 0.5444642 0.4425996 ## 8 variance 0.6027510 0.5405530 0.4335117 ## 8 extratrees 0.6056861 0.5421808 0.4421095 ## 9 variance 0.6027381 0.5403465 0.4335727 ## 9 extratrees 0.6056653 0.5415703 0.4420213 ## 10 variance 0.6053699 0.5356645 0.4348873 ## 10 extratrees 0.6057595 0.5407165 0.4409643 ## 11 variance 0.6044200 0.5371265 0.4348973 ## 11 extratrees 0.6054799 0.5405573 0.4409718 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 4, splitrule = variance ## and min.node.size = 5. plot(model) 17.6 Random forest model Custom tuning of grids can be used in caret by using tuneGrid(). While it is the most flexible method for fitting caret models and allows complete control over how the model is fit, it requires significant knowledge of the model and dramatically increases run time. tuneGrid &lt;- data.frame( .mtry = c(2, 3, 7), .splitrule = &quot;variance&quot;, .min.node.size = 5 ) # Fit random forest: model model &lt;- train( quality ~ ., tuneGrid = tuneGrid, data = wine, method = &quot;ranger&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: mtry=2, splitrule=variance, min.node.size=5 ## - Fold1: mtry=2, splitrule=variance, min.node.size=5 ## + Fold1: mtry=3, splitrule=variance, min.node.size=5 ## - Fold1: mtry=3, splitrule=variance, min.node.size=5 ## + Fold1: mtry=7, splitrule=variance, min.node.size=5 ## - Fold1: mtry=7, splitrule=variance, min.node.size=5 ## + Fold2: mtry=2, splitrule=variance, min.node.size=5 ## - Fold2: mtry=2, splitrule=variance, min.node.size=5 ## + Fold2: mtry=3, splitrule=variance, min.node.size=5 ## - Fold2: mtry=3, splitrule=variance, min.node.size=5 ## + Fold2: mtry=7, splitrule=variance, min.node.size=5 ## - Fold2: mtry=7, splitrule=variance, min.node.size=5 ## + Fold3: mtry=2, splitrule=variance, min.node.size=5 ## - Fold3: mtry=2, splitrule=variance, min.node.size=5 ## + Fold3: mtry=3, splitrule=variance, min.node.size=5 ## - Fold3: mtry=3, splitrule=variance, min.node.size=5 ## + Fold3: mtry=7, splitrule=variance, min.node.size=5 ## - Fold3: mtry=7, splitrule=variance, min.node.size=5 ## + Fold4: mtry=2, splitrule=variance, min.node.size=5 ## - Fold4: mtry=2, splitrule=variance, min.node.size=5 ## + Fold4: mtry=3, splitrule=variance, min.node.size=5 ## - Fold4: mtry=3, splitrule=variance, min.node.size=5 ## + Fold4: mtry=7, splitrule=variance, min.node.size=5 ## - Fold4: mtry=7, splitrule=variance, min.node.size=5 ## + Fold5: mtry=2, splitrule=variance, min.node.size=5 ## - Fold5: mtry=2, splitrule=variance, min.node.size=5 ## + Fold5: mtry=3, splitrule=variance, min.node.size=5 ## - Fold5: mtry=3, splitrule=variance, min.node.size=5 ## + Fold5: mtry=7, splitrule=variance, min.node.size=5 ## - Fold5: mtry=7, splitrule=variance, min.node.size=5 ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 3, splitrule = variance, min.node.size = 5 on full training set # Print model to console model ## Random Forest ## ## 4898 samples ## 11 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3918, 3918, 3919, 3919, 3918 ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 0.6103536 0.5354755 0.4422691 ## 3 0.6093128 0.5341585 0.4391198 ## 7 0.6122607 0.5255398 0.4391880 ## ## Tuning parameter &#39;splitrule&#39; was held constant at a value of variance ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 3, splitrule = variance ## and min.node.size = 5. # Plot model plot(model) "]]
