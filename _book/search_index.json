[["index.html", "Bill’s Drills Book Chapter 1 Drills: Part of Every Healthy Intellectual Diet", " Bill’s Drills Book William McDonald Last edited 2021-10-02 Chapter 1 Drills: Part of Every Healthy Intellectual Diet The goal of this book is to organize my R drills into reasonable chunks, the better to understand my strengths and weaknesses, and to plan new forays into data science. "],["bookdownplan.html", "Chapter 2 bookdown Tips for This Document 2.1 Basic conventions 2.2 Inserting pictures 2.3 Referencing other parts of the document 2.4 Referencing citations: 2.5 Figures. 2.6 Working with Github", " Chapter 2 bookdown Tips for This Document 2.1 Basic conventions I am deeply indepted to Yihui Xie and his tireless efforts to improve open source publishing via bookdown. The basic structure of this book is taken directly from Xie’s bookdown-demo (1). The _bookdown.yml file contains a snippet that is important to inserting the word “Chapter” before the chapter number in each of the Rmd files. _output.yml is modified from that used by Xie in his bookdown-demo (1); it evokes style.css, toc.css, preamble.tex, which are also borrowed from Xie. Packages are indicated in bold, like dplyr Programs, like RStudio or MS Word, are in regular typeface Inline code, functions, and file names are indicated in typewriter face using backticks, like _bookdown.yml. Functions are generally written with parentheses, like mean(). Chapters are set in order by using adding 01, 02, 03, … before the name of their Rmd, like 01chpter.Rmd. Note that they can have short descriptive phrases, since the actual chapter titles are determined by the hashtag. index.Rmd always comes first in the book build, and contains the yml front matter. The most frequently used keyboard shortcuts from RStudio include: Option-Hyphen (the assignment operator) Shift-Control-c (comment code hashtag) Option-Command-i (code chunk) Shift-Control-m (pipe) When building PDF’s with bookdown, the following will prevent source code from running off the page: library(knitr) opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE) 2.2 Inserting pictures Pictures can be included in the pathologyImages subdirectory and referenced with a descriptive title in brackets followed by the file path in parentheses, like this: ![Pithy title. Figure A. Figure B](filepath/image_name.jpg) Note that I had to use double backticks around the above code in order to display the “Pithy title” figure reference. Here’s an example with a real image (code hidden in the final document): Tpit immunohistochemical stain. Figure A silent corticotroph. Figure B gonadotroph Some of the subdirectories throw an error in building the book, so I settled on _bookdown_files/pathologyImages as the location. Also, note that the build does not generate the caption unless the reference is on it’s own line. Also note that some controls on image size are available. For instance, the same image can be displayed at 50% size using {} after the reference, as follows: ![Tpit immunohistochemical stain. Figure A silent corticotroph. Figure B gonadotroph](pathologyImages/TpitIHC.jpg){#id.class width=50% height=50%} Tpit immunohistochemical stain. Figure A silent corticotroph. Figure B gonadotroph 2.3 Referencing other parts of the document Say that I want to refer the reader to a figure. I accomplish this with by inserting a reference with a snippet like this: \\@ref(fig:example_figure-1) Now for a real reference. See Figure 2.1. I can reference other pages in a similar fashion, for instance, \\@ref(bookdownplan). Note that this works by referencing a {#label} placed in the chapter title. See Chapter 1 See Chapter 2. Note that the {#label} uses a single run-together word. It does not tolerate spaces and this cannot be overcome by ‘quoting’ it. 2.4 Referencing citations: In order to insert citations, one needs a .bib file in the project. I’ve included one in this project as book.bib and another for packages as packages.bib. The yml header in Chapter 1 needs to have a \\(bibliography:\\). I use EndNote extensively in my writing and research. I enjoy the convenience of EndNote in MS Word, so I populate the book.bib and packages.bib files carefully, with .txt files generated in EndNote. A dump of my EndNote library is in bookFromEndnote.txt. This can be opened in RStudio, and I can copy-and-paste references from the .txt file to my book.bib. Alternately, individual references can be copied as BibTex Export in the EndNote summary interface. These can be added to the book.bib file. Then, for instance, if I have a breast pathology paper that I want to cite here (2), I’d copy-and-paste the reference from bookFromEndnote.txt to book.bib. Generally speaking, I maintain all references in EndNote, and manually generate a .txt file to update my references. Of note, Yihui Xie includes a nifty bit of code to automatically generate a bib database for all the R packages that I’d like to cite: knitr::write_bib(c(.packages(), &#39;bookdown&#39;, &#39;knitr&#39;, &#39;rmarkdown&#39;, &#39;tidyverse&#39;, &#39;caret&#39;, &#39;citr&#39;), &#39;packages.bib&#39;) All packages are maintained in packages.bib. bookdown, for instance, is cited thusly [@R-bookdown] (1). If I’m not using citr, then individual references can be strung together with semicolons (1,3,4). Reference styles are altered by dropping a line in the yml header, for example, csl: vancouver.csl. Some nice details can be found at https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html. Different styles can be found at https://www.zotero.org/styles. In the knitted document, references appear automatically at the end of a chapter the document. 2.5 Figures. tidverse and gglplot2 are used extensively in this book. The most powerful arguments that can be brought to these analyses are made with graphs and tables. library(tidyverse) Now to generate a figure. The code chunk contains the following: {r starfig-1, fig.cap='Starwars Figure 1'}. This labels the figure with a short name, starfig-1, and gives it the caption Starwars Figure 1. starwars %&gt;% filter(!is.na(species)) %&gt;% mutate(species = fct_lump(species, 5)) %&gt;% mutate(species = species %&gt;% fct_infreq() %&gt;% fct_rev()) %&gt;% ggplot() + geom_bar(aes(species, fill = gender)) + labs( title = &quot;Nifty Starwars Figure&quot;, x = &quot;Species&quot;, y = &quot;Count&quot; ) + coord_flip() Figure 2.1: Starwars Figure 1 2.6 Working with Github Document control is important. Using the terminal, one can maintain a Github repository for a project. Recent changes at require the use of a personal access token rather than a password in order to update the repo from the terminal. Instructions can be found at https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token. The personal access token replaces the password in the Apple keychain. Following the above link explains this, and offers a further link to illustrate the process of changing the keychain. References "],["datashape.html", "Chapter 3 Shape of Data 3.1 Making data frames 3.2 Gather, spread, pivoting in the tidyverse 3.3 Gathering steam… 3.4 Spread your wings 3.5 Missing Data: Ich vemisse Dich! 3.6 Pivoting to something new 3.7 Larger structures", " Chapter 3 Shape of Data One of my fundamental stumbling blocks is understanding the shape of data, even “rectangular data” of the sort that Hadley Wickham refers to in R4DS. library(tidyverse) library(lubridate) 3.1 Making data frames DF1 &lt;- tibble( Symbol = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), Value = 1:3 ) DF1 ## # A tibble: 3 × 2 ## Symbol Value ## &lt;chr&gt; &lt;int&gt; ## 1 a 1 ## 2 b 2 ## 3 c 3 DF2 &lt;- tibble( Symbol = c(&quot;c&quot;, &quot;d&quot;, &quot;e&quot;), Value = 4:6 ) DF2 ## # A tibble: 3 × 2 ## Symbol Value ## &lt;chr&gt; &lt;int&gt; ## 1 c 4 ## 2 d 5 ## 3 e 6 DFmerge &lt;- rbind(DF1,DF2) DFmerge ## # A tibble: 6 × 2 ## Symbol Value ## &lt;chr&gt; &lt;int&gt; ## 1 a 1 ## 2 b 2 ## 3 c 3 ## 4 c 4 ## 5 d 5 ## 6 e 6 3.2 Gather, spread, pivoting in the tidyverse The following exercises are based on R4DS. Several simple data tables are available for practice. Check them out: Note that only table 1 is tidy. Tidy rules the day, making cool things possible. For instance: table1 ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 table1 %&gt;% count(year, wt = cases) ## # A tibble: 2 × 2 ## year n ## &lt;int&gt; &lt;int&gt; ## 1 1999 250740 ## 2 2000 296920 table1 %&gt;% count(year, wt = NULL) ## # A tibble: 2 × 2 ## year n ## &lt;int&gt; &lt;int&gt; ## 1 1999 3 ## 2 2000 3 # BTW, note the difference between the above count and this one: table1 %&gt;% group_by(year) %&gt;% count(cases) ## # A tibble: 6 × 3 ## # Groups: year [2] ## year cases n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1999 745 1 ## 2 1999 37737 1 ## 3 1999 212258 1 ## 4 2000 2666 1 ## 5 2000 80488 1 ## 6 2000 213766 1 # Or the following group and summarize: table1 %&gt;% group_by(year) %&gt;% summarise(AggCases = sum(cases)) ## # A tibble: 2 × 2 ## year AggCases ## &lt;int&gt; &lt;int&gt; ## 1 1999 250740 ## 2 2000 296920 # Or the following plot (I couldn&#39;t help but embellish this with a fct_reorder--vital to ggplot/geom_line displays,but a trick that I always forget): table1 %&gt;% mutate(country = fct_reorder(country, -population), year = ymd(year, truncated = 2)) %&gt;% ggplot(aes(year, cases)) + geom_line(aes(group = country), colour = &quot;grey50&quot;) + geom_point(aes(colour = country)) + scale_x_date(labels = scales::date_format(&quot;%Y&quot;)) An aside on count() and tally(): tally() is a convenient wrapper for summarise that will either call n() or sum(n) depending on whether you’re tallying for the first time, or re-tallying. count() is similar but calls group_by() before and ungroup() after. If the data is already grouped, count() adds an additional group that is removed afterwards. Tidy data is not space-efficient, though. Note how values are reduplicated. This is not an issue in small datasets, but can be a bummer if datasets get large or human data entry is necessary–manually entering demographics is always error-prone. Table 2 on the other hand contains 2 sorts of data in the count column–cases and population. Not only is this less efficient than table 1, it is much more verbose. table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Table 3 contains unnecessary internal structure. I combines two values into one field. Kudos for transparency, but little is actually gained by doing this, and it complicates analyses. Best to break them apart. table3 ## # A tibble: 6 × 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 Tables 4a and 4b break data into separate structures when a single table would suffice. Many database tables will be arranged like this. However, to perform calculations on data from each table, it can be better to weld the tables into a single structure. table4a ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4b ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 3.3 Gathering steam… Table 4a has a key field–the year–and a value field–cases. To bring the key down into the table, we gather it: table4a ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 tidy4a &lt;- table4a %&gt;% gather(`1999`, `2000`, key = &quot;year&quot;, value = &quot;cases&quot;) tidy4a ## # A tibble: 6 × 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Brazil 1999 37737 ## 3 China 1999 212258 ## 4 Afghanistan 2000 2666 ## 5 Brazil 2000 80488 ## 6 China 2000 213766 Note that we replace column names that should be data fields with a key (in this example we call it “year”), and the corresponding data held in table4a with a value (in this example “cases”). The same thing can be done with 4b, except now the value is population. table4b ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 tidy4b &lt;- table4b %&gt;% gather(`1999`, `2000`, key = &quot;year&quot;, value = &quot;population&quot;) tidy4b ## # A tibble: 6 × 3 ## country year population ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 19987071 ## 2 Brazil 1999 172006362 ## 3 China 1999 1272915272 ## 4 Afghanistan 2000 20595360 ## 5 Brazil 2000 174504898 ## 6 China 2000 1280428583 Note that we get new key and value columns and the gathered columns are dropped. This has the effect of gathering them down into more rows. It should be noted that many examples of gathering are not written in this way. By default, the key and value simply dropped into the correct part of the gather(). Columns that are not part of the gather are then exempted by negation. table4b %&gt;% gather(year, cases, -country) ## # A tibble: 6 × 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 19987071 ## 2 Brazil 1999 172006362 ## 3 China 1999 1272915272 ## 4 Afghanistan 2000 20595360 ## 5 Brazil 2000 174504898 ## 6 China 2000 1280428583 David Robinson has even shown examples where he simply uses gather(key, value, -exemptedColumn) (see his YouTube at https://www.youtube.com/watch?v=KzRP40PzopY). The two new tidy tables are joined as follows: left_join(tidy4a, tidy4b) ## Joining, by = c(&quot;country&quot;, &quot;year&quot;) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Brazil 1999 37737 172006362 ## 3 China 1999 212258 1272915272 ## 4 Afghanistan 2000 2666 20595360 ## 5 Brazil 2000 80488 174504898 ## 6 China 2000 213766 1280428583 David Robinson shows in this bit https://www.youtube.com/watch?v=KzRP40PzopY (about 8m 30s into the YouTube) that you don’t need to make the key and value explicit, and also that you can exempt some columns from the process. 3.4 Spread your wings So spread is the opposite of gather. Consider Table 2. table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Now our task is to spread cases and population from the type column into their own columns. In table2 the type column should be distributed into variable with their own columns. Note that this has the effect of spreading table into more columns. table2 %&gt;% spread(key = type, value = count) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Hadley provides an interesting example that shows how gather and spread are NOT perfect compliments. stocks &lt;- tibble( year = c(2015, 2015, 2016, 2016), half = c( 1, 2, 1, 2), return = c(1.88, 0.59, 0.92, 0.17) ) stocks ## # A tibble: 4 × 3 ## year half return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2016 1 0.92 ## 4 2016 2 0.17 stocks %&gt;% spread(year, return) %&gt;% gather(&quot;year&quot;, &quot;return&quot;, `2015`:`2016`) ## # A tibble: 4 × 3 ## half year return ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2015 1.88 ## 2 2 2015 0.59 ## 3 1 2016 0.92 ## 4 2 2016 0.17 Note how the correct columns are in place, but the year column is now character data. 3.5 Missing Data: Ich vemisse Dich! Hadley provides a different stock example: stocks &lt;- tibble( year = c(2015, 2015, 2015, 2015, 2016, 2016, 2016), qtr = c( 1, 2, 3, 4, 2, 3, 4), return = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66) ) stocks ## # A tibble: 7 × 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2015 4 NA ## 5 2016 2 0.92 ## 6 2016 3 0.17 ## 7 2016 4 2.66 Hadley points out that 2 data points are missing in this data set: one explicit and one implicit. One way to think about the difference is with this Zen-like koan: An explicit missing value is the presence of an absence; an implicit missing value is the absence of a presence. The implicitly missing value here can be made explicit by spreading the years. stocks %&gt;% spread(year, return) ## # A tibble: 4 × 3 ## qtr `2015` `2016` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.88 NA ## 2 2 0.59 0.92 ## 3 3 0.35 0.17 ## 4 4 NA 2.66 Note exactly how this works: the rows are now determined by qtr. The 2015 4th quarter value, explicitly NA before, remains NA. Now, however, when 2016 is spread as a variable, the gap in quarter 1 is revealed. These NA’s can be removed by gathering the data back up using na.rm = TRUE. stocks %&gt;% spread(year, return) %&gt;% gather(year, return, `2015`:`2016`, na.rm = TRUE) ## # A tibble: 6 × 3 ## qtr year return ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2015 1.88 ## 2 2 2015 0.59 ## 3 3 2015 0.35 ## 4 2 2016 0.92 ## 5 3 2016 0.17 ## 6 4 2016 2.66 3.5.1 You complete(me) The complete() function is also an important way to make missing values explicit. &lt; complete() takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit NAs where necessary stocks ## # A tibble: 7 × 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2015 4 NA ## 5 2016 2 0.92 ## 6 2016 3 0.17 ## 7 2016 4 2.66 stocks %&gt;% complete(year, qtr) ## # A tibble: 8 × 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2015 4 NA ## 5 2016 1 NA ## 6 2016 2 0.92 ## 7 2016 3 0.17 ## 8 2016 4 2.66 3.5.2 fill(ing) in the gaps While complete() can make explicit the missing values, fill() lets you address a particular type of missing value. Here, the last observation is carried forward. This is very common in data sets. treatment &lt;- tribble( ~ person, ~ treatment, ~response, &quot;Derrick Whitmore&quot;, 1, 7, NA, 2, 10, NA, 3, 9, &quot;Katherine Burke&quot;, 1, 4 ) treatment ## # A tibble: 4 × 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 &lt;NA&gt; 2 10 ## 3 &lt;NA&gt; 3 9 ## 4 Katherine Burke 1 4 treatment %&gt;% fill(person) ## # A tibble: 4 × 3 ## person treatment response ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Derrick Whitmore 1 7 ## 2 Derrick Whitmore 2 10 ## 3 Derrick Whitmore 3 9 ## 4 Katherine Burke 1 4 3.6 Pivoting to something new These are great exercises, but gather() and spread() are being upgraded to pivot_longer() and pivot_wider(). See https://www.r-bloggers.com/data-pivoting-with-tidyr/ table4a ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4a_long &lt;- table4a %&gt;% pivot_longer( `1999`:`2000`, names_to = &quot;year&quot;, values_to = &quot;value&quot; ) table4a_long ## # A tibble: 6 × 3 ## country year value ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 table4b_long &lt;- table4b %&gt;% pivot_longer( `1999`:`2000`, names_to = &quot;year&quot;, values_to = &quot;value&quot; ) table4b_long ## # A tibble: 6 × 3 ## country year value ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 19987071 ## 2 Afghanistan 2000 20595360 ## 3 Brazil 1999 172006362 ## 4 Brazil 2000 174504898 ## 5 China 1999 1272915272 ## 6 China 2000 1280428583 This is presumably reversible. table4aNew &lt;- table4a_long %&gt;% pivot_wider( names_from = &quot;year&quot;, values_from = &quot;value&quot; ) table4aNew ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4a ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 So they are the same. Table 2 can also be spread()… ahem, pivot_wider()ed.  table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 table2 %&gt;% pivot_wider( names_from = c(&quot;year&quot;, &quot;type&quot;), values_from = &quot;count&quot; ) ## # A tibble: 3 × 5 ## country `1999_cases` `1999_population` `2000_cases` `2000_population` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 19987071 2666 20595360 ## 2 Brazil 37737 172006362 80488 174504898 ## 3 China 212258 1272915272 213766 1280428583 Note that, names_from can come from multiple columns. When there are multiple names_from or values_from columns, names_sep will be used to join values together to form column names. 3.7 Larger structures The dataframe feels like a natural place to begin. Many times, though I’ll want to call functions on more than one variable. . . As I struggle to get my feet under me in R, StackOverflow has been a wonderful resource, although the community can be very pissy to newbies. The following post is offers a nice description of the difference between lists and dataframes in R: https://stackoverflow.com/a/15902963/7361502 df &lt;- tibble::tibble( a = rnorm(10), b = rbinom(10, 1, 0.5), c = rchisq(10, df = 6), d = rexp(10, rate = 1) ) map_dbl(df, mean) ## a b c d ## 0.0006191513 0.4000000000 5.7383392077 1.0044690337 map_dfc(df, mean) ## # A tibble: 1 × 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000619 0.4 5.74 1.00 map_dfr(df, mean) ## # A tibble: 1 × 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000619 0.4 5.74 1.00 "],["changenames.html", "Chapter 4 By Any Other Name 4.1 A financial example", " Chapter 4 By Any Other Name This deceptively simple-seeming idea gets complex quickly. The following YouTube was a nice description of the process: https://www.youtube.com/watch?v=Okc0IL5uTnA my.data &lt;- data.frame(colOne=1:3, column2=4:6, column_3=7:9) rownames(my.data) &lt;- c(&quot;ant&quot;, &quot;bee&quot;, &quot;cat&quot;) names(my.data) ## [1] &quot;colOne&quot; &quot;column2&quot; &quot;column_3&quot; colnames(my.data) ## [1] &quot;colOne&quot; &quot;column2&quot; &quot;column_3&quot; #make some changes names(my.data) &lt;- c(&quot;col_1&quot;, &quot;col_2&quot;, &quot;col_3&quot;) my.data ## col_1 col_2 col_3 ## ant 1 4 7 ## bee 2 5 8 ## cat 3 6 9 names(my.data)[3] &lt;- &quot;col.3&quot; my.data ## col_1 col_2 col.3 ## ant 1 4 7 ## bee 2 5 8 ## cat 3 6 9 names(my.data)[names(my.data)==&quot;col_2&quot;] ## [1] &quot;col_2&quot; my.data[&quot;col_2&quot;] ## col_2 ## ant 4 ## bee 5 ## cat 6 my.data$col_2 ## [1] 4 5 6 my.data[,2] ## [1] 4 5 6 names(my.data)[names(my.data)==&quot;col_2&quot;] &lt;- &quot;col.2&quot; my.data ## col_1 col.2 col.3 ## ant 1 4 7 ## bee 2 5 8 ## cat 3 6 9 names(my.data) &lt;- gsub(&quot;_&quot;, &quot;.&quot;, names(my.data)) my.data ## col.1 col.2 col.3 ## ant 1 4 7 ## bee 2 5 8 ## cat 3 6 9 rownames(my.data) ## [1] &quot;ant&quot; &quot;bee&quot; &quot;cat&quot; my.data$species &lt;- rownames(my.data) my.data ## col.1 col.2 col.3 species ## ant 1 4 7 ant ## bee 2 5 8 bee ## cat 3 6 9 cat rownames(my.data) &lt;- NULL my.data ## col.1 col.2 col.3 species ## 1 1 4 7 ant ## 2 2 5 8 bee ## 3 3 6 9 cat colnames(my.data) &lt;- c(&quot;good&quot;, &quot;better&quot;, &quot;best&quot;, &quot;species&quot;) my.data ## good better best species ## 1 1 4 7 ant ## 2 2 5 8 bee ## 3 3 6 9 cat keep &lt;- 2:ncol(my.data) my.data[,keep] ## better best species ## 1 4 7 ant ## 2 5 8 bee ## 3 6 9 cat 4.1 A financial example Dealing with financial data often means cleaning variable names and getting pesky dollar signs and commas out of downloaded files. Check out this example: library(tidyverse) First, load the tidyverse etf_data &lt;- tribble( ~Sign, ~&quot;Market Value&quot;, &quot;VTI&quot;, &quot;$172.22&quot;, &quot;VXUS&quot;, &quot;$52.99&quot; ) etf_data ## # A tibble: 2 × 2 ## Sign `Market Value` ## &lt;chr&gt; &lt;chr&gt; ## 1 VTI $172.22 ## 2 VXUS $52.99 # First let&#39;s address the crappy variable names: etf_data_namedOK &lt;- janitor::clean_names(etf_data) etf_data_namedOK ## # A tibble: 2 × 2 ## sign market_value ## &lt;chr&gt; &lt;chr&gt; ## 1 VTI $172.22 ## 2 VXUS $52.99 # Now remove the offending dollar signs and commas (if present) etf_data_namedOK$market_value &lt;- as.numeric(gsub(&#39;\\\\$|,&#39;,&#39;&#39;,as.character(etf_data_namedOK$market_value))) etf_data_namedOK ## # A tibble: 2 × 2 ## sign market_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 VTI 172. ## 2 VXUS 53.0 # Don&#39;t be alarmed by the rounding behavior in the console. You can see that the data is intact. str(etf_data_namedOK) ## tibble [2 × 2] (S3: tbl_df/tbl/data.frame) ## $ sign : chr [1:2] &quot;VTI&quot; &quot;VXUS&quot; ## $ market_value: num [1:2] 172 53 glimpse(etf_data_namedOK) ## Rows: 2 ## Columns: 2 ## $ sign &lt;chr&gt; &quot;VTI&quot;, &quot;VXUS&quot; ## $ market_value &lt;dbl&gt; 172.22, 52.99 etf_data_namedOK$market_value ## [1] 172.22 52.99 "],["factorpractice.html", "Chapter 5 Factor Practice 5.1 The basic structure 5.2 Not all nominal data is a factor 5.3 Making variables into factors 5.4 Inspecting factors 5.5 fct_lump() 5.6 fct_infreq() and fct_rev() 5.7 Additional practice 5.8 Reordering Factors", " Chapter 5 Factor Practice 5.1 The basic structure Let’s say that you have 100 cups of sizes small, medium, or large. cups &lt;- c(&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;) set.seed(2020) manyCups &lt;- sample(cups, size = 100, replace = TRUE) table(manyCups) ## manyCups ## large medium small ## 25 39 36 str(manyCups) ## chr [1:100] &quot;large&quot; &quot;medium&quot; &quot;small&quot; &quot;small&quot; &quot;medium&quot; &quot;medium&quot; &quot;small&quot; ... Note that table() renders these in alphabetical order. It’s usually more useful to render them in order of magnitude by setting levels. sizesCups &lt;- factor(manyCups, levels = c(&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;)) sizesCups ## [1] large medium small small medium medium small small medium medium ## [11] medium small medium large medium small medium medium medium large ## [21] medium large medium small large medium medium medium large medium ## [31] large large small small small medium large small small medium ## [41] large small small small large small small large medium medium ## [51] medium small large medium small medium large small small medium ## [61] small medium large medium large medium medium small large medium ## [71] large small medium large medium medium medium small small small ## [81] large medium medium small small medium small small medium small ## [91] large small large small small large large large medium small ## Levels: small medium large table(sizesCups) ## sizesCups ## small medium large ## 36 39 25 levels(sizesCups) ## [1] &quot;small&quot; &quot;medium&quot; &quot;large&quot; levels(manyCups) ## NULL The forcats package is designed to work with factors. Although I use it frequently, I haven’t generalized the ideas, or internalized them. That’s a mistake. The RStudio cheat sheet is a nice place to begin with this: https://rstudio.com/resources/cheatsheets/. Let’s use my favorite starwars illustration to help illustrate some of forcats’s usefulness. starwars %&gt;% filter(!is.na(species)) %&gt;% count(species, sort = TRUE) ## # A tibble: 37 × 2 ## species n ## &lt;chr&gt; &lt;int&gt; ## 1 Human 35 ## 2 Droid 6 ## 3 Gungan 3 ## 4 Kaminoan 2 ## 5 Mirialan 2 ## 6 Twi&#39;lek 2 ## 7 Wookiee 2 ## 8 Zabrak 2 ## 9 Aleena 1 ## 10 Besalisk 1 ## # … with 27 more rows The forcats cheat sheet contains a bunch of often-used functions that I rarely take the time to think about. 5.2 Not all nominal data is a factor Note that many categorical variables are not encoded as factors. This has certain advantages, since levels can sometimes be seen as complicating ballast that is dragged around by the variable. See, so instance, that the species variable is not encoded as a factor: class(starwars$species) ## [1] &quot;character&quot; See? species is character, not factor, class. 5.3 Making variables into factors The factor() function (or the as_factor() function) make a variable into a factor and allow the assignation of levels. class(factor(starwars$species)) ## [1] &quot;factor&quot; The levels() function is useful for returning these levels: levels(factor(starwars$species)) ## [1] &quot;Aleena&quot; &quot;Besalisk&quot; &quot;Cerean&quot; &quot;Chagrian&quot; ## [5] &quot;Clawdite&quot; &quot;Droid&quot; &quot;Dug&quot; &quot;Ewok&quot; ## [9] &quot;Geonosian&quot; &quot;Gungan&quot; &quot;Human&quot; &quot;Hutt&quot; ## [13] &quot;Iktotchi&quot; &quot;Kaleesh&quot; &quot;Kaminoan&quot; &quot;Kel Dor&quot; ## [17] &quot;Mirialan&quot; &quot;Mon Calamari&quot; &quot;Muun&quot; &quot;Nautolan&quot; ## [21] &quot;Neimodian&quot; &quot;Pau&#39;an&quot; &quot;Quermian&quot; &quot;Rodian&quot; ## [25] &quot;Skakoan&quot; &quot;Sullustan&quot; &quot;Tholothian&quot; &quot;Togruta&quot; ## [29] &quot;Toong&quot; &quot;Toydarian&quot; &quot;Trandoshan&quot; &quot;Twi&#39;lek&quot; ## [33] &quot;Vulptereen&quot; &quot;Wookiee&quot; &quot;Xexto&quot; &quot;Yoda&#39;s species&quot; ## [37] &quot;Zabrak&quot; levels() can also be used to set the levels. And additional structure can be observed with unclass(). Note that the default level order in species is alphabetical. unclass(factor(starwars$species)) ## [1] 11 6 6 11 11 11 11 6 11 11 11 11 34 11 24 12 11 11 36 11 11 6 31 11 11 ## [26] 18 11 11 8 26 11 21 11 10 10 10 NA 30 7 NA 11 37 32 32 33 35 29 11 3 20 ## [51] 37 27 13 23 16 4 11 11 11 9 17 17 11 11 11 11 5 2 15 15 11 1 6 25 19 ## [76] 28 14 34 11 NA 22 11 11 11 6 NA 11 ## attr(,&quot;levels&quot;) ## [1] &quot;Aleena&quot; &quot;Besalisk&quot; &quot;Cerean&quot; &quot;Chagrian&quot; ## [5] &quot;Clawdite&quot; &quot;Droid&quot; &quot;Dug&quot; &quot;Ewok&quot; ## [9] &quot;Geonosian&quot; &quot;Gungan&quot; &quot;Human&quot; &quot;Hutt&quot; ## [13] &quot;Iktotchi&quot; &quot;Kaleesh&quot; &quot;Kaminoan&quot; &quot;Kel Dor&quot; ## [17] &quot;Mirialan&quot; &quot;Mon Calamari&quot; &quot;Muun&quot; &quot;Nautolan&quot; ## [21] &quot;Neimodian&quot; &quot;Pau&#39;an&quot; &quot;Quermian&quot; &quot;Rodian&quot; ## [25] &quot;Skakoan&quot; &quot;Sullustan&quot; &quot;Tholothian&quot; &quot;Togruta&quot; ## [29] &quot;Toong&quot; &quot;Toydarian&quot; &quot;Trandoshan&quot; &quot;Twi&#39;lek&quot; ## [33] &quot;Vulptereen&quot; &quot;Wookiee&quot; &quot;Xexto&quot; &quot;Yoda&#39;s species&quot; ## [37] &quot;Zabrak&quot; 5.4 Inspecting factors We see that there are a ton of unique species in this list. fct_count(starwars$species, sort = TRUE) ## # A tibble: 38 × 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Human 35 ## 2 Droid 6 ## 3 &lt;NA&gt; 4 ## 4 Gungan 3 ## 5 Kaminoan 2 ## 6 Mirialan 2 ## 7 Twi&#39;lek 2 ## 8 Wookiee 2 ## 9 Zabrak 2 ## 10 Aleena 1 ## # … with 28 more rows fct_unique(factor(starwars$species)) ## [1] Aleena Besalisk Cerean Chagrian Clawdite ## [6] Droid Dug Ewok Geonosian Gungan ## [11] Human Hutt Iktotchi Kaleesh Kaminoan ## [16] Kel Dor Mirialan Mon Calamari Muun Nautolan ## [21] Neimodian Pau&#39;an Quermian Rodian Skakoan ## [26] Sullustan Tholothian Togruta Toong Toydarian ## [31] Trandoshan Twi&#39;lek Vulptereen Wookiee Xexto ## [36] Yoda&#39;s species Zabrak ## 37 Levels: Aleena Besalisk Cerean Chagrian Clawdite Droid Dug ... Zabrak 5.5 fct_lump() We often don’t want to see all of the rare outcomes for a nominal variable, and are content to lump the uncommon ones together in an other category. starwars %&gt;% filter(!is.na(species)) %&gt;% mutate(species = fct_lump(species, 5)) %&gt;% count(species, sort = TRUE) ## # A tibble: 9 × 2 ## species n ## &lt;fct&gt; &lt;int&gt; ## 1 Human 35 ## 2 Other 29 ## 3 Droid 6 ## 4 Gungan 3 ## 5 Kaminoan 2 ## 6 Mirialan 2 ## 7 Twi&#39;lek 2 ## 8 Wookiee 2 ## 9 Zabrak 2 fct_lump() is will produce more than the requested number of categories (in this case 5), when ties are present in the last place. In this case, since 5 species each have 2 members, all of these species are listed. 5.6 fct_infreq() and fct_rev() These are very important in plotting. See Figure 7.5 for a good example. 5.7 Additional practice The following is based on code and materials from http://stat545.com/block029_factors.html with some running commentary by me. Get to know the factors by assaying the gapminder dataset, particularly the factor “continent.” skimr::skim(gapminder) Table 5.1: Data summary Name gapminder Number of rows 1704 Number of columns 6 _______________________ Column type frequency: factor 2 numeric 4 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts country 0 1 FALSE 142 Afg: 12, Alb: 12, Alg: 12, Ang: 12 continent 0 1 FALSE 5 Afr: 624, Asi: 396, Eur: 360, Ame: 300 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist year 0 1 1979.50 17.27 1952.00 1965.75 1979.50 1993.25 2007.0 ▇▅▅▅▇ lifeExp 0 1 59.47 12.92 23.60 48.20 60.71 70.85 82.6 ▁▆▇▇▇ pop 0 1 29601212.32 106157896.74 60011.00 2793664.00 7023595.50 19585221.75 1318683096.0 ▇▁▁▁▁ gdpPercap 0 1 7215.33 9857.45 241.17 1202.06 3531.85 9325.46 113523.1 ▇▁▁▁▁ glimpse(gapminder) ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, … ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, … ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8… ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12… ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, … str(gapminder) ## tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... ## $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... ## $ gdpPercap: num [1:1704] 779 821 853 836 740 ... levels(gapminder$continent) ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; nlevels(gapminder$continent) ## [1] 5 class(gapminder$continent) ## [1] &quot;factor&quot; Now to study the continent a little more carefully. gapminder %&gt;% count(continent, sort = TRUE) ## # A tibble: 5 × 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Asia 396 ## 3 Europe 360 ## 4 Americas 300 ## 5 Oceania 24 fct_count(gapminder$continent, sort = TRUE) ## # A tibble: 5 × 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Asia 396 ## 3 Europe 360 ## 4 Americas 300 ## 5 Oceania 24 However, R keeps levels even when you filter out values. It drags them around like invisible dead weight. See, for instance: nlevels(gapminder$country) ## [1] 142 h_countries &lt;- c(&quot;Egypt&quot;, &quot;Haiti&quot;, &quot;Romania&quot;, &quot;Thailand&quot;, &quot;Venezuela&quot;) h_gap &lt;- gapminder %&gt;% filter(country %in% h_countries) nlevels(h_gap$country) ## [1] 142 str(h_gap$country) ## Factor w/ 142 levels &quot;Afghanistan&quot;,..: 39 39 39 39 39 39 39 39 39 39 ... str(h_gap) ## tibble [60 × 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 39 39 39 39 39 39 39 39 39 39 ... ## $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ year : int [1:60] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:60] 41.9 44.4 47 49.3 51.1 ... ## $ pop : int [1:60] 22223309 25009741 28173309 31681188 34807417 38783863 45681811 52799062 59402198 66134291 ... ## $ gdpPercap: num [1:60] 1419 1459 1693 1815 2024 ... So h_gap has only 60 records from only 5 countries, but it retains 142 country levels. There are 2 easy solutions that drop these levels: droplevels() from base R and forcats::fct_drop(). h_gap_dropped &lt;- h_gap %&gt;% droplevels() nlevels(h_gap_dropped$country) ## [1] 5 str(h_gap_dropped) ## tibble [60 × 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 5 levels &quot;Egypt&quot;,&quot;Haiti&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 4 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ year : int [1:60] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:60] 41.9 44.4 47 49.3 51.1 ... ## $ pop : int [1:60] 22223309 25009741 28173309 31681188 34807417 38783863 45681811 52799062 59402198 66134291 ... ## $ gdpPercap: num [1:60] 1419 1459 1693 1815 2024 ... Now we can see that h_gap_dropped has only 5 country levels, as one would expect. It still retains all 60 rows. fct_drop() is the other way to do this: x &lt;- fct_drop(h_gap$country) levels(x) ## [1] &quot;Egypt&quot; &quot;Haiti&quot; &quot;Romania&quot; &quot;Thailand&quot; &quot;Venezuela&quot; nlevels(x) ## [1] 5 5.8 Reordering Factors gap_asia_2007 &lt;- gapminder %&gt;% filter(year == 2007, continent == &quot;Asia&quot;) gap_asia_2007 %&gt;% ggplot(aes(x = lifeExp, y = country)) + geom_point() This is helter skelter. Much better is an reordered display, based upon life expectancy. gap_asia_2007 %&gt;% ggplot(aes(x = lifeExp, y = fct_reorder(country, lifeExp))) + geom_point() ### Improving legibility: change the linetype h_countries &lt;- c(&quot;Egypt&quot;, &quot;Haiti&quot;, &quot;Romania&quot;, &quot;Thailand&quot;, &quot;Venezuela&quot;) h_gap &lt;- gapminder %&gt;% filter(country %in% h_countries) %&gt;% droplevels() h_gap %&gt;% ggplot(aes(x = year, y = lifeExp, color = country, linetype = country)) + geom_line() 5.8.1 fct_reorder2(): another way to improve legibility Use fct_reorder2() when you have a line chart of a quantitative x against another quantitative y and your factor provides the color. The legend appears in some order as the data. h_gap %&gt;% ggplot(aes(x = year, y = lifeExp, color = fct_reorder2(country, year, lifeExp))) + geom_line() + labs(color = &quot;country&quot;) Now work on combining the two elements. Note in the following that the color and linetype are treated separately in both the aes() and the labs(). The “Spiffy Title” has to be named explicitly–and they must be exactly the same–for both aesthetic values or two legends will be generated. h_gap %&gt;% ggplot(aes(x = year, y = lifeExp, color = fct_reorder2(country, year, lifeExp), linetype = fct_reorder2(country, year, lifeExp))) + geom_line() + labs(color = &quot;Spiffy Title&quot;, linetype = &quot;Spiffy Title&quot;) "],["subset.html", "Chapter 6 Subsetting 6.1 Subsetting using brackets 6.2 Subset using brackets by omitting the rows and columns we don’t want 6.3 Subset using brackets in combination with the which() function and the %in% operator 6.4 Subset using the subset() function 6.5 Subset using dyplyr’s filter() and select()", " Chapter 6 Subsetting From https://www.r-bloggers.com/5-ways-to-subset-a-data-frame-in-r/ Note: since this is down for maintenance, I will turn off evaluation on these chunks: education &lt;- read.csv(&quot;https://vincentarelbundock.github.io/Rdatasets/csv/robustbase/education.csv&quot;, stringsAsFactors = FALSE) colnames(education) &lt;- c(&quot;X&quot;,&quot;State&quot;,&quot;Region&quot;,&quot;Urban.Population&quot;,&quot;Per.Capita.Income&quot;,&quot;Minor.Population&quot;,&quot;Education.Expenditures&quot;) glimpse(education) ## Rows: 50 ## Columns: 7 ## $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ State &lt;chr&gt; &quot;ME&quot;, &quot;NH&quot;, &quot;VT&quot;, &quot;MA&quot;, &quot;RI&quot;, &quot;CT&quot;, &quot;NY&quot;, &quot;NJ&quot;,… ## $ Region &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,… ## $ Urban.Population &lt;int&gt; 508, 564, 322, 846, 871, 774, 856, 889, 715, 75… ## $ Per.Capita.Income &lt;int&gt; 3944, 4578, 4011, 5233, 4780, 5889, 5663, 5759,… ## $ Minor.Population &lt;int&gt; 325, 323, 328, 305, 303, 307, 301, 310, 300, 32… ## $ Education.Expenditures &lt;int&gt; 235, 231, 270, 261, 300, 317, 387, 285, 300, 22… 6.1 Subsetting using brackets education[c(10:21),c(2,6:7)] ## State Minor.Population Education.Expenditures ## 10 OH 324 221 ## 11 IN 329 264 ## 12 IL 320 308 ## 13 MI 337 379 ## 14 WI 328 342 ## 15 MN 330 378 ## 16 IA 318 232 ## 17 MO 309 231 ## 18 ND 333 246 ## 19 SD 330 230 ## 20 NB 318 268 ## 21 KS 304 337 6.2 Subset using brackets by omitting the rows and columns we don’t want education[-c(1:9,22:50),-c(1,3:5)] ## State Minor.Population Education.Expenditures ## 10 OH 324 221 ## 11 IN 329 264 ## 12 IL 320 308 ## 13 MI 337 379 ## 14 WI 328 342 ## 15 MN 330 378 ## 16 IA 318 232 ## 17 MO 309 231 ## 18 ND 333 246 ## 19 SD 330 230 ## 20 NB 318 268 ## 21 KS 304 337 6.3 Subset using brackets in combination with the which() function and the %in% operator education[which(education$Region == 2),names(education) %in% c(&quot;State&quot;,&quot;Minor.Population&quot;,&quot;Education.Expenditures&quot;)] ## State Minor.Population Education.Expenditures ## 10 OH 324 221 ## 11 IN 329 264 ## 12 IL 320 308 ## 13 MI 337 379 ## 14 WI 328 342 ## 15 MN 330 378 ## 16 IA 318 232 ## 17 MO 309 231 ## 18 ND 333 246 ## 19 SD 330 230 ## 20 NB 318 268 ## 21 KS 304 337 6.4 Subset using the subset() function subset(education, Region == 2, select = c(&quot;State&quot;,&quot;Minor.Population&quot;,&quot;Education.Expenditures&quot;)) ## State Minor.Population Education.Expenditures ## 10 OH 324 221 ## 11 IN 329 264 ## 12 IL 320 308 ## 13 MI 337 379 ## 14 WI 328 342 ## 15 MN 330 378 ## 16 IA 318 232 ## 17 MO 309 231 ## 18 ND 333 246 ## 19 SD 330 230 ## 20 NB 318 268 ## 21 KS 304 337 6.5 Subset using dyplyr’s filter() and select() select(filter(education, Region == 2),c(State,Minor.Population:Education.Expenditures)) ## State Minor.Population Education.Expenditures ## 1 OH 324 221 ## 2 IN 329 264 ## 3 IL 320 308 ## 4 MI 337 379 ## 5 WI 328 342 ## 6 MN 330 378 ## 7 IA 318 232 ## 8 MO 309 231 ## 9 ND 333 246 ## 10 SD 330 230 ## 11 NB 318 268 ## 12 KS 304 337 "],["dataexploration.html", "Chapter 7 Data Exploration 7.1 Counting things. The naming of parts. 7.2 fct_infreq 7.3 Weight weight, don’t tell me! 7.4 Summarize is another very useful function: 7.5 Graphical displays 7.6 Relative versus absolute risk 7.7 Removing duplicates", " Chapter 7 Data Exploration Data exploration is one of the most important aspects of data science and forms the cornerstone of my drills. Nonetheless, I have lots of room for improvement. I like Hadley Wickham’s writing and find his approach exceptionally clear. Therefore, I’ll use the tidyverse. library(tidyverse) 7.1 Counting things. The naming of parts. starwars %&gt;% filter(!is.na(species)) %&gt;% count(species = fct_lump(species, 5), sort = TRUE) %&gt;% mutate(species = fct_reorder(species, n)) %&gt;% ggplot(aes(species, n)) + geom_col() + coord_flip() Figure 7.1: Starwars Figure 1 I like stacked bars for their economy, but it’s easy to over do it. Supperimposing gender onto the columns seems easy… starwars %&gt;% filter(!is.na(species)) %&gt;% count(species = fct_lump(species, 5), gender = fct_lump(gender, 2), sort = TRUE) %&gt;% mutate(species = fct_reorder(species, n)) %&gt;% ggplot(aes(species, n, fill = gender)) + geom_col() + coord_flip() Figure 7.2: Starwars Figure 2 But note that I’ve got a problem: the Droids, which outnumber the Gungans, are now reordered to after the Gungans. This happens because the \\(n\\) that we’re counting comprises subcategories of species and gender. Only three Gungan males exist (and no females), but that is enough to tie the Droid NA category. The Droid NA category come after the Gungan category, presumably because male comes before NA, or because NA comes last (more likely). Exploring this, I see that I’m getting warning messages about the implicit NA’s in gender. Note that the following renders a slightly different plot. I still have not fixed the order of the species. starwars %&gt;% filter(!is.na(species)) %&gt;% count(species = fct_lump(species, 5), gender = fct_lump(gender, 2), sort = TRUE) %&gt;% mutate(gender = fct_explicit_na(gender), species = fct_reorder(species, n)) %&gt;% ggplot(aes(species, n, fill = gender)) + geom_col() + coord_flip() Figure 7.3: Starwars Figure 3 The trick here is to use group_by() and ungroup() wisely. starwars %&gt;% filter(!is.na(species)) %&gt;% mutate(species = fct_lump(species, 5)) %&gt;% group_by(species) %&gt;% mutate(typeCount = n()) %&gt;% ungroup() %&gt;% mutate(species = fct_reorder(species, typeCount)) %&gt;% ggplot()+ geom_bar(aes(species, fill = gender))+ coord_flip() Figure 7.4: Starwars Figure 4 As opposed to using count(), which progressively narrows the information available to be used, by using group_by()/mutate()/ungroup() with geom_bar() we have all of the variables still available for plotting. 7.2 fct_infreq As expected, Hadley Wickham and Garrett Grolemund solve this more simply in R4DS: starwars %&gt;% filter(!is.na(species)) %&gt;% mutate(species = fct_lump(species, 5)) %&gt;% mutate(species = species %&gt;% fct_infreq() %&gt;% fct_rev()) %&gt;% ggplot()+ geom_bar(aes(species, fill = gender))+ coord_flip() Figure 7.5: Starwars Figure 5. Serial mutates are used. Note that a single mutate() suffices to both lump factors, organize by frequency and reverse the order, as follows. starwars %&gt;% filter(!is.na(species)) %&gt;% mutate(species = fct_lump(species, 5) %&gt;% fct_infreq() %&gt;% fct_rev()) %&gt;% ggplot()+ geom_bar(aes(species, fill = gender))+ coord_flip() Figure 7.6: Starwars Figure 6. A single mutate is used. 7.3 Weight weight, don’t tell me! One feature of count() is difficult for me to remember, mostly because it is not a count–it is a sum. df &lt;- tibble( Symbol = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;), Value = 1:6 ) df %&gt;% count(Symbol, sort = TRUE) ## # A tibble: 5 × 2 ## Symbol n ## &lt;chr&gt; &lt;int&gt; ## 1 c 2 ## 2 a 1 ## 3 b 1 ## 4 d 1 ## 5 e 1 df %&gt;% count(Symbol, wt = Value, sort = TRUE) ## # A tibble: 5 × 2 ## Symbol n ## &lt;chr&gt; &lt;int&gt; ## 1 c 7 ## 2 e 6 ## 3 d 5 ## 4 b 2 ## 5 a 1 The above example shows that count() performs in the usual way unless we use the wt = argument, which then sums the value specified and generates n, which now indicates a weight, not a count of a nominal variable. 7.4 Summarize is another very useful function: starwars %&gt;% filter(!(is.na(species))) %&gt;% group_by(species) %&gt;% summarize(n=n(), mean = mean(height, na.rm = TRUE)) %&gt;% arrange(desc(n)) ## # A tibble: 37 × 3 ## species n mean ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Human 35 177. ## 2 Droid 6 131. ## 3 Gungan 3 209. ## 4 Kaminoan 2 221 ## 5 Mirialan 2 168 ## 6 Twi&#39;lek 2 179 ## 7 Wookiee 2 231 ## 8 Zabrak 2 173 ## 9 Aleena 1 79 ## 10 Besalisk 1 198 ## # … with 27 more rows 7.5 Graphical displays At the risk of biting off far more than I can chew without starting a separate chapter on my favorite data displays, I’d like to include a few plots not represented elsewhere. 7.5.1 Joyplots Joyplots are a little hokey but can be visually satisfying. library(ggridges) The following display is from data taken from The Cancer Genome Atlas in 2017. It combines glioblastoma with lower grade infiltrating gliomas. Since I’m only interested in diagnosis and patient age, that’s all that I’ve assembled in the CSV. DiffuseGlioma &lt;- read_csv(&quot;data/tcga9.11.17.csv&quot;) ## Rows: 1122 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): HISTOLOGY ## dbl (1): AGE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. DiffuseGlioma &lt;- na.omit(DiffuseGlioma) #gets rid of the NA&#39;s head(DiffuseGlioma) ## # A tibble: 6 × 2 ## HISTOLOGY AGE ## &lt;chr&gt; &lt;dbl&gt; ## 1 astrocytoma 31 ## 2 astrocytoma 67 ## 3 astrocytoma 44 ## 4 astrocytoma 37 ## 5 astrocytoma 50 ## 6 oligodendroglioma 47 ggplot(DiffuseGlioma,aes(x=AGE,y=HISTOLOGY)) + geom_density_ridges(rel_min_height = 0.01) + # removes tails scale_y_discrete(expand = c(0.01, 0)) + # removes cutoff top labs(x=&quot;age (y)&quot;,y=&quot;number&quot;) + theme_minimal() ## Picking joint bandwidth of 4.16 7.6 Relative versus absolute risk Relative risk is often used in medical literature to emphasize the importance of a finding–it often makes a result sound more impressive. However, when one considers absolute risk, the effect is obviously much smaller. The following is adopted from https://www.r-bloggers.com/lying-with-statistics-one-beer-a-day-will-kill-you/ personograph allows for cute displays of individuals in big groups library(personograph) Start with 2000 people. Some of them will have problems without alcohol exposure, about 18, in fact. The blogger choses 2000 people to start with because \\(0.7*18=1\\) Note that this doesn’t stratify for any other health problems, age, socio-economic status, etc. n &lt;- 2000 probl_wo_alc &lt;- 18 / n data &lt;- list(first = probl_wo_alc, second = 1-probl_wo_alc) personograph(data, colors = list(first = &quot;black&quot;, second = &quot;#efefef&quot;), fig.title = &quot;18 of 2000 people with health problems&quot;, draw.legend = FALSE, n.icons = n, dimensions = c(20, 100), plot.width = 0.97) Now we illustrate the affect of 500 mL of alcohol per day. According to the Lancet article, the relative risk of serious illness following consumption of about 25 mL ethanol (500 mL beer at 5% ABV) increases by about 7%. probl_w_alc &lt;- 1 / n data_2 &lt;- list(first = probl_wo_alc, second = probl_w_alc, third = 1-(probl_wo_alc+probl_w_alc)) personograph(data_2, colors = list(first = &quot;black&quot;, second = &quot;red&quot;, third = &quot;#efefef&quot;), fig.title = &quot;About 1 additional case with half a litre of beer per day&quot;, draw.legend = FALSE, n.icons = n, dimensions = c(20, 100), plot.width = 0.97) 7.7 Removing duplicates The following is taken from https://www.r-bloggers.com/2021/08/how-to-remove-duplicates-in-r-with-example/. data &lt;- data.frame(Column1 = c(&#39;P1&#39;, &#39;P1&#39;, &#39;P2&#39;, &#39;P3&#39;, &#39;P1&#39;, &#39;P1&#39;, &#39;P3&#39;, &#39;P4&#39;, &#39;P2&#39;, &#39;P4&#39;), Column2 = c(5, 5, 3, 5, 2, 3, 4, 7, 10, 14)) data ## Column1 Column2 ## 1 P1 5 ## 2 P1 5 ## 3 P2 3 ## 4 P3 5 ## 5 P1 2 ## 6 P1 3 ## 7 P3 4 ## 8 P4 7 ## 9 P2 10 ## 10 P4 14 Note the duplication of rows with P1 and 5. The whole row can be removed as follows. distinct(data) ## Column1 Column2 ## 1 P1 5 ## 2 P2 3 ## 3 P3 5 ## 4 P1 2 ## 5 P1 3 ## 6 P3 4 ## 7 P4 7 ## 8 P2 10 ## 9 P4 14 Reduplicated information in a column can also be removed: distinct(data, Column2) ## Column2 ## 1 5 ## 2 3 ## 3 2 ## 4 4 ## 5 7 ## 6 10 ## 7 14 To remove duplicates from a column but to keep all the information in Column1: distinct(data, Column2, .keep_all = TRUE) ## Column1 Column2 ## 1 P1 5 ## 2 P2 3 ## 3 P1 2 ## 4 P3 4 ## 5 P4 7 ## 6 P2 10 ## 7 P4 14 The duplicated function can also be used. duplicated(data) ## [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE data[!duplicated(data),] ## Column1 Column2 ## 1 P1 5 ## 3 P2 3 ## 4 P3 5 ## 5 P1 2 ## 6 P1 3 ## 7 P3 4 ## 8 P4 7 ## 9 P2 10 ## 10 P4 14 Finally, unique is also useful: unique(data) ## Column1 Column2 ## 1 P1 5 ## 3 P2 3 ## 4 P3 5 ## 5 P1 2 ## 6 P1 3 ## 7 P3 4 ## 8 P4 7 ## 9 P2 10 ## 10 P4 14 "],["sampling.html", "Chapter 8 Sampling 8.1 Think about throwing a bunch of dice. 8.2 A keen way to divide up a dataset into testing and training components.", " Chapter 8 Sampling 8.1 Think about throwing a bunch of dice. sample(1:6, size=100, replace=TRUE) ## [1] 5 4 6 1 5 4 1 4 5 4 6 2 1 1 5 2 3 3 3 1 5 2 6 5 2 3 3 4 3 1 4 3 5 4 5 4 5 ## [38] 1 5 4 3 3 2 2 6 1 5 6 3 2 4 3 1 2 6 4 2 2 3 5 3 4 1 3 3 6 1 4 4 1 3 2 3 4 ## [75] 3 4 3 4 4 5 1 5 4 4 6 1 4 4 5 2 5 3 5 2 1 4 5 1 1 1 sample(1:6, size=100, replace=TRUE) %&gt;% table() ## . ## 1 2 3 4 5 6 ## 15 18 25 15 10 17 sample(1:6, size=100, replace=TRUE) %&gt;% table() %&gt;% prop.table() ## . ## 1 2 3 4 5 6 ## 0.09 0.15 0.13 0.20 0.20 0.23 8.2 A keen way to divide up a dataset into testing and training components. x &lt;- 1:10 y &lt;- 11:30 df &lt;- data.frame(x,y) df ## x y ## 1 1 11 ## 2 2 12 ## 3 3 13 ## 4 4 14 ## 5 5 15 ## 6 6 16 ## 7 7 17 ## 8 8 18 ## 9 9 19 ## 10 10 20 ## 11 1 21 ## 12 2 22 ## 13 3 23 ## 14 4 24 ## 15 5 25 ## 16 6 26 ## 17 7 27 ## 18 8 28 ## 19 9 29 ## 20 10 30 set.seed(0) train_indexes = sample(1:nrow(df), .7 * nrow(df)) train_set &lt;- df[train_indexes,] test_set &lt;- df[-train_indexes,] train_set ## x y ## 14 4 24 ## 4 4 14 ## 7 7 17 ## 1 1 11 ## 2 2 12 ## 13 3 23 ## 18 8 28 ## 11 1 21 ## 16 6 26 ## 15 5 25 ## 3 3 13 ## 17 7 27 ## 5 5 15 ## 8 8 18 test_set ## x y ## 6 6 16 ## 9 9 19 ## 10 10 20 ## 12 2 22 ## 19 9 29 ## 20 10 30 "],["simulating.html", "Chapter 9 Simulating data 9.1 Sample() 9.2 replicate() 9.3 sample() revisited 9.4 generating fixed levels ————————————————- 9.5 generating numerical sequences 9.6 seq_along() and seq_len(). 9.7 generating random data from a probability distribution 9.8 Normal distribution: 9.9 Binomial distribution: 9.10 Uniform distribution 9.11 Sampling from multiple distributions (building in a “difference”) 9.12 The good stuff: building in a difference based on a categorical variable 9.13 A demonstration of the Central Limit Theorem 9.14 Overlaying normal curve on histogram 9.15 Crossing trial", " Chapter 9 Simulating data I’ve just begun to explore R, and I realize that many of my questions could be improved with example data. Generating this kind of data takes practice, though. Some good websites: https://clayford.github.io/dwir/dwr_12_generating_data.html https://cran.r-project.org/web/packages/simstudy/vignettes/simstudy.html https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/ Also, remember that R packages have a lot of great data. For a quick and well-written overview of the for loop, see https://www.r-bloggers.com/2021/09/r-for-loop/. data() 9.1 Sample() Starting with Clayford’s nice (and long page): sample(5) #sample without replacement ## [1] 2 5 4 3 1 # or generate a random permutation of a vector: dat &lt;- c(10,12,18,16,18,9) sample(dat) ## [1] 10 18 9 16 18 12 # bootstrap resampling: sampling the same number of items WITH replacement. The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. sample(dat, replace = TRUE) ## [1] 18 18 12 12 9 10 rm(dat) sample(state.abb, size = 10) ## [1] &quot;UT&quot; &quot;IA&quot; &quot;NC&quot; &quot;MD&quot; &quot;OH&quot; &quot;CO&quot; &quot;GA&quot; &quot;TN&quot; &quot;PA&quot; &quot;WA&quot; # Using 1:6 and size=1, we can simulate the roll of a die: sample(1:6, size=1) ## [1] 4 # We can simulate the roll of a die 100 times by setting size=100 and # replace=TRUE sample(1:6, size=100, replace=TRUE) ## [1] 4 2 4 1 6 1 4 1 6 2 3 2 6 6 2 5 2 6 6 6 1 3 3 6 4 6 3 1 4 5 1 1 6 4 5 5 4 ## [38] 6 5 4 4 1 5 5 6 1 1 3 6 2 2 3 6 2 4 3 5 2 2 1 3 3 2 2 5 2 5 4 5 4 6 1 3 2 ## [75] 3 3 1 5 6 6 6 4 4 1 5 5 6 1 3 6 3 6 3 3 4 1 1 4 2 6 # sample produces a vector, so we can manipulate it as we would any other # vector. For example, simulate a 100 die rolls and tally up the totals using # table() and prop.table(): table(sample(1:6, size=100, replace=TRUE)) ## ## 1 2 3 4 5 6 ## 20 17 14 17 17 15 prop.table(table(sample(1:6, size=100, replace=TRUE))) ## ## 1 2 3 4 5 6 ## 0.20 0.17 0.13 0.20 0.19 0.11 table(sample(state.abb, size = 1000, replace = TRUE)) ## ## AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME MI MN MO MS MT ## 22 23 19 18 30 20 18 19 26 24 23 14 19 30 18 18 15 11 26 23 21 22 27 27 18 20 ## NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT WA WI WV WY ## 20 17 18 23 17 20 18 17 17 22 18 14 19 21 17 20 10 24 21 16 20 19 28 13 prop.table(table(sample(state.abb, size = 1000, replace = TRUE))) ## ## AK AL AR AZ CA CO CT DE FL GA HI IA ID ## 0.025 0.010 0.023 0.024 0.021 0.018 0.023 0.013 0.019 0.014 0.024 0.019 0.022 ## IL IN KS KY LA MA MD ME MI MN MO MS MT ## 0.021 0.018 0.022 0.018 0.020 0.023 0.019 0.020 0.019 0.015 0.015 0.028 0.023 ## NC ND NE NH NJ NM NV NY OH OK OR PA RI ## 0.015 0.023 0.015 0.014 0.017 0.019 0.023 0.025 0.020 0.011 0.016 0.019 0.023 ## SC SD TN TX UT VA VT WA WI WV WY ## 0.024 0.032 0.025 0.020 0.015 0.019 0.026 0.024 0.016 0.016 0.027 # using the forward-pipe operator: %&gt;% library(magrittr) ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract sample(1:6, size=100, replace=TRUE) %&gt;% table() %&gt;% prop.table() ## . ## 1 2 3 4 5 6 ## 0.20 0.17 0.13 0.19 0.15 0.16 # Or simulate rolling two dice and summing the total: sum(sample(1:6, size=2, replace=TRUE)) ## [1] 6 # same thing with %&gt;% sample(6, size=2, replace=TRUE) %&gt;% sum() ## [1] 11 # simulate rolling two dice 100 times by updating the sample &quot;space&quot; sample(2:12, size=100, replace=TRUE) ## [1] 10 7 12 7 9 5 9 11 9 7 12 10 3 2 12 4 5 10 12 11 10 12 10 8 10 ## [26] 5 11 8 12 12 4 5 5 9 11 9 11 10 2 7 9 8 11 7 8 12 5 6 11 3 ## [51] 9 5 7 10 11 5 8 11 4 6 4 6 4 5 7 12 9 10 2 11 6 11 9 5 4 ## [76] 12 10 6 11 5 11 8 10 10 10 11 7 11 5 4 9 10 11 6 9 3 11 12 3 11 # proportion of &quot;snake-eyes&quot; in 1000 rolls mean(sample(2:12, size = 1000, replace = TRUE) == 2) ## [1] 0.089 9.2 replicate() We can use the replicate() function to replicate samples. The replicate() function allows you to replicate an expression as many times as you specify. The basix syntax is replicate(n, expr) where n is the number of replications and expr is the expression you want to replicate. # Roll 2 dice and keep the largest number, 10,000 times: rolls &lt;- replicate(n=1e5, expr = max(sample(1:6, size=2, replace=TRUE))) # calculate proportions: prop.table(table(rolls)) ## rolls ## 1 2 3 4 5 6 ## 0.02840 0.08260 0.13818 0.19460 0.24991 0.30631 barplot(table(rolls)) rm(rolls) 9.3 sample() revisited The sample function also has a prob argument that allows you to assign probabilities to your items. For example to simulate the flip of a loaded coin, with Tails having probability 0.65: flips &lt;- sample(c(&quot;H&quot;,&quot;T&quot;), 1000, replace=TRUE, prob = c(0.35,0.65)) prop.table(table(flips)) ## flips ## H T ## 0.337 0.663 rm(flips) Coins are nice, but we can also use sample to generate practical data, for example males and females. A web site says UVa has 11,632 female students and 10,353 male students as of Fall 2015. uva &lt;- c(11632, 10353) # female, male round(uva/sum(uva),2) ## [1] 0.53 0.47 Note how elegantly this answers a basic question. Nice! We can generate a fake random sample of 500 UVa students with a weighted sampling scheme like so: students &lt;- sample(c(&quot;female&quot;,&quot;male&quot;), 500, replace=TRUE, prob = c(0.53, 0.47)) prop.table(table(students)) ## students ## female male ## 0.54 0.46 rm(students, uva) When used with subsetting brackets, sample() can be used to create training and test sets. For example, say we want to build some sort of predictive model using our training data. We may want to use half our data to build the model and then use the other half to evaluate its performance. train &lt;- sample(nrow(iris), size= nrow(iris)/2) # train is a random sample of numbers from 1 - 365. We can treat these like row numbers. irisTrain &lt;- iris[train,] irisTest &lt;- iris[-train,] # confirm no intersection dplyr::intersect(irisTrain, irisTest) ## [1] Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;0 rows&gt; (or 0-length row.names) rm(train, irisTest, irisTrain) 9.4 generating fixed levels ————————————————- Often generating data means creating a series of fixed levels, such as 10 males and 10 females. The rep() function can be useful for this. Below we replicate 10 each of “M” and “F”: rep(c(&quot;M&quot;,&quot;F&quot;), each=10) ## [1] &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ## [20] &quot;F&quot; rep(c(&quot;M&quot;,&quot;F&quot;), times=10) ## [1] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; ## [20] &quot;F&quot; rep(c(&quot;M&quot;,&quot;F&quot;), length.out = 15) ## [1] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; # or just length, for short rep(c(&quot;M&quot;,&quot;F&quot;), length = 15) ## [1] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; # Notice that all these generated a character vector. To use as a &quot;factor&quot;, we would need to wrap it in the factor() function. factor(rep(c(&quot;M&quot;,&quot;F&quot;), each=10)) ## [1] M M M M M M M M M M F F F F F F F F F F ## Levels: F M # A function specifically for creating factors is the gl() function. gl = # &quot;generate levels&quot;. Below we generate a factor with 2 levels of 10 each and # labels of &quot;M&quot; and &quot;F&quot;. Notice the result is a factor. gl(n = 2, k = 10, labels = c(&quot;M&quot;,&quot;F&quot;)) ## [1] M M M M M M M M M M F F F F F F F F F F ## Levels: M F # A more common occurence is combinations of fixed levels, say gender, # education, and status. A function that helps create every combination of # levels is expand.grid(). Below we generate every combination of the levels # provided for gender, education, and status. Notice the first factors vary # fastest. expand.grid(gender=c(&quot;M&quot;,&quot;F&quot;), education=c(&quot;HS&quot;,&quot;College&quot;,&quot;Advanced&quot;), status=c(&quot;Single&quot;,&quot;Married&quot;,&quot;Divorced&quot;,&quot;Widowed&quot;)) ## gender education status ## 1 M HS Single ## 2 F HS Single ## 3 M College Single ## 4 F College Single ## 5 M Advanced Single ## 6 F Advanced Single ## 7 M HS Married ## 8 F HS Married ## 9 M College Married ## 10 F College Married ## 11 M Advanced Married ## 12 F Advanced Married ## 13 M HS Divorced ## 14 F HS Divorced ## 15 M College Divorced ## 16 F College Divorced ## 17 M Advanced Divorced ## 18 F Advanced Divorced ## 19 M HS Widowed ## 20 F HS Widowed ## 21 M College Widowed ## 22 F College Widowed ## 23 M Advanced Widowed ## 24 F Advanced Widowed # Notice that creates a data frame that we can save: DF &lt;- expand.grid(gender=c(&quot;M&quot;,&quot;F&quot;), education=c(&quot;HS&quot;,&quot;College&quot;,&quot;Advanced&quot;), status=c(&quot;Single&quot;,&quot;Married&quot;,&quot;Divorced&quot;,&quot;Widowed&quot;)) class(DF) ## [1] &quot;data.frame&quot; rm(DF) Or imagine an experiment where 3 people throw 3 different kinds of paper airplanes, made of 3 paper types (3x3 = 9 planes), throwing each plane 8 times. schedule &lt;- expand.grid(thrower=c(&quot;Clay&quot;,&quot;Rod&quot;,&quot;Kevin&quot;), paper=c(&quot;18&quot;, &quot;20&quot;, &quot;24&quot;), design=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), rep=1:8) # Randomize and drop the rep column. The sample(nrow(schedule)) code scrambles the numbers 1 through 216, which I then use to randomly shuffle the schedule of throws. k &lt;- sample(nrow(schedule)) schedule &lt;- schedule[k,1:3] head(schedule, n = 10) ## thrower paper design ## 21 Kevin 18 c ## 88 Clay 24 a ## 173 Rod 18 b ## 162 Kevin 24 c ## 98 Rod 24 b ## 216 Kevin 24 c ## 215 Rod 24 c ## 175 Clay 20 b ## 46 Clay 18 c ## 123 Kevin 20 b # output to csv file for logging &quot;distance flown&quot; data write.csv(schedule, file=&quot;throwLog.csv&quot;, row.names=FALSE) rm(k, schedule) This is a great way to set up an experiment, but I’d like to also add data for the throw, based on interesting distributions (normal, etc.). How would I generate samples for each contestant that was based on slightly different distributions? What sort of distribution? See this page to get a quick refresher on common distributions: https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/. Note also that ?distributions gives you the distributions in {stats}. Persevere for a time. 9.5 generating numerical sequences # The seq() function allows you to generate sequences of numbers: seq(from = 0, to = 10, by = 2) ## [1] 0 2 4 6 8 10 seq(0, 10, 0.2) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 ## [16] 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 ## [31] 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 ## [46] 9.0 9.2 9.4 9.6 9.8 10.0 # Go backwards seq(1000, 0, -100) ## [1] 1000 900 800 700 600 500 400 300 200 100 0 # The seq() function has a length.out argument that allows you to specify the # size of the vector you want to create. It automatically calculates the # increment. We usually just abbreviate to length seq(1, 10, length = 30) ## [1] 1.000000 1.310345 1.620690 1.931034 2.241379 2.551724 2.862069 ## [8] 3.172414 3.482759 3.793103 4.103448 4.413793 4.724138 5.034483 ## [15] 5.344828 5.655172 5.965517 6.275862 6.586207 6.896552 7.206897 ## [22] 7.517241 7.827586 8.137931 8.448276 8.758621 9.068966 9.379310 ## [29] 9.689655 10.000000 # The colon operator(:) also allows you to generate regular sequences in steps # of 1. 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 10:-10 # reverse direction ## [1] 10 9 8 7 6 5 4 3 2 1 0 -1 -2 -3 -4 -5 -6 -7 -8 ## [20] -9 -10 # When used with factors, the colon operator generates an interaction factor: f1 &lt;- gl(n = 2, k = 3); f1 ## [1] 1 1 1 2 2 2 ## Levels: 1 2 f2 &lt;- gl(n = 3, k = 2, labels = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)); f2 ## [1] a a b b c c ## Levels: a b c f1:f2 ## [1] 1:a 1:a 1:b 2:b 2:c 2:c ## Levels: 1:a 1:b 1:c 2:a 2:b 2:c rm(f1,f2) The last step seems akin to perfectly shuffling two decks of cards (the decks must be of equal length). 9.6 seq_along() and seq_len(). seq_along() returns the indices of a vector while seq_len(n) returns an integer vector of 1:n. seq_along(100:120) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 seq_along(state.abb) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 seq_len(12) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 9.7 generating random data from a probability distribution A central idea in inferential statistics is that the distribution of data can often be approximated by a theoretical distribution. R provides functions for working with several well-known theoretical distributions, including the ability to generate data from those distributions. A common one is the rnorm() function which generates data from a Normal distribution. In R, the functions for theoretical distributions take the form of dxxx, pxxx, qxxx and rxxx. dxxx is for the probability density/mass function (dnorm) pxxx is for the cumulative distribution function (pnorm) qxxx is for the quantile function (qnorm) rxxx is for random variate generation (rnorm) For random variate generation we’re interested in the rxxx variety. 9.8 Normal distribution: # 10 random draws from N(100,5) rnorm(n = 10, mean = 100, sd = 5) ## [1] 98.66742 101.16132 102.19461 86.84940 101.51612 99.95910 99.20285 ## [8] 92.69651 100.47816 97.25796 9.9 Binomial distribution: # 10 random draws from b(1,0.5) # AKA, 10 coin flips (size is the number of trials) rbinom(n = 10, size = 1, prob = 0.5) ## [1] 1 1 0 1 1 1 0 1 1 0 # 10 random draws from b(1,0.8) # AKA, 10 coin flips with a coin loaded Heads (or Tails) 80% of time rbinom(n = 10, size = 1, prob = 0.8) ## [1] 0 1 1 0 1 0 1 1 1 0 # 10 random draws from b(10,0.5) # AKA, 10 results of 10 coin flips rbinom(n = 10, size = 10, prob = 0.5) ## [1] 5 3 4 5 8 5 5 3 5 7 # We can use a binomial distribution to simulate dichotmous answers such as # Yes/No or success/fail. Simulate a vector of responses where respondents are 65% likely to say Yes (1) versus No (0) rbinom(n = 10, size = 1, prob = 0.65) ## [1] 0 1 1 1 1 1 1 1 1 0 # could also just use sample sample(c(&quot;Y&quot;,&quot;N&quot;), size = 10, replace = TRUE, prob = c(.65, .35)) ## [1] &quot;Y&quot; &quot;Y&quot; &quot;N&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; 9.10 Uniform distribution # 10 random draws from a uniform distribution u(0,100) runif(10,0,100) ## [1] 81.88379 12.41300 37.03454 68.18687 58.59851 29.09355 38.68486 70.76103 ## [9] 57.85143 26.29435 # A uniform distribution can be good for random sampling. Let&#39;s say we want to sample about 10% of iris data: k &lt;- runif(nrow(iris),0,1) # [0,1] interval is default sbSamp &lt;- iris[k &lt; 0.1, ] # sample about 10% of rows dim(sbSamp) ## [1] 19 5 # dplyr does this as well without the need for runif; and it&#39;s precise in its # sampling fraction. sbSamp &lt;- dplyr::sample_frac(iris, 0.1) # sample exactly 10% of rows dim(sbSamp) ## [1] 15 5 rm(sbSamp, k) 9.11 Sampling from multiple distributions (building in a “difference”) The arguments to rxxx functions can take vectors! This means we can use one function call to generate draws from multiple distributions. # alternating random values from N(10,4) and N(100,40) rnorm(10, mean = c(2,100),sd = c(2,40)) ## [1] 5.328161 66.468495 1.823437 119.813332 -1.211117 110.227283 ## [7] 3.420011 89.456878 -1.939389 102.805432 # 30 random draws, 10 each from N(10,4), N(90,4) and N(400,4) rnorm(30, mean = rep(c(10,90,400),each=10), sd = 4) ## [1] 11.848625 7.062857 10.235013 15.881340 12.131422 11.880648 ## [7] 8.966321 19.317164 5.844385 10.035814 94.940950 89.576164 ## [13] 90.230197 85.438534 93.371686 88.581047 91.377628 96.031555 ## [19] 91.066694 90.106946 403.013600 404.661590 401.158407 395.135567 ## [25] 395.681059 399.562900 403.008095 398.163003 399.153346 401.710149 # 100 random draws, 50 each from b(5,0.5) and b(50,0.5) rbinom(n = 100, size = rep(c(5,50),each=50), prob = 0.5) ## [1] 2 1 1 3 2 3 3 2 4 3 3 1 1 2 3 3 1 3 5 3 2 2 1 4 1 ## [26] 2 4 2 2 2 1 1 2 1 2 3 2 2 2 1 3 5 3 1 2 3 2 1 3 3 ## [51] 22 28 24 25 17 29 24 29 24 21 27 17 18 25 27 28 24 21 21 19 24 24 30 26 25 ## [76] 25 24 26 19 21 25 30 21 24 23 28 29 26 24 21 20 25 30 24 25 27 28 28 25 25 # Combined with matrix(), one can generate &quot;multiple&quot; random samples from a # distribution. For example, draw 5 random samples of size 10 from a N(10,1): matrix(rnorm(10*5,10,1),ncol=5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 9.036184 8.228816 8.428310 10.091784 10.065903 ## [2,] 11.913161 10.620178 9.533036 9.962295 10.750959 ## [3,] 10.225506 9.167468 10.672955 9.281624 8.699020 ## [4,] 8.944468 9.821513 9.451991 9.983957 9.879833 ## [5,] 10.245307 10.086259 7.673951 8.919391 7.219819 ## [6,] 9.172797 9.202411 9.046613 10.015565 11.270271 ## [7,] 10.881257 9.188296 7.803665 10.467794 11.208078 ## [8,] 9.592817 10.217848 10.368692 10.538772 10.116299 ## [9,] 9.986472 10.271840 10.405884 9.796671 9.990211 ## [10,] 10.512477 9.200774 9.942726 10.109449 12.321032 Note that in the last example, we technically drew one sample of size 50 and then laid it out in a 10x5 matrix. 9.12 The good stuff: building in a difference based on a categorical variable Using ifelse() we can generate different data based on a TRUE/FALSE condition. Let’s say we have treated and untreated subjects. I’d like to generate Normal data that differs based on the treatment. trtmt &lt;- sample(c(&quot;Treated&quot;,&quot;Untreated&quot;), size = 20, replace = TRUE) ifelse(trtmt==&quot;Treated&quot;, yes = rnorm(20, 10, 1), no = rnorm(20, 20, 1)) ## [1] 20.723912 9.103286 20.321116 9.547927 11.594527 9.951724 20.725621 ## [8] 19.123367 10.712649 10.387431 11.443424 10.326516 20.600475 19.225502 ## [15] 12.802787 20.061200 18.900170 9.107065 10.968940 19.913210 Notice we have to make the length of the yes/no arguments the SAME LENGTH as the trtmt==“Treated” logical vector! What happens if we use rnorm(n=1,…)? # What about more than two groups? n &lt;- 200 trtmt &lt;- sample(LETTERS[1:6], size = n, replace = TRUE) # Say we want to generate differnt Normal data for each group. One way is to do a for-loop with multiple if statements: val &lt;- numeric(n) # empty vector for(i in seq_along(trtmt)){ if(trtmt[i]==&quot;A&quot;) val[i] &lt;- rnorm(1, 10, 2) else if(trtmt[i]==&quot;B&quot;) val[i] &lt;- rnorm(1, 20, 4) else if(trtmt[i]==&quot;C&quot;) val[i] &lt;- rnorm(1, 30, 6) else if(trtmt[i]==&quot;D&quot;) val[i] &lt;- rnorm(1, 40, 8) else if(trtmt[i]==&quot;E&quot;) val[i] &lt;- rnorm(1, 50, 10) else val[i] &lt;- rnorm(1, 60, 12) } val ## [1] 46.428174 44.149738 57.184741 9.444044 21.952955 40.899507 18.971545 ## [8] 67.328571 8.829315 23.373316 40.745680 35.435106 22.076307 43.229001 ## [15] 9.933163 63.906776 11.864437 37.008983 36.467848 30.492950 8.892441 ## [22] 25.672618 31.616476 10.657223 8.788681 34.070005 28.740924 10.902777 ## [29] 12.561193 40.689616 48.250545 40.021587 75.263679 24.693692 19.667534 ## [36] 9.904587 15.224703 57.870358 24.683829 39.118094 40.870632 8.263384 ## [43] 48.458739 28.779333 34.003611 12.509631 22.316084 46.162946 25.784933 ## [50] 22.618044 30.772797 43.094284 48.876835 17.692924 20.991486 7.169333 ## [57] 24.267032 24.119535 25.705284 22.971583 24.535180 26.107435 51.398646 ## [64] 43.035241 52.284173 17.363235 30.782361 63.252764 30.796179 43.137001 ## [71] 54.610548 45.011287 36.655019 52.862881 41.153461 42.421820 26.524985 ## [78] 34.965579 12.135669 45.094289 51.411124 45.331861 33.654875 34.265608 ## [85] 8.905355 10.811275 66.883548 31.759255 67.279860 19.700615 46.482875 ## [92] 62.759488 17.019218 14.568455 49.128917 24.946838 9.148043 68.921827 ## [99] 10.273990 72.894250 38.748923 19.193598 9.934198 53.964612 23.631532 ## [106] 34.327352 41.509801 7.875897 77.793672 8.115391 60.009636 58.913676 ## [113] 17.612787 40.103619 50.379651 10.133235 18.808507 25.141638 23.825021 ## [120] 33.311633 60.245335 38.960925 62.230869 8.032158 61.796108 12.053449 ## [127] 61.890543 9.581448 40.886643 17.785047 43.934795 51.361943 41.651089 ## [134] 50.673786 32.345359 60.682757 32.732852 20.205942 10.714730 33.482836 ## [141] 13.743926 62.813966 21.270051 67.357492 11.149672 32.115698 41.641582 ## [148] 41.262360 19.833400 32.546026 12.723434 33.186941 15.858358 25.123512 ## [155] 12.186821 35.192922 39.477760 52.813907 76.205267 58.630979 41.665793 ## [162] 34.020803 52.238890 14.895215 21.571451 49.759234 34.510709 11.315521 ## [169] 6.851515 29.957946 24.752574 31.406862 9.867611 10.730757 10.398206 ## [176] 52.019241 7.746008 71.048458 22.342669 43.450229 30.747764 30.276315 ## [183] 9.931689 11.477259 33.096321 25.101114 9.361784 35.146203 41.400045 ## [190] 56.273348 61.415905 10.008457 9.709782 34.436554 52.076178 10.826817 ## [197] 76.394982 30.280178 52.615341 56.993582 A more R-like way would be to take advantage of vectorized functions. First create a data frame with one row for each group and the mean and standard deviations we want to use to generate the data for that group. dat &lt;- data.frame(g=LETTERS[1:6],mean=seq(10,60,10),sd=seq(2,12,2)) dat ## g mean sd ## 1 A 10 2 ## 2 B 20 4 ## 3 C 30 6 ## 4 D 40 8 ## 5 E 50 10 ## 6 F 60 12 dat is currently a petite little dataframe of 6 rows. Now sample the row numbers (1 - 6) WITH replacement. We can use these to randomly sample the data frame rows. ASIDE: Recall that we can repeatedly call a row or element using subsetting brackets. For example, call the first row of iris 5 times: iris[c(1,1,1,1,1),] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 1.1 5.1 3.5 1.4 0.2 setosa ## 1.2 5.1 3.5 1.4 0.2 setosa ## 1.3 5.1 3.5 1.4 0.2 setosa ## 1.4 5.1 3.5 1.4 0.2 setosa Let’s exploit that to randomly sample with replacement our data frame of groups: n &lt;- 200 k &lt;- sample(1:6, n, replace = TRUE) dat &lt;- dat[k,] str(dat) ## &#39;data.frame&#39;: 200 obs. of 3 variables: ## $ g : chr &quot;E&quot; &quot;E&quot; &quot;E&quot; &quot;F&quot; ... ## $ mean: num 50 50 50 60 30 50 60 60 10 30 ... ## $ sd : num 10 10 10 12 6 10 12 12 2 6 ... # Now generate our data for each group using ONE call to rnorm. dat$vals &lt;- rnorm(n, mean=dat$mean, sd=dat$sd) head(dat) ## g mean sd vals ## 5 E 50 10 63.35170 ## 5.1 E 50 10 49.93744 ## 5.2 E 50 10 50.23682 ## 6 F 60 12 32.64805 ## 3 C 30 6 36.57889 ## 5.3 E 50 10 51.80706 This is pretty neat. We go from one little dataframe to a larger one in a few lines of code. Mean and SD can be varied by the class, “g” in this case. 9.13 A demonstration of the Central Limit Theorem The Central Limit Theorem states that the sum of a large number of independent random variables will be approximately normally distributed almost regardless of their individual distributions. We can demonstrate this using various rxxx functions. # sum 6 values from 6 different distributions (sample size = 6) n &lt;- 1e4 # simulate 1000 times clt &lt;- rexp(n, rate = 1) + rbinom(n,10,0.4) + rchisq(n,df = 6) + rnorm(n, 12, 12) + rpois(n, lambda = 3) + rt(n, df = 7) hist(clt, freq=FALSE) # overlay a normal density curve X &lt;- seq(min(clt),max(clt),length = 500) # x Y &lt;- dnorm(X, mean = mean(clt), sd = sd(clt)) # f(x) = dnorm lines(X,Y,type = &quot;l&quot;, col=&quot;blue&quot;) # plot (x,y) coordinates as a &quot;blue&quot; line (&quot;l&quot;) rm(X, Y, clt) Let’s unpack some of this: clt1 &lt;- rexp(n, rate = 1) hist(clt1, freq=FALSE) clt2 &lt;- rbinom(n,10,0.4) hist(clt2, freq=FALSE) clt3 &lt;- rchisq(n,df = 6) hist(clt3, freq=FALSE) clt4 &lt;-rnorm(n, 12, 12) hist(clt4, freq=FALSE) clt5 &lt;- rpois(n, lambda = 3) hist(clt5, freq=FALSE) clt6 &lt;- rt(n, df = 7) hist(clt6, freq=FALSE) # All of this base R graphing is clunky and doesn&#39;t lend itself to modification as well as ggplot() figures. library(tidyverse) df &lt;- data.frame(clt1, clt2, clt3, clt4, clt5, clt6) df %&gt;% ggplot(aes(clt3)) + geom_histogram(bins = 30) df %&gt;% ggplot(aes(clt3)) + geom_density() 9.14 Overlaying normal curve on histogram The following solution was on StackOverflow at https://stackoverflow.com/questions/6967664/ggplot2-histogram-with-normal-curve set.seed(1) df1 &lt;- data.frame(PF = 10*rnorm(1000)) ggplot(df1, aes(x = PF)) + geom_histogram(aes(y =..density..), breaks = seq(-40, 40, by = 5), colour = &quot;black&quot;, fill = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean(df1$PF), sd = sd(df1$PF)), color = &quot;blue&quot;) From the {ggplot2} help: “This stat makes it easy to superimpose a function on top of an existing plot. The function is called with a grid of evenly spaced values along the x axis, and the results are drawn (by default) with a line.” Note how stat_function() lends itself to quick addition: simply feed the correct fun and args to the function. Now accomplish this for clt3 ggplot(df, aes(x = clt3)) + geom_histogram(aes(y =..density..), breaks = seq(0, 30, by = 1), colour = &quot;black&quot;, fill = &quot;white&quot;) + stat_function(fun = dchisq, args = list(df = 6), color = &quot;blue&quot;) + labs(title = &quot;chi-squared distribution&quot;) Note how the args in dchisq includes only the df = 6. No mean needs to be calculated (as in dnorm). Now accomplish this for clt1 ggplot(df, aes(x = clt1)) + geom_histogram(aes(y =..density..), breaks = seq(0, 8, by = .5), colour = &quot;black&quot;, fill = &quot;white&quot;) + stat_function(fun = dexp, args = list(rate = 1), color = &quot;blue&quot;) + labs(title = &quot;exponential distribution&quot;) 9.15 Crossing trial From David Robinson birthday paradox Rblogger at https://www.r-bloggers.com/the-birthday-paradox-puzzle-tidy-simulation-in-r/ summarized &lt;- crossing(people = seq(2, 50, 2), trial = 1:100) %&gt;% mutate(birthday = map(people, ~ sample(365, .x, replace = TRUE)), multiple = map_lgl(birthday, ~ any(duplicated(.x)))) %&gt;% group_by(people) %&gt;% summarize(chance = mean(multiple)) ggplot(summarized, aes(people, chance)) + geom_line() + scale_y_continuous(labels = scales::percent_format()) + labs(y = &quot;Probability two have the same birthday&quot;) # Checking the work with pbirthday function summarized %&gt;% mutate(exact = map_dbl(people, pbirthday)) %&gt;% ggplot(aes(people, chance)) + geom_line() + geom_line(aes(y = exact), lty = 2, color = &quot;blue&quot;) + scale_y_continuous(labels = scales::percent_format()) + labs(y = &quot;Probability two have the same birthday&quot;) "],["functions.html", "Chapter 10 Functions 10.1 A little Dungeons &amp; Dragons (tm) to spice it up 10.2 Geometric and harmonic means 10.3 for loop 10.4 case_when() 10.5 Compare this with if_else()", " Chapter 10 Functions library(tidyverse) From R4DS and the Solution book for R4DS. Make functions to calculate the coefficient of variation (cV) and the variance (xVar) of a vector. x &lt;- c(1:10, rep(NA,3)) cV &lt;- function(x){ sd(x, na.rm = TRUE)/mean(x, na.rm = TRUE) } cV(x) ## [1] 0.5504819 xVar &lt;- function(x) { n &lt;- length(x[!is.na(x)]) m &lt;- mean(x, na.rm = TRUE) sq_err &lt;- (x - m)^2 sum(sq_err, na.rm = TRUE) / (n - 1) } xVar(x) ## [1] 9.166667 The interesting thing is that these functions are exceptionally modular. By working stepwise through the calculation, it’s possible to take a much more difficult or obscure problem and “unpack” it. 10.1 A little Dungeons &amp; Dragons (tm) to spice it up Adapted from Richie Cotton’s DataCamp introductory course on function writing. The following example shows how a function can emulate the rolling of a die, for instance, in D&amp;D. roll_die &lt;- function(n_die){ die_sides &lt;- 1:6 sample(die_sides, n_die, replace = TRUE) } roll_die(6) ## [1] 6 5 1 6 1 2 Let’s apply some of these simple ideas to D&amp;D. Say I play a character that can cast the sleep spell. This scales to different levels. At first level, one rolls 5 six-sided die. sleep &lt;- function(){ sum(sample(1:8, size = 5, replace = TRUE)) } sleep() ## [1] 27 If cast at higher levels, the spell adds 2 die per level above first: 7 die at second level, 9 die at third level, etc. Let’s address this by adding a level variable to our function. sleep_scaled &lt;- function(level){ if(level &gt; 1){add_die = (level-1)*2 } else{ add_die = 0 } sum(sample(1:8, size = 5 + add_die, replace = TRUE)) } A couple of things can be added to avoid the following goofy mishaps (or at least render useful error codes: sleep_scaled(0) ## [1] 24 # sleep_scaled(&quot;rex&quot;) # removed so that **knitr** doesn&#39;t choke on this First we load Richie Cotton’s package {assertive}. library(assertive) Then we get to work on the function. Note how assert_is_numeric() generates an automatic message if a non-numeric input is handed to the function. More refined rules that include custom error messages can also be used, as in the if(){} statement, below. sleep_scaled &lt;- function(level = 1){ assert_is_numeric(level) if(any(is_non_positive(level))){ return(&quot;x contains nonpositive values, so the spell makes no sense&quot;) } if(level &gt; 1){add_die = (level-1)*2 } else{ add_die = 0 } sum(sample(1:8, size = 5 + add_die, replace = TRUE)) } sleep_scaled(0) ## [1] &quot;x contains nonpositive values, so the spell makes no sense&quot; sleep_scaled() ## [1] 26 sleep_scaled(2) ## [1] 32 # sleep_scaled(&quot;joseph&quot;) # removed so that **knitr** doesn&#39;t choke on this Now, it would be useful to see the distribution of these rolls, to predict whether the spell would be effective, say, on a horde of kobolds. hist(replicate(10000, sleep_scaled()), breaks = 70) 10.2 Geometric and harmonic means Good examples on functions. Also taken from Richie Cotton’s DataCamp introductory course on function writing. Geometric mean (GM) is calculated differently than the arithmetic mean (AM), and is often used in investment. Outliers are dampened in the geometric mean. Notice, however, that only positive numbers may be examined with geometric mean. The harmonic mean (HM) is useful for sets of numbers defined in relationship to some unit, as in the case of speed (in m/sec, for instance). Note the following relationship: AM &gt;= GM &gt;= HM x &lt;- c(1,2,3,4,4,12, NA) y &lt;- c(1,2,3,4,4,12,-2,NA) mean(x, na.rm = TRUE) ## [1] 4.333333 mean(y, na.rm = TRUE) ## [1] 3.428571 log(x) %&gt;% mean() %&gt;% exp() ## [1] NA log(x) ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.3862944 2.4849066 NA exp(x) ## [1] 2.718282e+00 7.389056e+00 2.008554e+01 5.459815e+01 5.459815e+01 ## [6] 1.627548e+05 NA calc_geom_mean &lt;- function(x, na.rm = TRUE){ log(x) %&gt;% mean(na.rm =na.rm) %&gt;% exp() } calc_geom_mean(x) ## [1] 3.237741 calc_geom_mean(y) ## Warning in log(x): NaNs produced ## [1] 3.237741 get_reciprocal &lt;- function(x) { 1 / x } calc_harmonic_mean &lt;- function(x, na.rm = TRUE) { x %&gt;% get_reciprocal() %&gt;% mean(na.rm = na.rm) %&gt;% get_reciprocal() } calc_harmonic_mean(x) ## [1] 2.482759 calc_harmonic_mean(y) ## [1] 3.652174 10.3 for loop z &lt;- 1:3 total &lt;- 0 for(value in z){ total &lt;- total + value } total ## [1] 6 10.4 case_when() case_when() from Hadley Wickhams’s dplyr https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/case_when x &lt;- 1:50 y &lt;- 51:100 df &lt;- data.frame(x,y) df ## x y ## 1 1 51 ## 2 2 52 ## 3 3 53 ## 4 4 54 ## 5 5 55 ## 6 6 56 ## 7 7 57 ## 8 8 58 ## 9 9 59 ## 10 10 60 ## 11 11 61 ## 12 12 62 ## 13 13 63 ## 14 14 64 ## 15 15 65 ## 16 16 66 ## 17 17 67 ## 18 18 68 ## 19 19 69 ## 20 20 70 ## 21 21 71 ## 22 22 72 ## 23 23 73 ## 24 24 74 ## 25 25 75 ## 26 26 76 ## 27 27 77 ## 28 28 78 ## 29 29 79 ## 30 30 80 ## 31 31 81 ## 32 32 82 ## 33 33 83 ## 34 34 84 ## 35 35 85 ## 36 36 86 ## 37 37 87 ## 38 38 88 ## 39 39 89 ## 40 40 90 ## 41 41 91 ## 42 42 92 ## 43 43 93 ## 44 44 94 ## 45 45 95 ## 46 46 96 ## 47 47 97 ## 48 48 98 ## 49 49 99 ## 50 50 100 case_when( x %% 35 == 0 ~ &quot;fizz buzz&quot;, x %% 5 == 0 ~ &quot;fizz&quot;, x %% 7 == 0 ~ &quot;buzz&quot;, TRUE ~ as.character(x) ) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;fizz&quot; &quot;6&quot; ## [7] &quot;buzz&quot; &quot;8&quot; &quot;9&quot; &quot;fizz&quot; &quot;11&quot; &quot;12&quot; ## [13] &quot;13&quot; &quot;buzz&quot; &quot;fizz&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; ## [19] &quot;19&quot; &quot;fizz&quot; &quot;buzz&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; ## [25] &quot;fizz&quot; &quot;26&quot; &quot;27&quot; &quot;buzz&quot; &quot;29&quot; &quot;fizz&quot; ## [31] &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;fizz buzz&quot; &quot;36&quot; ## [37] &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;fizz&quot; &quot;41&quot; &quot;buzz&quot; ## [43] &quot;43&quot; &quot;44&quot; &quot;fizz&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; ## [49] &quot;buzz&quot; &quot;fizz&quot; 10.5 Compare this with if_else() if_else(x %% 2 == 0, &quot;even&quot;, &quot;odd&quot;) ## [1] &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; ## [11] &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; ## [21] &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; ## [31] &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; ## [41] &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; &quot;odd&quot; &quot;even&quot; "],["correlation.html", "Chapter 11 Correlation Plots", " Chapter 11 Correlation Plots library(tidyverse) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa iris %&gt;% select(-Species) %&gt;% cor() ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 M &lt;- iris %&gt;% select(-Species) %&gt;% cor(method = &quot;kendall&quot;) corrplot::corrplot(M) corrplot::corrplot(M, method = &quot;color&quot;) corrplot::corrplot(M, method = &quot;color&quot;, type = &quot;upper&quot;) corrplot::corrplot(M, method = &quot;color&quot;, type = &quot;upper&quot;, order = &quot;hclust&quot;) corrplot::corrplot(M, method = &quot;color&quot;, type = &quot;upper&quot;, order = &quot;hclust&quot;, addCoef.col = &quot;black&quot;) corrplot::corrplot(M, method = &quot;color&quot;, type = &quot;upper&quot;, order = &quot;hclust&quot;, addCoef.col = &quot;black&quot;, tl.col=&quot;black&quot;) corrplot::corrplot(M, method = &quot;color&quot;, type = &quot;upper&quot;, order = &quot;hclust&quot;, addCoef.col = &quot;black&quot;, tl.col=&quot;black&quot;, tl.srt = 45) "],["dimensionalityreduction.html", "Chapter 12 Dimensionality Reduction 12.1 Background 12.2 Experiment in showing dimensionality reduction for pathologists", " Chapter 12 Dimensionality Reduction 12.1 Background Most complex diagnosis is conducted without recourse to perfect prevalence data, or to the characteristics of available tests. Indeed, we often don’t know which variables are relevant. An expert in pathology is defined, more or less, by an intuitive understanding of prevalence and the properties of tests which can be used to classify diseases. The process by which pathologists classify disease is often poorly understood. Magical thinking and arguments from authority infuse the process, particularly at the edges of what is known–especially when disease is rare and evidence is weak. Careers in diagnostic pathology were formerly made by adding a new rare disease to the ever-growing bestiary. Beginners often stumble when trying to visualize the testing process, and how classes are established. Dimension reduction methods are useful to prioritize variables (factors), and begin the iterative process of classification-test validation-clinical testing, it is often useful to see which ones permit the clearest distinction between clusters in high dimensional spaces–like those created when numerous tests are deployed to establish a single diagnosis. Resources to understand dimensionality reduction: https://www.datacamp.com/community/tutorials/introduction-t-sne http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/ library(tidyverse) library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout 12.2 Experiment in showing dimensionality reduction for pathologists The first step is to build three separate “clouds” of data that do not overlap in three dimensions, but which do overlap in two dimensions. df &lt;- tibble( diagnostic_class = rep(&quot;A&quot;, 100), x = rnorm(100, mean = 1, sd = 0.1), y = rnorm(100, mean = 1, sd = 0.1), z = rnorm(100, mean = 1, sd = 0.1) ) df1 &lt;- tibble( diagnostic_class = rep(&quot;B&quot;, 100), x = rnorm(100, mean = 1, sd = 0.1), y = rnorm(100, mean = 2, sd = 0.1), z = rnorm(100, mean = 1, sd = 0.1) ) df2 &lt;- tibble( diagnostic_class = rep(&quot;C&quot;, 100), x = rnorm(100, mean = 1, sd = 0.1), y = rnorm(100, mean = 1, sd = 0.1), z = rnorm(100, mean = 2, sd = 0.1) ) df &lt;- df %&gt;% rbind(df1) %&gt;% rbind(df2) The following 2D and 3D plots make it clear that the diagnostic class is only obvious when one takes into account the z-axis. df %&gt;% ggplot(aes(x,y, color = diagnostic_class)) + geom_point() plot_ly(x=df$x, y=df$y, z=df$z, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, color = df$diagnostic_class) 12.2.1 Principle component analysis PCA concentrates on placing dissimilar data points far apart in a lower dimension representation. Some useful websites for principle component analysis: http://huboqiang.cn/2016/03/03/RscatterPlotPCA library(ggfortify) autoplot(prcomp(df[,2:4]), data = df, colour = &#39;diagnostic_class&#39;, size = 8) 12.2.2 t-distributed stochastic neighbor embedding library(Rtsne) df_unique &lt;- unique(df) #removes duplicates df_matrix &lt;- as.matrix(df[,2:4]) set.seed(2020) tsne_out &lt;- Rtsne(df_matrix) tsne_plot &lt;- data.frame(x = tsne_out$Y[,1], y = tsne_out$Y[,2], col = df$diagnostic_class) ggplot(tsne_plot) + geom_point(aes(x=x, y=y, color=col)) "],["clustering.html", "Chapter 13 Clustering", " Chapter 13 Clustering The page by Rohan Joseph does a really nice job illustrative hierarchical clustering: https://medium.com/@rohanjoseph_91119/learn-with-an-example-hierarchical-clustering-873b5b50890c Joseph uses a one dimensional dataset to show the difference between types of hierarchical clustering. Joseph’s hand-drawn calculations actually make the distinctionb between single and complete linkage very well. library(tidyverse) x &lt;- c(7,10,20,28,35) #Joseph&#39;s 1D dataset The first step is to make a distance matrix between the elements of the dataset. In one dimension, this is accomplished by subtraction. d &lt;- dist(x) d ## 1 2 3 4 ## 2 3 ## 3 13 10 ## 4 21 18 8 ## 5 28 25 15 7 Before calling the hclust function, it’s helpful to know the methods. Single Linkage : merge in each step the two clusters, whose two closest members have the smallest distance. Complete Linkage : merge the members of the clusters in each step which provide the smallest maximum pairwise distance. hclust(d) ## ## Call: ## hclust(d = d) ## ## Cluster method : complete ## Distance : euclidean ## Number of objects: 5 plot(hclust(d)) Note that the default method is complete. hclust(d, method = &quot;single&quot;) ## ## Call: ## hclust(d = d, method = &quot;single&quot;) ## ## Cluster method : single ## Distance : euclidean ## Number of objects: 5 plot(hclust(d, method = &quot;single&quot;)) "],["regex.html", "Chapter 14 Regular expressions 14.1 Seeking patterns", " Chapter 14 Regular expressions 14.1 Seeking patterns I’m not very good at manipulating regular expressions, so a page to practice manipulating them seems like a good idea. The RStudio cheat sheet is a nice place to start: https://rstudio.com/resources/cheatsheets/ Or Hadley Wickham and Garrett Grolemund’s R4DS chapter on strings: https://r4ds.had.co.nz/strings.html string &lt;- c(&quot;Hiphopopotamus&quot;, &quot;Rhymenoceros&quot;, &quot;time for bottomless lyrics&quot;) pattern &lt;- &quot;t.m&quot; grep(pattern, string) ## [1] 1 3 grep(pattern, string, value = TRUE) ## [1] &quot;Hiphopopotamus&quot; &quot;time for bottomless lyrics&quot; grepl(pattern, string) ## [1] TRUE FALSE TRUE stringr::str_detect(string, pattern) ## [1] TRUE FALSE TRUE regexpr(pattern, string) ## [1] 10 -1 1 ## attr(,&quot;match.length&quot;) ## [1] 3 -1 3 ## attr(,&quot;index.type&quot;) ## [1] &quot;chars&quot; ## attr(,&quot;useBytes&quot;) ## [1] TRUE # This finds the starting position and the length of the 1st match gregexpr(pattern, string) ## [[1]] ## [1] 10 ## attr(,&quot;match.length&quot;) ## [1] 3 ## attr(,&quot;index.type&quot;) ## [1] &quot;chars&quot; ## attr(,&quot;useBytes&quot;) ## [1] TRUE ## ## [[2]] ## [1] -1 ## attr(,&quot;match.length&quot;) ## [1] -1 ## attr(,&quot;index.type&quot;) ## [1] &quot;chars&quot; ## attr(,&quot;useBytes&quot;) ## [1] TRUE ## ## [[3]] ## [1] 1 13 ## attr(,&quot;match.length&quot;) ## [1] 3 3 ## attr(,&quot;index.type&quot;) ## [1] &quot;chars&quot; ## attr(,&quot;useBytes&quot;) ## [1] TRUE # This finds starting position and length of all matches stringr::str_locate(string, pattern) ## start end ## [1,] 10 12 ## [2,] NA NA ## [3,] 1 3 # This finds starting and end position of first match stringr::str_locate_all(string, pattern) ## [[1]] ## start end ## [1,] 10 12 ## ## [[2]] ## start end ## ## [[3]] ## start end ## [1,] 1 3 ## [2,] 13 15 # This finds starting and end position of all matches It seems like dates and combined alphanumeric strings are frequently subjected to the gaze of grep and company. string2 &lt;- c(&quot;123.abc&quot;, &quot;09/28/2020&quot;, &quot;ABC..123&quot;, &quot;...&quot;) grep(pattern = &quot;[a-z]+&quot;, x = string2) ## [1] 1 grepl(pattern = &quot;[a-z]+&quot;, x = string2) ## [1] TRUE FALSE FALSE FALSE From the Intermediate R DataCamp course: rand_w &lt;- c(&quot;A_2_4b3c&quot;, &quot;Gi2/3_5&quot;) sub(pattern = &quot;[a-zA-Z]&quot;, replacement = &quot;X&quot;, x = rand_w) ## [1] &quot;X_2_4b3c&quot; &quot;Xi2/3_5&quot; # sub replaces only the 1st instance of the match per string gsub(pattern = &quot;[a-zA-Z]&quot;, replacement = &quot;X&quot;, x = rand_w) ## [1] &quot;X_2_4X3X&quot; &quot;XX2/3_5&quot; # gsub replaces all instances of the match per string stringr markedly alters the base R process. To get a handle on it, R4DS comes to the rescue. For instance. Special characters have to be escaped. These include, \", ', \\, \\n, \\t. For a complete list of these characters, see ?\"'\". string1 &lt;- &quot;This is a string&quot; double_quote &lt;- &quot;\\&quot;&quot; # or &#39;&quot;&#39; x &lt;- c(&quot;\\&quot;&quot;, &quot;\\\\&quot;) x ## [1] &quot;\\&quot;&quot; &quot;\\\\&quot; writeLines(x) ## &quot; ## \\ "],["census.html", "Chapter 15 US_Census_Data_Kyle_Walker_Presentation_YouTube 15.1 Variables 15.2 Part 2: Wrangling Census Data with tidyverse Tools", " Chapter 15 US_Census_Data_Kyle_Walker_Presentation_YouTube These are notes from a course taught by Kyle Walker, PhD and posted to YouTube by John DeWitt at the following YouTube https://www.youtube.com/watch?v=PnFJfuJ83NI library(tidyverse) library(tidycensus) library(plotly) # install a new API key if necessary by obtaining a new key and installing it with the following # census_api_key(&quot;aec016261a3b069f2318c77075e5224445517668&quot;, install = TRUE) Let’s see if this will connect me with the census data with a few simple queries. get_decennial() defaults to the latest 10-year, simple survey performed by the Census Bureau. The 10-year survey renders numbers. get_acs() contains much more information, and is based on the latest American Community Survey. ACS variables are rendered as estimates with a margin of error (MOE). pop10 &lt;- get_decennial(geography = &quot;state&quot;, variables = &quot;P001001&quot;) ## Getting data from the 2010 decennial Census ## Using Census Summary File 1 pop10 ## # A tibble: 52 × 4 ## GEOID NAME variable value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01 Alabama P001001 4779736 ## 2 02 Alaska P001001 710231 ## 3 04 Arizona P001001 6392017 ## 4 05 Arkansas P001001 2915918 ## 5 06 California P001001 37253956 ## 6 22 Louisiana P001001 4533372 ## 7 21 Kentucky P001001 4339367 ## 8 08 Colorado P001001 5029196 ## 9 09 Connecticut P001001 3574097 ## 10 10 Delaware P001001 897934 ## # … with 42 more rows income_15to19 &lt;- get_acs(geography = &quot;state&quot;, variables = &quot;B19013_001&quot;) ## Getting data from the 2015-2019 5-year ACS income_19 &lt;- get_acs(geography = &quot;state&quot;, variables = &quot;B19013_001&quot;, survey = &quot;acs1&quot;) ## The 1-year ACS provides data for geographies with populations of 65,000 and greater. ## Getting data from the 2019 1-year ACS That looks great, but would be very labor intensive to assemble piecemeal. Luckily, common tables of variables are available. age_table&lt;- get_acs(geography = &quot;state&quot;, table = &quot;B01001&quot;) ## Getting data from the 2015-2019 5-year ACS ## Loading ACS5 variables for 2019 from table B01001. To cache this dataset for faster access to ACS tables in the future, run this function with `cache_table = TRUE`. You only need to do this once per ACS dataset. age_table ## # A tibble: 2,548 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Alabama B01001_001 4876250 NA ## 2 01 Alabama B01001_002 2359355 1270 ## 3 01 Alabama B01001_003 149090 704 ## 4 01 Alabama B01001_004 153494 2290 ## 5 01 Alabama B01001_005 158617 2274 ## 6 01 Alabama B01001_006 98257 468 ## 7 01 Alabama B01001_007 64980 834 ## 8 01 Alabama B01001_008 35870 1436 ## 9 01 Alabama B01001_009 35040 1472 ## 10 01 Alabama B01001_010 95065 1916 ## # … with 2,538 more rows That’s great, but the variables are still encoded in the long form of the table. We’ll need to solve that at some point. wi_income &lt;- get_acs(geography = &quot;county&quot;, variables = &quot;B19013_001&quot;, state = &quot;WI&quot;, year = 2019) ## Getting data from the 2015-2019 5-year ACS wi_income ## # A tibble: 72 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 55001 Adams County, Wisconsin B19013_001 46369 1834 ## 2 55003 Ashland County, Wisconsin B19013_001 42510 2858 ## 3 55005 Barron County, Wisconsin B19013_001 52703 2104 ## 4 55007 Bayfield County, Wisconsin B19013_001 56096 1877 ## 5 55009 Brown County, Wisconsin B19013_001 62340 1112 ## 6 55011 Buffalo County, Wisconsin B19013_001 57829 1873 ## 7 55013 Burnett County, Wisconsin B19013_001 52672 1388 ## 8 55015 Calumet County, Wisconsin B19013_001 75814 2425 ## 9 55017 Chippewa County, Wisconsin B19013_001 59742 1759 ## 10 55019 Clark County, Wisconsin B19013_001 54012 1223 ## # … with 62 more rows Querying by census tract is also possible. Census tracts are loosely analogous to neighborhoods and contain about 4000 people. dane_income &lt;- get_acs(geography = &quot;tract&quot;, variables = &quot;B19013_001&quot;, state = &quot;WI&quot;, county = &quot;Dane&quot;) ## Getting data from the 2015-2019 5-year ACS dane_income ## # A tibble: 107 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 55025000100 Census Tract 1, Dane County, Wisconsin B19013_… 72471 12984 ## 2 55025000201 Census Tract 2.01, Dane County, Wisconsin B19013_… 94821 11860 ## 3 55025000202 Census Tract 2.02, Dane County, Wisconsin B19013_… 84145 7021 ## 4 55025000204 Census Tract 2.04, Dane County, Wisconsin B19013_… 79617 11823 ## 5 55025000205 Census Tract 2.05, Dane County, Wisconsin B19013_… 91326 13453 ## 6 55025000300 Census Tract 3, Dane County, Wisconsin B19013_… 53778 7593 ## 7 55025000401 Census Tract 4.01, Dane County, Wisconsin B19013_… 98178 7330 ## 8 55025000402 Census Tract 4.02, Dane County, Wisconsin B19013_… 107440 6585 ## 9 55025000405 Census Tract 4.05, Dane County, Wisconsin B19013_… 68911 4141 ## 10 55025000406 Census Tract 4.06, Dane County, Wisconsin B19013_… 74489 10451 ## # … with 97 more rows 15.1 Variables vars &lt;- load_variables(2019, &quot;acs5&quot;) vars ## # A tibble: 27,040 × 3 ## name label concept ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 B01001_001 Estimate!!Total: SEX BY AGE ## 2 B01001_002 Estimate!!Total:!!Male: SEX BY AGE ## 3 B01001_003 Estimate!!Total:!!Male:!!Under 5 years SEX BY AGE ## 4 B01001_004 Estimate!!Total:!!Male:!!5 to 9 years SEX BY AGE ## 5 B01001_005 Estimate!!Total:!!Male:!!10 to 14 years SEX BY AGE ## 6 B01001_006 Estimate!!Total:!!Male:!!15 to 17 years SEX BY AGE ## 7 B01001_007 Estimate!!Total:!!Male:!!18 and 19 years SEX BY AGE ## 8 B01001_008 Estimate!!Total:!!Male:!!20 years SEX BY AGE ## 9 B01001_009 Estimate!!Total:!!Male:!!21 years SEX BY AGE ## 10 B01001_010 Estimate!!Total:!!Male:!!22 to 24 years SEX BY AGE ## # … with 27,030 more rows This can be searched using RStudio’s View() function, but it’s still a mess. De Witt uses Census Reporter a lot–search online for this. Try https://censusreporter.org/, especially the Explore search field, which is a good way to identify good variables. One useful trick is to find the table identifier in Census Reporter and then use that to explore variables loaded using load_variables(). Another invaluable resource is https://rconsortium.github.io/censusguide/. The census.gov tools are also extensive. See for instance https://www.census.gov/data/academy/data-gems/2021/how-to-visualize-your-data-using-thematic-maps-on-data-census-gov.html. hhinc &lt;- get_acs( geography = &quot;state&quot;, table = &quot;B19001&quot;, survey = &quot;acs1&quot; ) ## The 1-year ACS provides data for geographies with populations of 65,000 and greater. ## Getting data from the 2019 1-year ACS ## Loading ACS1 variables for 2019 from table B19001. To cache this dataset for faster access to ACS tables in the future, run this function with `cache_table = TRUE`. You only need to do this once per ACS dataset. hhinc ## # A tibble: 884 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Alabama B19001_001 1897576 10370 ## 2 01 Alabama B19001_002 154558 5883 ## 3 01 Alabama B19001_003 103653 6001 ## 4 01 Alabama B19001_004 108500 5926 ## 5 01 Alabama B19001_005 98706 6491 ## 6 01 Alabama B19001_006 90916 5859 ## 7 01 Alabama B19001_007 105146 4149 ## 8 01 Alabama B19001_008 85014 5417 ## 9 01 Alabama B19001_009 87118 5163 ## 10 01 Alabama B19001_010 82323 4231 ## # … with 874 more rows glimpse(hhinc) ## Rows: 884 ## Columns: 5 ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… ## $ NAME &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alaba… ## $ variable &lt;chr&gt; &quot;B19001_001&quot;, &quot;B19001_002&quot;, &quot;B19001_003&quot;, &quot;B19001_004&quot;, &quot;B190… ## $ estimate &lt;dbl&gt; 1897576, 154558, 103653, 108500, 98706, 90916, 105146, 85014,… ## $ moe &lt;dbl&gt; 10370, 5883, 6001, 5926, 6491, 5859, 4149, 5417, 5163, 4231, … str(hhinc) ## tibble [884 × 5] (S3: tbl_df/tbl/data.frame) ## $ GEOID : chr [1:884] &quot;01&quot; &quot;01&quot; &quot;01&quot; &quot;01&quot; ... ## $ NAME : chr [1:884] &quot;Alabama&quot; &quot;Alabama&quot; &quot;Alabama&quot; &quot;Alabama&quot; ... ## $ variable: chr [1:884] &quot;B19001_001&quot; &quot;B19001_002&quot; &quot;B19001_003&quot; &quot;B19001_004&quot; ... ## $ estimate: num [1:884] 1897576 154558 103653 108500 98706 ... ## $ moe : num [1:884] 10370 5883 6001 5926 6491 ... This is pretty tough to read. A wide form is easier and can be had without using pivot_wider(). hhinc_wide &lt;- get_acs( geography = &quot;state&quot;, table = &quot;B19001&quot;, survey = &quot;acs1&quot;, output = &quot;wide&quot; ) ## The 1-year ACS provides data for geographies with populations of 65,000 and greater. ## Getting data from the 2019 1-year ACS ## Loading ACS1 variables for 2019 from table B19001. To cache this dataset for faster access to ACS tables in the future, run this function with `cache_table = TRUE`. You only need to do this once per ACS dataset. hhinc_wide ## # A tibble: 52 × 36 ## GEOID NAME B19001_001E B19001_001M B19001_002E B19001_002M B19001_003E ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17 Illinois 4866006 12627 289515 9500 178230 ## 2 13 Georgia 3852714 14425 237054 8319 163741 ## 3 16 Idaho 655859 5316 27773 3127 24498 ## 4 15 Hawaii 465299 5012 23344 2470 12238 ## 5 18 Indiana 2597765 12716 153355 7188 104333 ## 6 19 Iowa 1287221 6606 65503 3958 52788 ## 7 20 Kansas 1138329 6595 57967 4269 49134 ## 8 21 Kentucky 1748732 8789 137394 7450 96775 ## 9 22 Louisiana 1741076 11011 175845 7581 98971 ## 10 23 Maine 573618 4999 29156 2776 26772 ## # … with 42 more rows, and 29 more variables: B19001_003M &lt;dbl&gt;, ## # B19001_004E &lt;dbl&gt;, B19001_004M &lt;dbl&gt;, B19001_005E &lt;dbl&gt;, B19001_005M &lt;dbl&gt;, ## # B19001_006E &lt;dbl&gt;, B19001_006M &lt;dbl&gt;, B19001_007E &lt;dbl&gt;, B19001_007M &lt;dbl&gt;, ## # B19001_008E &lt;dbl&gt;, B19001_008M &lt;dbl&gt;, B19001_009E &lt;dbl&gt;, B19001_009M &lt;dbl&gt;, ## # B19001_010E &lt;dbl&gt;, B19001_010M &lt;dbl&gt;, B19001_011E &lt;dbl&gt;, B19001_011M &lt;dbl&gt;, ## # B19001_012E &lt;dbl&gt;, B19001_012M &lt;dbl&gt;, B19001_013E &lt;dbl&gt;, B19001_013M &lt;dbl&gt;, ## # B19001_014E &lt;dbl&gt;, B19001_014M &lt;dbl&gt;, B19001_015E &lt;dbl&gt;, … glimpse(hhinc_wide) ## Rows: 52 ## Columns: 36 ## $ GEOID &lt;chr&gt; &quot;17&quot;, &quot;13&quot;, &quot;16&quot;, &quot;15&quot;, &quot;18&quot;, &quot;19&quot;, &quot;20&quot;, &quot;21&quot;, &quot;22&quot;, &quot;23&quot;… ## $ NAME &lt;chr&gt; &quot;Illinois&quot;, &quot;Georgia&quot;, &quot;Idaho&quot;, &quot;Hawaii&quot;, &quot;Indiana&quot;, &quot;Iowa… ## $ B19001_001E &lt;dbl&gt; 4866006, 3852714, 655859, 465299, 2597765, 1287221, 113832… ## $ B19001_001M &lt;dbl&gt; 12627, 14425, 5316, 5012, 12716, 6606, 6595, 8789, 11011, … ## $ B19001_002E &lt;dbl&gt; 289515, 237054, 27773, 23344, 153355, 65503, 57967, 137394… ## $ B19001_002M &lt;dbl&gt; 9500, 8319, 3127, 2470, 7188, 3958, 4269, 7450, 7581, 2776… ## $ B19001_003E &lt;dbl&gt; 178230, 163741, 24498, 12238, 104333, 52788, 49134, 96775,… ## $ B19001_003M &lt;dbl&gt; 7552, 6674, 2683, 1722, 5489, 3671, 3808, 5086, 5899, 2777… ## $ B19001_004E &lt;dbl&gt; 183540, 166221, 30937, 12277, 114209, 51996, 47555, 91258,… ## $ B19001_004M &lt;dbl&gt; 7592, 7842, 3245, 2332, 5524, 2979, 3760, 4652, 5590, 2377… ## $ B19001_005E &lt;dbl&gt; 206595, 173428, 28519, 15179, 130573, 55813, 46910, 97164,… ## $ B19001_005M &lt;dbl&gt; 6895, 7666, 2743, 2454, 6385, 3597, 3061, 5289, 5982, 2447… ## $ B19001_006E &lt;dbl&gt; 189948, 169736, 29674, 12991, 119781, 53262, 45121, 87632,… ## $ B19001_006M &lt;dbl&gt; 7371, 9173, 3382, 2087, 6143, 3656, 3036, 5571, 4877, 2930… ## $ B19001_007E &lt;dbl&gt; 197382, 174416, 33553, 16607, 134479, 64428, 56401, 87383,… ## $ B19001_007M &lt;dbl&gt; 6397, 8422, 3043, 2164, 5695, 4104, 3843, 4678, 5559, 2342… ## $ B19001_008E &lt;dbl&gt; 186475, 160146, 29333, 12211, 119809, 55396, 52296, 79311,… ## $ B19001_008M &lt;dbl&gt; 6024, 8450, 2652, 1880, 6480, 3239, 3277, 4462, 4746, 2811… ## $ B19001_009E &lt;dbl&gt; 197027, 168658, 28390, 14811, 126321, 64728, 54980, 82031,… ## $ B19001_009M &lt;dbl&gt; 8242, 9366, 2763, 1770, 5521, 3751, 3076, 4207, 4875, 2634… ## $ B19001_010E &lt;dbl&gt; 170536, 148721, 28771, 13260, 112198, 53469, 44142, 72412,… ## $ B19001_010M &lt;dbl&gt; 7120, 7865, 2955, 1601, 5822, 3312, 3229, 4349, 4467, 2747… ## $ B19001_011E &lt;dbl&gt; 338947, 286625, 59944, 31700, 225269, 106115, 91195, 13847… ## $ B19001_011M &lt;dbl&gt; 9644, 10319, 4165, 2968, 7454, 4657, 4413, 5578, 6506, 354… ## $ B19001_012E &lt;dbl&gt; 465073, 417847, 75990, 46591, 278719, 145070, 128644, 1840… ## $ B19001_012M &lt;dbl&gt; 10099, 13725, 4653, 3741, 8045, 5722, 4856, 6505, 7022, 36… ## $ B19001_013E &lt;dbl&gt; 622878, 490823, 93083, 62750, 335223, 181075, 154883, 2175… ## $ B19001_013M &lt;dbl&gt; 11460, 11586, 5688, 3941, 8714, 6372, 5325, 7666, 8214, 39… ## $ B19001_014E &lt;dbl&gt; 485409, 345431, 62355, 54269, 239586, 129838, 109663, 1447… ## $ B19001_014M &lt;dbl&gt; 11851, 8991, 3869, 3256, 7242, 4739, 5628, 5547, 6002, 309… ## $ B19001_015E &lt;dbl&gt; 335706, 224841, 36160, 38537, 142304, 73169, 65709, 79088,… ## $ B19001_015M &lt;dbl&gt; 8103, 8362, 3380, 3111, 6171, 4237, 3518, 4307, 4712, 2141… ## $ B19001_016E &lt;dbl&gt; 379693, 243294, 34280, 47879, 139536, 70712, 66556, 81644,… ## $ B19001_016M &lt;dbl&gt; 8588, 7657, 3485, 3352, 5495, 3654, 3704, 4722, 4979, 2540… ## $ B19001_017E &lt;dbl&gt; 439052, 281732, 32599, 50655, 122070, 63859, 67173, 71788,… ## $ B19001_017M &lt;dbl&gt; 8929, 8944, 2923, 3266, 5142, 3712, 3598, 3720, 5284, 2666… str(hhinc_wide) ## tibble [52 × 36] (S3: tbl_df/tbl/data.frame) ## $ GEOID : chr [1:52] &quot;17&quot; &quot;13&quot; &quot;16&quot; &quot;15&quot; ... ## $ NAME : chr [1:52] &quot;Illinois&quot; &quot;Georgia&quot; &quot;Idaho&quot; &quot;Hawaii&quot; ... ## $ B19001_001E: num [1:52] 4866006 3852714 655859 465299 2597765 ... ## $ B19001_001M: num [1:52] 12627 14425 5316 5012 12716 ... ## $ B19001_002E: num [1:52] 289515 237054 27773 23344 153355 ... ## $ B19001_002M: num [1:52] 9500 8319 3127 2470 7188 ... ## $ B19001_003E: num [1:52] 178230 163741 24498 12238 104333 ... ## $ B19001_003M: num [1:52] 7552 6674 2683 1722 5489 ... ## $ B19001_004E: num [1:52] 183540 166221 30937 12277 114209 ... ## $ B19001_004M: num [1:52] 7592 7842 3245 2332 5524 ... ## $ B19001_005E: num [1:52] 206595 173428 28519 15179 130573 ... ## $ B19001_005M: num [1:52] 6895 7666 2743 2454 6385 ... ## $ B19001_006E: num [1:52] 189948 169736 29674 12991 119781 ... ## $ B19001_006M: num [1:52] 7371 9173 3382 2087 6143 ... ## $ B19001_007E: num [1:52] 197382 174416 33553 16607 134479 ... ## $ B19001_007M: num [1:52] 6397 8422 3043 2164 5695 ... ## $ B19001_008E: num [1:52] 186475 160146 29333 12211 119809 ... ## $ B19001_008M: num [1:52] 6024 8450 2652 1880 6480 ... ## $ B19001_009E: num [1:52] 197027 168658 28390 14811 126321 ... ## $ B19001_009M: num [1:52] 8242 9366 2763 1770 5521 ... ## $ B19001_010E: num [1:52] 170536 148721 28771 13260 112198 ... ## $ B19001_010M: num [1:52] 7120 7865 2955 1601 5822 ... ## $ B19001_011E: num [1:52] 338947 286625 59944 31700 225269 ... ## $ B19001_011M: num [1:52] 9644 10319 4165 2968 7454 ... ## $ B19001_012E: num [1:52] 465073 417847 75990 46591 278719 ... ## $ B19001_012M: num [1:52] 10099 13725 4653 3741 8045 ... ## $ B19001_013E: num [1:52] 622878 490823 93083 62750 335223 ... ## $ B19001_013M: num [1:52] 11460 11586 5688 3941 8714 ... ## $ B19001_014E: num [1:52] 485409 345431 62355 54269 239586 ... ## $ B19001_014M: num [1:52] 11851 8991 3869 3256 7242 ... ## $ B19001_015E: num [1:52] 335706 224841 36160 38537 142304 ... ## $ B19001_015M: num [1:52] 8103 8362 3380 3111 6171 ... ## $ B19001_016E: num [1:52] 379693 243294 34280 47879 139536 ... ## $ B19001_016M: num [1:52] 8588 7657 3485 3352 5495 ... ## $ B19001_017E: num [1:52] 439052 281732 32599 50655 122070 ... ## $ B19001_017M: num [1:52] 8929 8944 2923 3266 5142 ... We still aren’t transparent with respect to vectors. I will never remember them. Named vectors can be used: ga_wide &lt;- get_acs( geography = &quot;county&quot;, state = &quot;GA&quot;, variables = c(median_inc = &quot;B19013_001&quot;, median_age = &quot;B01002_001&quot;), output = &quot;wide&quot; ) ## Getting data from the 2015-2019 5-year ACS ga_wide ## # A tibble: 159 × 6 ## GEOID NAME median_incE median_incM median_ageE median_ageM ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 13005 Bacon County, Georgia 37519 5492 36.7 0.7 ## 2 13025 Brantley County, Georgia 38857 3480 41.1 0.8 ## 3 13017 Ben Hill County, Georgia 32229 3845 39.9 1.1 ## 4 13033 Burke County, Georgia 44151 2438 37.4 0.6 ## 5 13047 Catoosa County, Georgia 56235 2290 40.4 0.4 ## 6 13053 Chattahoochee County, Georgia 47096 5158 24.5 0.5 ## 7 13055 Chattooga County, Georgia 36807 2268 39.4 0.7 ## 8 13073 Columbia County, Georgia 82339 3532 36.9 0.4 ## 9 13087 Decatur County, Georgia 41481 3584 37.8 0.6 ## 10 13115 Floyd County, Georgia 48336 2266 38.3 0.3 ## # … with 149 more rows Let’s try something closer to home: med_age_Hennepin_wide &lt;- get_acs( geography = &quot;tract&quot;, state = &quot;MN&quot;, variables = c(median_age = &quot;B01002_001&quot;), output = &quot;wide&quot; ) ## Getting data from the 2015-2019 5-year ACS med_age_Hennepin_wide ## # A tibble: 1,338 × 4 ## GEOID NAME median_ageE median_ageM ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27053025401 Census Tract 254.01, Hennepin County, Minnesota 37.4 3.8 ## 2 27053025805 Census Tract 258.05, Hennepin County, Minnesota 52.2 6 ## 3 27053026018 Census Tract 260.18, Hennepin County, Minnesota 41.9 1.8 ## 4 27053026402 Census Tract 264.02, Hennepin County, Minnesota 36.1 2.9 ## 5 27053026404 Census Tract 264.04, Hennepin County, Minnesota 50.5 3.7 ## 6 27053026809 Census Tract 268.09, Hennepin County, Minnesota 29.1 2.2 ## 7 27053027602 Census Tract 276.02, Hennepin County, Minnesota 45.9 3.6 ## 8 27053102000 Census Tract 1020, Hennepin County, Minnesota 33.9 3.6 ## 9 27053103600 Census Tract 1036, Hennepin County, Minnesota 47.9 5.1 ## 10 27053022902 Census Tract 229.02, Hennepin County, Minnesota 40.2 3.1 ## # … with 1,328 more rows vs15 &lt;- load_variables(2015, &quot;acs5&quot;, cache = TRUE) mn_wide &lt;- get_acs( geography = &quot;county&quot;, state = &quot;MN&quot;, variables = c(total_pop = &quot;B01003_001&quot;, median_inc = &quot;B19013_001&quot;, median_age = &quot;B01002_001&quot;, white = &quot;B02001_002&quot;, black = &quot;B02001_003&quot;, native_am = &quot;B02001_004&quot;, asian = &quot;B02001_005&quot;, doctor = &quot;B15003_025&quot;), output = &quot;wide&quot; ) ## Getting data from the 2015-2019 5-year ACS mn_wide ## # A tibble: 87 × 18 ## GEOID NAME total_popE total_popM median_incE median_incM median_ageE ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27093 Meeker Count… 23105 NA 63452 1601 42.2 ## 2 27131 Rice County,… 66185 NA 68584 2227 36.5 ## 3 27125 Red Lake Cou… 4015 NA 58576 4262 42.7 ## 4 27027 Clay County,… 63446 NA 65269 2909 32.6 ## 5 27045 Fillmore Cou… 20949 NA 61207 1604 42.3 ## 6 27119 Polk County,… 31521 NA 59343 2282 38.7 ## 7 27143 Sibley Count… 14892 NA 63439 1646 41.7 ## 8 27165 Watonwan Cou… 10972 NA 54065 2930 39.9 ## 9 27011 Big Stone Co… 4996 NA 53900 5054 49 ## 10 27021 Cass County,… 29268 NA 52204 1447 48.9 ## # … with 77 more rows, and 11 more variables: median_ageM &lt;dbl&gt;, whiteE &lt;dbl&gt;, ## # whiteM &lt;dbl&gt;, blackE &lt;dbl&gt;, blackM &lt;dbl&gt;, native_amE &lt;dbl&gt;, ## # native_amM &lt;dbl&gt;, asianE &lt;dbl&gt;, asianM &lt;dbl&gt;, doctorE &lt;dbl&gt;, doctorM &lt;dbl&gt; 15.2 Part 2: Wrangling Census Data with tidyverse Tools median_age &lt;- get_acs( geography = &quot;county&quot;, variables = &quot;B01002_001&quot; ) ## Getting data from the 2015-2019 5-year ACS arrange(median_age, estimate) ## # A tibble: 3,220 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51678 Lexington city, Virginia B01002_001 22.3 0.7 ## 2 51750 Radford city, Virginia B01002_001 23.4 0.5 ## 3 16065 Madison County, Idaho B01002_001 23.5 0.2 ## 4 46121 Todd County, South Dakota B01002_001 23.8 0.4 ## 5 02158 Kusilvak Census Area, Alaska B01002_001 24.1 0.2 ## 6 13053 Chattahoochee County, Georgia B01002_001 24.5 0.5 ## 7 53075 Whitman County, Washington B01002_001 24.7 0.3 ## 8 49049 Utah County, Utah B01002_001 24.8 0.1 ## 9 46027 Clay County, South Dakota B01002_001 24.9 0.4 ## 10 51830 Williamsburg city, Virginia B01002_001 24.9 0.7 ## # … with 3,210 more rows arrange(median_age, desc(estimate)) ## # A tibble: 3,220 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12119 Sumter County, Florida B01002_001 67.4 0.2 ## 2 51091 Highland County, Virginia B01002_001 60.9 3.5 ## 3 08027 Custer County, Colorado B01002_001 59.7 2.6 ## 4 12015 Charlotte County, Florida B01002_001 59.1 0.2 ## 5 41069 Wheeler County, Oregon B01002_001 59 3.3 ## 6 51133 Northumberland County, Virginia B01002_001 58.9 0.7 ## 7 26131 Ontonagon County, Michigan B01002_001 58.6 0.4 ## 8 35021 Harding County, New Mexico B01002_001 58.5 5.5 ## 9 53031 Jefferson County, Washington B01002_001 58.3 0.7 ## 10 26001 Alcona County, Michigan B01002_001 58.2 0.3 ## # … with 3,210 more rows above50 &lt;- filter(median_age, estimate &gt;=50) Note how DeWitt assembles the following race/ethnicity groups, and then applies a summary variable: race_vars &lt;- c( white = &quot;B03002_003&quot;, black = &quot;B03002_004&quot;, native = &quot;B03002_005&quot;, asian = &quot;B03002_006&quot;, HIPI = &quot;B03002_007&quot;, hispanic = &quot;B03002_012&quot; ) az_race &lt;- get_acs( geography = &quot;county&quot;, state = &quot;AZ&quot;, variables = race_vars, summary_var = &quot;B03002_001&quot; ) ## Getting data from the 2015-2019 5-year ACS az_race ## # A tibble: 90 × 7 ## GEOID NAME variable estimate moe summary_est summary_moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 04001 Apache County, Arizona white 13022 4 71511 NA ## 2 04001 Apache County, Arizona black 373 138 71511 NA ## 3 04001 Apache County, Arizona native 52285 234 71511 NA ## 4 04001 Apache County, Arizona asian 246 78 71511 NA ## 5 04001 Apache County, Arizona HIPI 16 16 71511 NA ## 6 04001 Apache County, Arizona hispanic 4531 NA 71511 NA ## 7 04003 Cochise County, Arizona white 69216 235 125867 NA ## 8 04003 Cochise County, Arizona black 4620 247 125867 NA ## 9 04003 Cochise County, Arizona native 1142 191 125867 NA ## 10 04003 Cochise County, Arizona asian 2431 162 125867 NA ## # … with 80 more rows 15.2.1 Normalizing the data with mutate(). az_race_percent &lt;- az_race %&gt;% mutate(percent = 100*(estimate/summary_est)) %&gt;% select(NAME, variable, percent) az_race_percent ## # A tibble: 90 × 3 ## NAME variable percent ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Apache County, Arizona white 18.2 ## 2 Apache County, Arizona black 0.522 ## 3 Apache County, Arizona native 73.1 ## 4 Apache County, Arizona asian 0.344 ## 5 Apache County, Arizona HIPI 0.0224 ## 6 Apache County, Arizona hispanic 6.34 ## 7 Cochise County, Arizona white 55.0 ## 8 Cochise County, Arizona black 3.67 ## 9 Cochise County, Arizona native 0.907 ## 10 Cochise County, Arizona asian 1.93 ## # … with 80 more rows 15.2.2 group_by() and summarize() in census analysis largest_group &lt;- az_race_percent %&gt;% group_by(NAME) %&gt;% filter(percent == max(percent)) largest_group ## # A tibble: 15 × 3 ## # Groups: NAME [15] ## NAME variable percent ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Apache County, Arizona native 73.1 ## 2 Cochise County, Arizona white 55.0 ## 3 Coconino County, Arizona white 54.1 ## 4 Gila County, Arizona white 62.3 ## 5 Graham County, Arizona white 50.9 ## 6 Greenlee County, Arizona hispanic 46.8 ## 7 La Paz County, Arizona white 57.4 ## 8 Maricopa County, Arizona white 55.2 ## 9 Mohave County, Arizona white 77.3 ## 10 Navajo County, Arizona native 43.5 ## 11 Pima County, Arizona white 51.7 ## 12 Pinal County, Arizona white 56.8 ## 13 Santa Cruz County, Arizona hispanic 83.5 ## 14 Yavapai County, Arizona white 80.5 ## 15 Yuma County, Arizona hispanic 63.8 az_race_percent %&gt;% group_by(variable) %&gt;% summarize(median_pct = median(percent)) ## # A tibble: 6 × 2 ## variable median_pct ## &lt;chr&gt; &lt;dbl&gt; ## 1 asian 0.924 ## 2 black 1.12 ## 3 HIPI 0.121 ## 4 hispanic 30.2 ## 5 native 3.58 ## 6 white 54.1 15.2.3 Margin of error considerations vars1 &lt;- paste0(&quot;B01001_0&quot;, c(20:25, 44:49)) salt_lake &lt;- get_acs( geography = &quot;tract&quot;, variables = vars1, state = &quot;Utah&quot;, county = &quot;Salt Lake&quot;, year = 2019 ) ## Getting data from the 2015-2019 5-year ACS example_tract &lt;- salt_lake %&gt;% filter(GEOID == &quot;49035100100&quot;) example_tract %&gt;% select(-NAME) ## # A tibble: 12 × 4 ## GEOID variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 49035100100 B01001_020 12 13 ## 2 49035100100 B01001_021 36 23 ## 3 49035100100 B01001_022 8 11 ## 4 49035100100 B01001_023 5 8 ## 5 49035100100 B01001_024 0 11 ## 6 49035100100 B01001_025 22 23 ## 7 49035100100 B01001_044 0 11 ## 8 49035100100 B01001_045 11 13 ## 9 49035100100 B01001_046 27 20 ## 10 49035100100 B01001_047 10 12 ## 11 49035100100 B01001_048 7 11 ## 12 49035100100 B01001_049 0 11 tidycensus has multiple functions already built to make margin of error calculations more straigtforward when you are assembling calculated values from multiple variables each with their own margin of error. One of these functions is mod_prop(). Check out the help documentation to understand the following example: moe_prop(25,100, 5, 3) ## [1] 0.0494343 At 1 hour 52 minutes in the YouTube, a nice example to reduce margin of error by grouping small bins of data into larger bins is introduced. I do not take it up here. Exercises for 2nd break: mn_bachelors_and_up &lt;- get_acs( geography = &quot;county&quot;, state = &quot;MN&quot;, variables = &quot;DP02_0068P&quot;) ## Getting data from the 2015-2019 5-year ACS ## Using the ACS Data Profile mn_bachelors_and_up$estimate %&gt;% median() ## [1] 22.2 median(mn_bachelors_and_up$estimate) ## [1] 22.2 # mn_bachelors_and_up %&gt;% median() New goal: find the percentage of commuters taking public transit to work in the 20 most populous metropolitan areas. metros &lt;- get_acs( geography = &quot;cbsa&quot;, variables = &quot;DP03_0021P&quot;, summary_var = &quot;B01003_001&quot;, survey = &quot;acs1&quot; ) %&gt;% filter(min_rank(desc(summary_est)) &lt;21) ## The 1-year ACS provides data for geographies with populations of 65,000 and greater. ## Getting data from the 2019 1-year ACS ## Using the ACS Data Profile glimpse(metros) ## Rows: 20 ## Columns: 7 ## $ GEOID &lt;chr&gt; &quot;12060&quot;, &quot;14460&quot;, &quot;16980&quot;, &quot;19100&quot;, &quot;19740&quot;, &quot;19820&quot;, &quot;264… ## $ NAME &lt;chr&gt; &quot;Atlanta-Sandy Springs-Alpharetta, GA Metro Area&quot;, &quot;Boston… ## $ variable &lt;chr&gt; &quot;DP03_0021P&quot;, &quot;DP03_0021P&quot;, &quot;DP03_0021P&quot;, &quot;DP03_0021P&quot;, &quot;D… ## $ estimate &lt;dbl&gt; 2.8, 13.4, 12.4, 1.3, 4.5, 1.4, 2.0, 4.8, 2.9, 4.5, 31.6, … ## $ moe &lt;dbl&gt; 0.2, 0.4, 0.3, 0.1, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.2, 0.3… ## $ summary_est &lt;dbl&gt; 6018744, 4873019, 9457867, 7573136, 2967239, 4319629, 7066… ## $ summary_moe &lt;dbl&gt; 3340, NA, 1469, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ggplot(data = metros, aes(x = NAME, y = estimate)) + geom_col() p &lt;- metros %&gt;% mutate(NAME = str_remove(NAME, &quot;-.*$&quot;)) %&gt;% mutate(NAME = str_remove(NAME, &quot;,.*$&quot;)) %&gt;% ggplot(aes(y = reorder(NAME, estimate), x = estimate)) + geom_col() p + theme_minimal() + labs(title = &quot;Nifty title&quot;, y = &quot;&quot;, x = &quot;acs estimate (percent)&quot;, caption = &quot;Nifty caption&quot;) 15.2.4 Visualizing margins of error maine_income &lt;- get_acs( state = &quot;Maine&quot;, geography = &quot;county&quot;, variables = c(hhincome = &quot;B19013_001&quot;)) %&gt;% mutate(NAME = str_remove(NAME, &quot; County, Maine&quot;)) ## Getting data from the 2015-2019 5-year ACS maine_income %&gt;% arrange(desc(moe)) ## # A tibble: 16 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 23015 Lincoln hhincome 57720 3240 ## 2 23007 Franklin hhincome 51422 2966 ## 3 23013 Knox hhincome 57751 2820 ## 4 23021 Piscataquis hhincome 40890 2613 ## 5 23025 Somerset hhincome 44256 2591 ## 6 23023 Sagadahoc hhincome 63694 2309 ## 7 23027 Waldo hhincome 51931 2170 ## 8 23009 Hancock hhincome 57178 2057 ## 9 23011 Kennebec hhincome 55365 1948 ## 10 23017 Oxford hhincome 49204 1879 ## 11 23001 Androscoggin hhincome 53509 1770 ## 12 23029 Washington hhincome 41347 1565 ## 13 23031 York hhincome 67830 1450 ## 14 23005 Cumberland hhincome 73072 1427 ## 15 23003 Aroostook hhincome 41123 1381 ## 16 23019 Penobscot hhincome 50808 1326 maine_income %&gt;% ggplot( aes(x = estimate, y = reorder(NAME, estimate))) + geom_errorbarh(aes(xmin = estimate-moe, xmax = estimate+moe)) + geom_point(size = 3, color = &quot;darkgreen&quot;) + labs(title = &quot;Spiffy title&quot;, subtitle = &quot;Counites of Maine&quot;, x = &quot;2015-2019 ACS Estimate&quot;, y = &quot;&quot;) + scale_x_continuous(labels = scales::dollar) 15.2.5 get_estimates() and how to use them utah &lt;- get_estimates( geography = &quot;state&quot;, state = &quot;UT&quot;, product = &quot;characteristics&quot;, breakdown = c(&quot;SEX&quot;, &quot;AGEGROUP&quot;), breakdown_labels = TRUE, year = 2019 ) utah ## # A tibble: 96 × 5 ## GEOID NAME value SEX AGEGROUP ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; ## 1 49 Utah 3205958 Both sexes All ages ## 2 49 Utah 247803 Both sexes Age 0 to 4 years ## 3 49 Utah 258976 Both sexes Age 5 to 9 years ## 4 49 Utah 1614917 Male All ages ## 5 49 Utah 132868 Male Age 5 to 9 years ## 6 49 Utah 1591041 Female All ages ## 7 49 Utah 126108 Female Age 5 to 9 years ## 8 49 Utah 23039 Female Age 80 to 84 years ## 9 49 Utah 267985 Both sexes Age 10 to 14 years ## 10 49 Utah 137940 Male Age 10 to 14 years ## # … with 86 more rows utah_filtered &lt;- filter(utah, str_detect(AGEGROUP, &quot;^Age&quot;), SEX != &quot;Both sexes&quot;) %&gt;% mutate(value = ifelse(SEX == &quot;Male&quot;, -value, value)) utah_filtered ## # A tibble: 36 × 5 ## GEOID NAME value SEX AGEGROUP ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; ## 1 49 Utah -132868 Male Age 5 to 9 years ## 2 49 Utah 126108 Female Age 5 to 9 years ## 3 49 Utah 23039 Female Age 80 to 84 years ## 4 49 Utah -137940 Male Age 10 to 14 years ## 5 49 Utah -129312 Male Age 15 to 19 years ## 6 49 Utah 130045 Female Age 10 to 14 years ## 7 49 Utah 124535 Female Age 15 to 19 years ## 8 49 Utah -135806 Male Age 20 to 24 years ## 9 49 Utah 128846 Female Age 20 to 24 years ## 10 49 Utah -111776 Male Age 30 to 34 years ## # … with 26 more rows utah_filtered %&gt;% ggplot(aes(x = value, y = AGEGROUP, fill = SEX)) + geom_col() utah_pyramid &lt;- utah_filtered %&gt;% ggplot(aes(x = value, y = AGEGROUP, fill = SEX)) + geom_col(width = 0.95, alpha = 0.75)+ theme_minimal(base_family = &quot;Verdana&quot;) + scale_x_continuous(labels = function(y)paste0(abs(y/1000), &quot;k&quot;)) + scale_y_discrete(labels = function(x)gsub(&quot;Age|years&quot;, &quot;&quot;, x)) + scale_fill_manual(values = c(&quot;darkred&quot;, &quot;navy&quot;)) + labs(x = &quot;&quot;, y = &quot;2019 Census Bureau population estimate&quot;, title = &quot;Population Structure in Utah&quot;, fill = &quot;&quot;, caption = &quot;Data source: US Census Bureau population estimates and tidycensus R package&quot;) utah_pyramid ggplotly(utah_pyramid) 15.2.6 ggbeeswarm() automates some jitter considerations mn_race_income &lt;- get_acs( geography = &quot;tract&quot;, state = &quot;MN&quot;, county = c(&quot;Hennepin&quot;, &quot;Ramsey&quot;, &quot;Washington&quot;, &quot;Carver&quot;, &quot;Dakota&quot;, &quot;Anoka&quot;, &quot;Wright&quot;, &quot;Scott&quot;), variables = c(White = &quot;B03002_003&quot;, Black = &quot;B03002_004&quot;, Asian = &quot;B03002_006&quot;, Hispanic = &quot;B03002_012&quot;), summary_var = &quot;B19013_001&quot; ) %&gt;% group_by(GEOID) %&gt;% filter(estimate == max(estimate, na.rm = TRUE)) %&gt;% ungroup() %&gt;% filter(estimate != 0) ## Getting data from the 2015-2019 5-year ACS library(ggbeeswarm) mn_race_income %&gt;% ggplot(aes(x = variable, y = summary_est, color = summary_est)) + geom_quasirandom(alpha = 0.5) + coord_flip() + theme_minimal() + scale_color_viridis_c(guide = FALSE) + scale_y_continuous(labels = scales::dollar) + labs(x = &quot;Largest Group in Census Tract&quot;, y = &quot;Median Household Income&quot;, title = &quot;Household Income By Largest Race/Ethnic Group&quot;, subtitle = &quot;Census Tracts, Twin Cities Metro Area&quot;, caption = &quot;Data source: 2015-2019 ACS&quot;) ## Warning: Removed 3 rows containing missing values (position_quasirandom). ## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please ## use `guide = &quot;none&quot;` instead. "],["knncv.html", "Chapter 16 K Nearest Neighbors Revisited: Cross Validation Added Tips for This Document", " Chapter 16 K Nearest Neighbors Revisited: Cross Validation Added Tips for This Document library(caret) library(tidyverse) theme_set(theme_minimal()) Good resource: https://www.r-bloggers.com/2021/04/knn-algorithm-machine-learning/?utm_source=feedburner&amp;utm_medium=email&amp;utm_campaign=Feed%3A+RBloggers+%28R+bloggers%29 An excellent source for the caret is https://topepo.github.io/caret/index.html The following code is from https://stats.stackexchange.com/questions/318968/knn-and-k-folding-in-r trControl &lt;- trainControl(method = &quot;cv&quot;, number = 5) fit &lt;- train(Species ~ ., method = &quot;knn&quot;, tuneGrid = expand.grid(k = 1:10), trControl = trControl, metric = &quot;Accuracy&quot;, data = iris) fit ## k-Nearest Neighbors ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 120, 120, 120, 120, 120 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.9600000 0.94 ## 2 0.9533333 0.93 ## 3 0.9600000 0.94 ## 4 0.9666667 0.95 ## 5 0.9600000 0.94 ## 6 0.9600000 0.94 ## 7 0.9733333 0.96 ## 8 0.9600000 0.94 ## 9 0.9733333 0.96 ## 10 0.9800000 0.97 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 10. plot(fit) I’d like to see whether this will help build a model to improve the classification of pituitary neuroendocrine tumors (pituitary adenomas). A copy of the patmandx data from PitAdTMA9.0 is already in the test book data directory. pit &lt;- read_csv(&quot;data/patmanDx.csv&quot;) ## Rows: 157 Columns: 36 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (12): Dx, SurgPathNo, CAM5.2Pattern, newDx, finDx, villaDx, villaCode, N... ## dbl (24): CaseID, Pit1Median, SF1Median, ASUMedian, PRLMedian, GHMedian, ACT... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. df &lt;- pit %&gt;% select(SF1Median, Pit1Median, TPITMedian, PRLMedian, GHMedian, TSHMedian, LHMedian, FSHMedian, ACTHMedian, ASUMedian, GATA3Median, manDx) %&gt;% na.omit() table(df$manDx) ## ## ACTH GH GON NULL PIT1 PLUR PRL UNK ## 23 16 77 10 4 1 15 1 # Now, the trouble with null cell adenoma is that the diagnosis abbreviation &quot;NULL&quot; is mistaken for the NULL value. I&#39;ll need to change this. The following is an ugly way to do this. df &lt;- df %&gt;% mutate(manDx = case_when( manDx == &quot;NULL&quot; ~ &quot;Null&quot;, TRUE ~ manDx )) table(df$manDx) ## ## ACTH GH GON Null PIT1 PLUR PRL UNK ## 23 16 77 10 4 1 15 1 Now I’ll ape the code from above and see whether it works. trControl &lt;- trainControl(method = &quot;cv&quot;, number = 5) fit &lt;- train(manDx ~ ., method = &quot;knn&quot;, tuneGrid = expand.grid(k = 1:10), trControl = trControl, metric = &quot;Accuracy&quot;, data = df) ## Warning: predictions failed for Fold3: k= 1 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k= 2 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k= 3 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k= 4 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k= 5 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k= 6 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k= 7 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k= 8 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k= 9 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning: predictions failed for Fold3: k=10 Error in dimnames(x) &lt;- dn : ## length of &#39;dimnames&#39; [2] not equal to array extent ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. fit ## k-Nearest Neighbors ## ## 147 samples ## 11 predictor ## 8 classes: &#39;ACTH&#39;, &#39;GH&#39;, &#39;GON&#39;, &#39;Null&#39;, &#39;PIT1&#39;, &#39;PLUR&#39;, &#39;PRL&#39;, &#39;UNK&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 118, 118, 116, 119, 117 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.9226806 0.8824466 ## 2 0.8962028 0.8402102 ## 3 0.9131568 0.8665709 ## 4 0.9042282 0.8523781 ## 5 0.9307061 0.8938411 ## 6 0.9482553 0.9210779 ## 7 0.9309934 0.8949336 ## 8 0.9220854 0.8809015 ## 9 0.8881773 0.8288791 ## 10 0.9051314 0.8552866 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 6. plot(fit) Note that if I run this multiple times, I get multiple different optimal values of k. set.seed(1234)* would render a single, reproducible (if not “correct”) value. So ostensibly, this works, but I have a couple important items to address: A fundimental lack of understanding “what’s going on under the hood.” A bunch of warnings that I’m simply ignoring for now. "],["modeling.html", "Chapter 17 Modeling 17.1 Resources: 17.2 Classification: what I’m really interested in 17.3 Class imbalance: a critical issue in pituitary adenoma classification 17.4 Modeling notes 17.5 Out of sample error example 17.6 A regression example from the course, this time using caret 17.7 Classification 17.8 From the DataCamp course 17.9 Random forest with caret 17.10 Random forest model and tuneGrid()", " Chapter 17 Modeling library(tidyverse) library(rpart) 17.1 Resources: Stefania Ashby has nice observations and comments her code very well at https://neurospection.netlify.app/post/machine-learning-basics-with-caret 17.2 Classification: what I’m really interested in Four types of classification are common in ML. See https://machinelearningmastery.com/types-of-classification-in-machine-learning/. Jason Brownlee’s list includes: Binary Multi-class Multi-label Imbalanced I think that binary, multiclass, and multilabel can all be imbalanced or balanced. While multilabel classification is interesting (see, for example, https://www.r-bloggers.com/2017/03/multilabel-classification-with-mlr/), it will not be addressed in these notes. 17.3 Class imbalance: a critical issue in pituitary adenoma classification Fundamental to modeling pituitary adenoma classification is the notion of class imbalance, which must be acknowledged in the modeling process, even if my options to address it are limited. Resources for class imbalance: https://www.svds.com/learning-imbalanced-classes/ https://dpmartin42.github.io/posts/r/imbalanced-classes-part-1 https://rstudio-pubs-static.s3.amazonaws.com/607601_57a11284917f4d79933f4c4db3d41713.html 17.4 Modeling notes Notes on/inspired by the Machine Learning with caret in R DataCamp course. I’d like to better understand machine learning, especially as it pertains to classification problems. Most discussions begin with regression examples, so these will also be addressed in order to build a better foundation. This is the general approach of James, Witten, Hastie and Tibshirani in An Introduction to Statistical Learning with Applications in R, too. But let’s back up and consider the whole rationale behind modeling: according to Haley Wickham in R4DS, The goal of a model is to provide a simple low-dimensional summary of a dataset. Modeling is a mode of supervised learning, which can be divided into classification and regression. Root mean squared error, RMSE, is an important concept in regression problems. For a review of what is meant by RMSE, see this Wikipedia page: https://en.wikipedia.org/wiki/Root-mean-square_deviation. The units RMSE are the same as the original data, so it is very interpretable. It’s worth taking a more careful look at RMSE, and practicing with it, to illustrate the underlying methods of regression before we get into caret. glimpse(diamonds) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… # Fit lm model: model model&lt;-lm(price~., diamonds) # Predict on full data: p p&lt;-predict(model, diamonds) # Compute errors: error error&lt;-p-diamonds$price # Calculate RMSE sqrt(mean(error^2)) ## [1] 1129.843 Note that the RMSE in this case is $1129.84, which is in keeping with what we know about the price of diamonds, which range from $326 to $18823 and average $3932.8. Within-sample RMSE always overestimates model accuracy–the model only “knows” what it has encountered, not what the rest of the universe holds in store. Hence, out-of-sample data is checked with the model by some means. A separate “validation set” of sample points is provided (as we did when we moved from exploration to validation in our first pituitary adenoma paper). Zach Mayer states this another way: In-sample validation almost guarantees overfitting. So in the wide world and blue, this is perhaps the nicest intellectual defense of study abroad, of learning the perspectives, habits, and languages of others in order to avoid overfitting of our mental models to local norms. Sample size cannot always grow, however: samples are expensive. caret simulates the process of having a validation set and permits the progressive refinement of a model. 17.5 Out of sample error example This example from the DataCamp course divides diamonds into test and training sets. Note the assumptions that get built into the process: the use of 80% train/20% test, for instance. How does one arrive at this figure? # Set seed set.seed(42) # Shuffle row indices in case the data set is inhomogeneous: rows rows&lt;-sample(nrow(diamonds)) # Randomly order data shuffled_diamonds&lt;-diamonds[rows,] # Determine row to split on: split split&lt;-round(nrow(diamonds)*0.80) # Create train train&lt;-diamonds[1:split,] # Create test test&lt;-diamonds[(split+1):nrow(diamonds),] # Fit lm model on train: model model&lt;-lm(price~., train) # Predict on test: p p&lt;-predict(model, test) # Compute errors: error error &lt;- p - test$price # Calculate RMSE sqrt(mean(error^2)) ## [1] 796.8922 So the RMSE for the model of diamond price, as measured by dividing the set this way, is $796.89. 17.6 A regression example from the course, this time using caret Note that the train() function has the method characteristic that can choose the type of model and that the trainControl() function has a method that determines cross validation. The number characteristic refers to the number of folds of cross validation. 10-fold cross validation is common, but takes more time than the use of smaller numbers. 5-fold will be used here to improve the speed of the processing. library(caret) model &lt;- train( price~., diamonds, method = &quot;lm&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: intercept=TRUE ## - Fold1: intercept=TRUE ## + Fold2: intercept=TRUE ## - Fold2: intercept=TRUE ## + Fold3: intercept=TRUE ## - Fold3: intercept=TRUE ## + Fold4: intercept=TRUE ## - Fold4: intercept=TRUE ## + Fold5: intercept=TRUE ## - Fold5: intercept=TRUE ## Aggregating results ## Fitting final model on full training set # Print model to console model ## Linear Regression ## ## 53940 samples ## 9 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 43152, 43152, 43152, 43152, 43152 ## Resampling results: ## ## RMSE Rsquared MAE ## 1137.965 0.9186084 741.3386 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE Note that caret handles the work of splitting test sets and calculating RMSE. Another example from the DataCamp course. library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select # Fit lm model using 5-fold CV: model model &lt;- train( medv~., Boston, method = &quot;lm&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: intercept=TRUE ## - Fold1: intercept=TRUE ## + Fold2: intercept=TRUE ## - Fold2: intercept=TRUE ## + Fold3: intercept=TRUE ## - Fold3: intercept=TRUE ## + Fold4: intercept=TRUE ## - Fold4: intercept=TRUE ## + Fold5: intercept=TRUE ## - Fold5: intercept=TRUE ## Aggregating results ## Fitting final model on full training set # Print model to console model ## Linear Regression ## ## 506 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 405, 405, 405, 403, 406 ## Resampling results: ## ## RMSE Rsquared MAE ## 4.811293 0.7238097 3.375276 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE Cross validation can itself be repeated. The following is a 5-fold cross validation repeated 5 times. # Fit lm model using 5 x 5-fold CV: model model &lt;- train( medv ~ ., Boston, method = &quot;lm&quot;, trControl = trainControl( method = &quot;repeatedcv&quot;, number = 5, repeats = 5, verboseIter = TRUE ) ) ## + Fold1.Rep1: intercept=TRUE ## - Fold1.Rep1: intercept=TRUE ## + Fold2.Rep1: intercept=TRUE ## - Fold2.Rep1: intercept=TRUE ## + Fold3.Rep1: intercept=TRUE ## - Fold3.Rep1: intercept=TRUE ## + Fold4.Rep1: intercept=TRUE ## - Fold4.Rep1: intercept=TRUE ## + Fold5.Rep1: intercept=TRUE ## - Fold5.Rep1: intercept=TRUE ## + Fold1.Rep2: intercept=TRUE ## - Fold1.Rep2: intercept=TRUE ## + Fold2.Rep2: intercept=TRUE ## - Fold2.Rep2: intercept=TRUE ## + Fold3.Rep2: intercept=TRUE ## - Fold3.Rep2: intercept=TRUE ## + Fold4.Rep2: intercept=TRUE ## - Fold4.Rep2: intercept=TRUE ## + Fold5.Rep2: intercept=TRUE ## - Fold5.Rep2: intercept=TRUE ## + Fold1.Rep3: intercept=TRUE ## - Fold1.Rep3: intercept=TRUE ## + Fold2.Rep3: intercept=TRUE ## - Fold2.Rep3: intercept=TRUE ## + Fold3.Rep3: intercept=TRUE ## - Fold3.Rep3: intercept=TRUE ## + Fold4.Rep3: intercept=TRUE ## - Fold4.Rep3: intercept=TRUE ## + Fold5.Rep3: intercept=TRUE ## - Fold5.Rep3: intercept=TRUE ## + Fold1.Rep4: intercept=TRUE ## - Fold1.Rep4: intercept=TRUE ## + Fold2.Rep4: intercept=TRUE ## - Fold2.Rep4: intercept=TRUE ## + Fold3.Rep4: intercept=TRUE ## - Fold3.Rep4: intercept=TRUE ## + Fold4.Rep4: intercept=TRUE ## - Fold4.Rep4: intercept=TRUE ## + Fold5.Rep4: intercept=TRUE ## - Fold5.Rep4: intercept=TRUE ## + Fold1.Rep5: intercept=TRUE ## - Fold1.Rep5: intercept=TRUE ## + Fold2.Rep5: intercept=TRUE ## - Fold2.Rep5: intercept=TRUE ## + Fold3.Rep5: intercept=TRUE ## - Fold3.Rep5: intercept=TRUE ## + Fold4.Rep5: intercept=TRUE ## - Fold4.Rep5: intercept=TRUE ## + Fold5.Rep5: intercept=TRUE ## - Fold5.Rep5: intercept=TRUE ## Aggregating results ## Fitting final model on full training set # Print model to console model ## Linear Regression ## ## 506 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 405, 404, 404, 406, 405, 405, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 4.8764 0.722424 3.421333 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE p &lt;- predict(model, Boston) error &lt;- p - Boston$medv sqrt(mean(error^2)) ## [1] 4.679191 17.7 Classification 17.7.1 An rpart classification example. model &lt;- rpart( Species ~ ., data = iris, method = &quot;class&quot; ) predicted &lt;- predict(model, iris, type = &quot;class&quot;) head(predicted) ## 1 2 3 4 5 6 ## setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica Study what’s going on in predict(): ?predict.rpart So predict() uses the model to assign a predicted value to Species based on the rest of the iris dataset data. The accuracy of this prediction can be tested by comparing this vector to the original Species. mean(predicted == iris$Species) ## [1] 0.96 17.8 From the DataCamp course library(mlbench) data(&quot;Sonar&quot;) # First randomize the dataset. rows &lt;- sample(nrow(Sonar)) Sonar &lt;- Sonar[rows,] # The split it into testing and training sets. split &lt;- round(nrow(Sonar)*0.60) train &lt;- Sonar[1:split,] test &lt;- Sonar[(split+1):nrow(Sonar),] nrow(train)/nrow(Sonar) ## [1] 0.6009615 model &lt;- glm( Class ~ ., family = binomial(link = &quot;logit&quot;), train ) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred p &lt;- predict(model, test, type = &quot;response&quot;) summary(p) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000000 0.000000 0.002428 0.438835 1.000000 1.000000 plot(p) p_class &lt;- as.factor(ifelse(p &gt; 0.1, &quot;M&quot;, &quot;R&quot;)) table(p_class, test[[&quot;Class&quot;]]) ## ## p_class M R ## M 11 26 ## R 35 11 confusionMatrix(p_class, test[[&quot;Class&quot;]]) ## Confusion Matrix and Statistics ## ## Reference ## Prediction M R ## M 11 26 ## R 35 11 ## ## Accuracy : 0.2651 ## 95% CI : (0.1742, 0.3734) ## No Information Rate : 0.5542 ## P-Value [Acc &gt; NIR] : 1.0000 ## ## Kappa : -0.4528 ## ## Mcnemar&#39;s Test P-Value : 0.3057 ## ## Sensitivity : 0.2391 ## Specificity : 0.2973 ## Pos Pred Value : 0.2973 ## Neg Pred Value : 0.2391 ## Prevalence : 0.5542 ## Detection Rate : 0.1325 ## Detection Prevalence : 0.4458 ## Balanced Accuracy : 0.2682 ## ## &#39;Positive&#39; Class : M ## According to Zach Mayer in DataCamp, manually evaluating classification threshholds is hard work and arbitrary: one would need to create dozens or hundreds of confusion matrices and then manually inspect them. Receiver operator curves add a new level of complexity and usefulness. To illustrate, we take the predicted probability of each Class for each case of the test set, and compare it with its actual Class: library(caTools) colAUC(p, test[[&quot;Class&quot;]], plotROC = TRUE) ## [,1] ## M vs. R 0.7532315 trainControl() in caret can use AUC (instead of accuracy), to tune the parameters of your models. The twoClassSummary() convenience function allows you to do this easily. When using twoClassSummary(), be sure to always include the argument classProbs = TRUE or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.) # Create trainControl object: myControl myControl &lt;- trainControl( method = &quot;cv&quot;, number = 10, summaryFunction = twoClassSummary, classProbs = TRUE, # IMPORTANT! verboseIter = TRUE ) # Train glm with custom trainControl: model model&lt;-train(Class~., data=Sonar, method=&quot;glm&quot;, trControl=myControl) ## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was not ## in the result set. ROC will be used instead. ## + Fold01: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold01: parameter=none ## + Fold02: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold02: parameter=none ## + Fold03: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold03: parameter=none ## + Fold04: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold04: parameter=none ## + Fold05: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold05: parameter=none ## + Fold06: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold06: parameter=none ## + Fold07: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold07: parameter=none ## + Fold08: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold08: parameter=none ## + Fold09: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold09: parameter=none ## + Fold10: parameter=none ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## - Fold10: parameter=none ## Aggregating results ## Fitting final model on full training set ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred # Print model to console model ## Generalized Linear Model ## ## 208 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 187, 187, 186, 188, 188, 187, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7221086 0.7378788 0.67 So twoClassSummary to use AUC to tune the parameters for the model generates a much more accurate model than our random assignment for p. 17.9 Random forest with caret For this set, we’ll use the white wine quality data set from UC Irvine. wine &lt;- read.csv(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&quot;, sep = &quot;;&quot;) glimpse(wine) ## Rows: 4,898 ## Columns: 12 ## $ fixed.acidity &lt;dbl&gt; 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0, 6.3, 8.1,… ## $ volatile.acidity &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.32, 0.27, 0… ## $ citric.acid &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.16, 0.36, 0… ## $ residual.sugar &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.00, 20.70,… ## $ chlorides &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.058, 0.050, 0.045, … ## $ free.sulfur.dioxide &lt;dbl&gt; 45, 14, 30, 47, 47, 30, 30, 45, 14, 28, 11, 17, 1… ## $ total.sulfur.dioxide &lt;dbl&gt; 170, 132, 97, 186, 186, 97, 136, 170, 132, 129, 6… ## $ density &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9956, 0.9951, 0… ## $ pH &lt;dbl&gt; 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.18, 3.00, 3… ## $ sulphates &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.47, 0.45, 0… ## $ alcohol &lt;dbl&gt; 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.8, 9.5, 11… ## $ quality &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6… # Note that quality is an integer, not a factor. **caret** seems to take this in stride. # Fit random forest: model model &lt;- train( quality~., tuneLength = 1, data = wine, method = &quot;ranger&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: mtry=3, min.node.size=5, splitrule=variance ## - Fold1: mtry=3, min.node.size=5, splitrule=variance ## + Fold1: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold1: mtry=3, min.node.size=5, splitrule=extratrees ## + Fold2: mtry=3, min.node.size=5, splitrule=variance ## - Fold2: mtry=3, min.node.size=5, splitrule=variance ## + Fold2: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold2: mtry=3, min.node.size=5, splitrule=extratrees ## + Fold3: mtry=3, min.node.size=5, splitrule=variance ## - Fold3: mtry=3, min.node.size=5, splitrule=variance ## + Fold3: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold3: mtry=3, min.node.size=5, splitrule=extratrees ## + Fold4: mtry=3, min.node.size=5, splitrule=variance ## - Fold4: mtry=3, min.node.size=5, splitrule=variance ## + Fold4: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold4: mtry=3, min.node.size=5, splitrule=extratrees ## + Fold5: mtry=3, min.node.size=5, splitrule=variance ## - Fold5: mtry=3, min.node.size=5, splitrule=variance ## + Fold5: mtry=3, min.node.size=5, splitrule=extratrees ## - Fold5: mtry=3, min.node.size=5, splitrule=extratrees ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 3, splitrule = variance, min.node.size = 5 on full training set # Print model to console model ## Random Forest ## ## 4898 samples ## 11 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3918, 3918, 3919, 3919, 3918 ## Resampling results across tuning parameters: ## ## splitrule RMSE Rsquared MAE ## variance 0.6050967 0.5413882 0.4391071 ## extratrees 0.6153818 0.5364792 0.4578305 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 3 ## Tuning ## parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 3, splitrule = variance ## and min.node.size = 5. plot(model) Let’s try this again with a longer tuneLength. This explores more models and potentially finds a better model. model &lt;- train( quality~., tuneLength = 10, data = wine, method = &quot;ranger&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: mtry= 2, min.node.size=5, splitrule=variance ## - Fold1: mtry= 2, min.node.size=5, splitrule=variance ## + Fold1: mtry= 3, min.node.size=5, splitrule=variance ## - Fold1: mtry= 3, min.node.size=5, splitrule=variance ## + Fold1: mtry= 4, min.node.size=5, splitrule=variance ## - Fold1: mtry= 4, min.node.size=5, splitrule=variance ## + Fold1: mtry= 5, min.node.size=5, splitrule=variance ## - Fold1: mtry= 5, min.node.size=5, splitrule=variance ## + Fold1: mtry= 6, min.node.size=5, splitrule=variance ## - Fold1: mtry= 6, min.node.size=5, splitrule=variance ## + Fold1: mtry= 7, min.node.size=5, splitrule=variance ## - Fold1: mtry= 7, min.node.size=5, splitrule=variance ## + Fold1: mtry= 8, min.node.size=5, splitrule=variance ## - Fold1: mtry= 8, min.node.size=5, splitrule=variance ## + Fold1: mtry= 9, min.node.size=5, splitrule=variance ## - Fold1: mtry= 9, min.node.size=5, splitrule=variance ## + Fold1: mtry=10, min.node.size=5, splitrule=variance ## - Fold1: mtry=10, min.node.size=5, splitrule=variance ## + Fold1: mtry=11, min.node.size=5, splitrule=variance ## - Fold1: mtry=11, min.node.size=5, splitrule=variance ## + Fold1: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold1: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold1: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold1: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold1: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold1: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold1: mtry=11, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 2, min.node.size=5, splitrule=variance ## - Fold2: mtry= 2, min.node.size=5, splitrule=variance ## + Fold2: mtry= 3, min.node.size=5, splitrule=variance ## - Fold2: mtry= 3, min.node.size=5, splitrule=variance ## + Fold2: mtry= 4, min.node.size=5, splitrule=variance ## - Fold2: mtry= 4, min.node.size=5, splitrule=variance ## + Fold2: mtry= 5, min.node.size=5, splitrule=variance ## - Fold2: mtry= 5, min.node.size=5, splitrule=variance ## + Fold2: mtry= 6, min.node.size=5, splitrule=variance ## - Fold2: mtry= 6, min.node.size=5, splitrule=variance ## + Fold2: mtry= 7, min.node.size=5, splitrule=variance ## - Fold2: mtry= 7, min.node.size=5, splitrule=variance ## + Fold2: mtry= 8, min.node.size=5, splitrule=variance ## - Fold2: mtry= 8, min.node.size=5, splitrule=variance ## + Fold2: mtry= 9, min.node.size=5, splitrule=variance ## - Fold2: mtry= 9, min.node.size=5, splitrule=variance ## + Fold2: mtry=10, min.node.size=5, splitrule=variance ## - Fold2: mtry=10, min.node.size=5, splitrule=variance ## + Fold2: mtry=11, min.node.size=5, splitrule=variance ## - Fold2: mtry=11, min.node.size=5, splitrule=variance ## + Fold2: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold2: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold2: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold2: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold2: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold2: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold2: mtry=11, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 2, min.node.size=5, splitrule=variance ## - Fold3: mtry= 2, min.node.size=5, splitrule=variance ## + Fold3: mtry= 3, min.node.size=5, splitrule=variance ## - Fold3: mtry= 3, min.node.size=5, splitrule=variance ## + Fold3: mtry= 4, min.node.size=5, splitrule=variance ## - Fold3: mtry= 4, min.node.size=5, splitrule=variance ## + Fold3: mtry= 5, min.node.size=5, splitrule=variance ## - Fold3: mtry= 5, min.node.size=5, splitrule=variance ## + Fold3: mtry= 6, min.node.size=5, splitrule=variance ## - Fold3: mtry= 6, min.node.size=5, splitrule=variance ## + Fold3: mtry= 7, min.node.size=5, splitrule=variance ## - Fold3: mtry= 7, min.node.size=5, splitrule=variance ## + Fold3: mtry= 8, min.node.size=5, splitrule=variance ## - Fold3: mtry= 8, min.node.size=5, splitrule=variance ## + Fold3: mtry= 9, min.node.size=5, splitrule=variance ## - Fold3: mtry= 9, min.node.size=5, splitrule=variance ## + Fold3: mtry=10, min.node.size=5, splitrule=variance ## - Fold3: mtry=10, min.node.size=5, splitrule=variance ## + Fold3: mtry=11, min.node.size=5, splitrule=variance ## - Fold3: mtry=11, min.node.size=5, splitrule=variance ## + Fold3: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold3: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold3: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold3: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold3: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold3: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold3: mtry=11, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 2, min.node.size=5, splitrule=variance ## - Fold4: mtry= 2, min.node.size=5, splitrule=variance ## + Fold4: mtry= 3, min.node.size=5, splitrule=variance ## - Fold4: mtry= 3, min.node.size=5, splitrule=variance ## + Fold4: mtry= 4, min.node.size=5, splitrule=variance ## - Fold4: mtry= 4, min.node.size=5, splitrule=variance ## + Fold4: mtry= 5, min.node.size=5, splitrule=variance ## - Fold4: mtry= 5, min.node.size=5, splitrule=variance ## + Fold4: mtry= 6, min.node.size=5, splitrule=variance ## - Fold4: mtry= 6, min.node.size=5, splitrule=variance ## + Fold4: mtry= 7, min.node.size=5, splitrule=variance ## - Fold4: mtry= 7, min.node.size=5, splitrule=variance ## + Fold4: mtry= 8, min.node.size=5, splitrule=variance ## - Fold4: mtry= 8, min.node.size=5, splitrule=variance ## + Fold4: mtry= 9, min.node.size=5, splitrule=variance ## - Fold4: mtry= 9, min.node.size=5, splitrule=variance ## + Fold4: mtry=10, min.node.size=5, splitrule=variance ## - Fold4: mtry=10, min.node.size=5, splitrule=variance ## + Fold4: mtry=11, min.node.size=5, splitrule=variance ## - Fold4: mtry=11, min.node.size=5, splitrule=variance ## + Fold4: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold4: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold4: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold4: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold4: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold4: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold4: mtry=11, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 2, min.node.size=5, splitrule=variance ## - Fold5: mtry= 2, min.node.size=5, splitrule=variance ## + Fold5: mtry= 3, min.node.size=5, splitrule=variance ## - Fold5: mtry= 3, min.node.size=5, splitrule=variance ## + Fold5: mtry= 4, min.node.size=5, splitrule=variance ## - Fold5: mtry= 4, min.node.size=5, splitrule=variance ## + Fold5: mtry= 5, min.node.size=5, splitrule=variance ## - Fold5: mtry= 5, min.node.size=5, splitrule=variance ## + Fold5: mtry= 6, min.node.size=5, splitrule=variance ## - Fold5: mtry= 6, min.node.size=5, splitrule=variance ## + Fold5: mtry= 7, min.node.size=5, splitrule=variance ## - Fold5: mtry= 7, min.node.size=5, splitrule=variance ## + Fold5: mtry= 8, min.node.size=5, splitrule=variance ## - Fold5: mtry= 8, min.node.size=5, splitrule=variance ## + Fold5: mtry= 9, min.node.size=5, splitrule=variance ## - Fold5: mtry= 9, min.node.size=5, splitrule=variance ## + Fold5: mtry=10, min.node.size=5, splitrule=variance ## - Fold5: mtry=10, min.node.size=5, splitrule=variance ## + Fold5: mtry=11, min.node.size=5, splitrule=variance ## - Fold5: mtry=11, min.node.size=5, splitrule=variance ## + Fold5: mtry= 2, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 2, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 3, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 3, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 4, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 4, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 5, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 5, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 6, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 6, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 7, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 7, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 8, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 8, min.node.size=5, splitrule=extratrees ## + Fold5: mtry= 9, min.node.size=5, splitrule=extratrees ## - Fold5: mtry= 9, min.node.size=5, splitrule=extratrees ## + Fold5: mtry=10, min.node.size=5, splitrule=extratrees ## - Fold5: mtry=10, min.node.size=5, splitrule=extratrees ## + Fold5: mtry=11, min.node.size=5, splitrule=extratrees ## - Fold5: mtry=11, min.node.size=5, splitrule=extratrees ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 4, splitrule = variance, min.node.size = 5 on full training set # Print model to console model ## Random Forest ## ## 4898 samples ## 11 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3919, 3918, 3918, 3918, 3919 ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 0.6057267 0.5439626 0.4405916 ## 2 extratrees 0.6236281 0.5298696 0.4653787 ## 3 variance 0.6046583 0.5423307 0.4382451 ## 3 extratrees 0.6156828 0.5361785 0.4564330 ## 4 variance 0.6043205 0.5415035 0.4369536 ## 4 extratrees 0.6121588 0.5385053 0.4515880 ## 5 variance 0.6055318 0.5381417 0.4372017 ## 5 extratrees 0.6105919 0.5382888 0.4489989 ## 6 variance 0.6059000 0.5371010 0.4376776 ## 6 extratrees 0.6100430 0.5376337 0.4473954 ## 7 variance 0.6053503 0.5373499 0.4365124 ## 7 extratrees 0.6103668 0.5356061 0.4465320 ## 8 variance 0.6058807 0.5364227 0.4372000 ## 8 extratrees 0.6088559 0.5371926 0.4442014 ## 9 variance 0.6084825 0.5316505 0.4382170 ## 9 extratrees 0.6090639 0.5360376 0.4442829 ## 10 variance 0.6093909 0.5296893 0.4394500 ## 10 extratrees 0.6095042 0.5344968 0.4435984 ## 11 variance 0.6096074 0.5291148 0.4389384 ## 11 extratrees 0.6088682 0.5352417 0.4435641 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 4, splitrule = variance ## and min.node.size = 5. plot(model) 17.10 Random forest model and tuneGrid() Custom tuning of grids can be used in caret by using tuneGrid(). While it is the most flexible method for fitting caret models and allows complete control over how the model is fit, it requires significant knowledge of the model and dramatically increases run time. For my uses, most of the time I anticipate using tuneLength and the default settings in caret to build my random forest models. tuneGrid &lt;- data.frame( .mtry = c(2, 3, 7), .splitrule = &quot;variance&quot;, .min.node.size = 5 ) # Fit random forest: model model &lt;- train( quality ~ ., tuneGrid = tuneGrid, data = wine, method = &quot;ranger&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) ) ## + Fold1: mtry=2, splitrule=variance, min.node.size=5 ## - Fold1: mtry=2, splitrule=variance, min.node.size=5 ## + Fold1: mtry=3, splitrule=variance, min.node.size=5 ## - Fold1: mtry=3, splitrule=variance, min.node.size=5 ## + Fold1: mtry=7, splitrule=variance, min.node.size=5 ## - Fold1: mtry=7, splitrule=variance, min.node.size=5 ## + Fold2: mtry=2, splitrule=variance, min.node.size=5 ## - Fold2: mtry=2, splitrule=variance, min.node.size=5 ## + Fold2: mtry=3, splitrule=variance, min.node.size=5 ## - Fold2: mtry=3, splitrule=variance, min.node.size=5 ## + Fold2: mtry=7, splitrule=variance, min.node.size=5 ## - Fold2: mtry=7, splitrule=variance, min.node.size=5 ## + Fold3: mtry=2, splitrule=variance, min.node.size=5 ## - Fold3: mtry=2, splitrule=variance, min.node.size=5 ## + Fold3: mtry=3, splitrule=variance, min.node.size=5 ## - Fold3: mtry=3, splitrule=variance, min.node.size=5 ## + Fold3: mtry=7, splitrule=variance, min.node.size=5 ## - Fold3: mtry=7, splitrule=variance, min.node.size=5 ## + Fold4: mtry=2, splitrule=variance, min.node.size=5 ## - Fold4: mtry=2, splitrule=variance, min.node.size=5 ## + Fold4: mtry=3, splitrule=variance, min.node.size=5 ## - Fold4: mtry=3, splitrule=variance, min.node.size=5 ## + Fold4: mtry=7, splitrule=variance, min.node.size=5 ## - Fold4: mtry=7, splitrule=variance, min.node.size=5 ## + Fold5: mtry=2, splitrule=variance, min.node.size=5 ## - Fold5: mtry=2, splitrule=variance, min.node.size=5 ## + Fold5: mtry=3, splitrule=variance, min.node.size=5 ## - Fold5: mtry=3, splitrule=variance, min.node.size=5 ## + Fold5: mtry=7, splitrule=variance, min.node.size=5 ## - Fold5: mtry=7, splitrule=variance, min.node.size=5 ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 3, splitrule = variance, min.node.size = 5 on full training set # Print model to console model ## Random Forest ## ## 4898 samples ## 11 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3918, 3918, 3919, 3918, 3919 ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 0.6076591 0.5397497 0.4417339 ## 3 0.6056877 0.5399161 0.4378556 ## 7 0.6083201 0.5319077 0.4367244 ## ## Tuning parameter &#39;splitrule&#39; was held constant at a value of variance ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 3, splitrule = variance ## and min.node.size = 5. # Plot model plot(model) "],["moremodeling.html", "Chapter 18 More modeling 18.1 glmnet models 18.2 Imputation discussion 18.3 Multiple preprocessing methods 18.4 Handling low-information predictors 18.5 Max Kuhn on reusing a trainControl 18.6 Comparing models", " Chapter 18 More modeling library(tidyverse) library(caret) 18.1 glmnet models glmnet is an extension of glm models with built-in variable selection, which could be really nice for pituitary adenoma classification purposes. glmnet helps deal with colinearity (correlation between predictors in a model) and small samples, both of which are relevant to my pituitary adenoma data. glmnet relies on two forms of regression: Lasso regression, which penalizes the number of nonzero coefficients Ridge regression, which penalizes the absolute magnitude of the coefficients Both sorts of regression can be combined glmnet attempst to find a simple model with few nonzero coefficients or small absolute magnitude of coefficients. glmnet pairs well with random forest models, since it often yields different results. 18.1.1 Parameters for tuning glmnet models alpha(0,1): pure ridge at zero, pure lasso at one lambda(0,infinity): size of the penalty For a single value of alpha, glmnet fits all values of lambda simultaneously. This is “many models for the price of one.” This is known as the “submodel trick.” Zach Mayer uses a dataset that I can’t seem to locate, so I’ll switch to the code used by Stefania Ashby, which uses iris: https://neurospection.netlify.app/post/machine-learning-basics-with-caret/#a-classification-example 18.1.2 An iris example of glmnet Ashby terms this use of glmnet as an elastic net. In keeping with her practice, which splits the dataset in order to have a test group on which to estimate the accuracy of the model, we’ll start with that (comments are Ashby’s): iris_data &lt;- data.frame(iris) set.seed(28) # Split the original dataset into a training set and a testing set # We are using species to partition so that we don&#39;t end up with an uneven amount of one species in either training or testing sets. partition_data &lt;- createDataPartition(iris_data$Species, times = 1, p = .7, list = FALSE) # Assign sets training.set &lt;- iris_data[partition_data, ] # Training set testing.set &lt;- iris_data[-partition_data, ] # Testing set # Sanity Check: Is data partitioned appropriately, do we have equal numbers of observations for our outcome variable? nrow(training.set) ## [1] 105 summary(training.set$Species) ## setosa versicolor virginica ## 35 35 35 nrow(testing.set) ## [1] 45 summary(testing.set$Species) ## setosa versicolor virginica ## 15 15 15 Now to build Ashby’s model: # Specify the cross-validation method(s) train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10, # k-folds CV with k=10 classProbs = TRUE, savePredictions = TRUE, summaryFunction = multiClassSummary)# save predictions for ROC train.control2 &lt;- trainControl(method = &quot;LOOCV&quot;, classProbs = TRUE, savePredictions = TRUE, summaryFunction = multiClassSummary) # leave-one-out CV, and save predictions for ROC # Example Model Tuning for Elastic Net glmnet.info &lt;- getModelInfo(&quot;glmnet&quot;) glmnet.info$glmnet$parameters ## parameter class label ## 1 alpha numeric Mixing Percentage ## 2 lambda numeric Regularization Parameter tune.grid &lt;- expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 100)) # Use the train function to perform model training glmnet.model &lt;- train(Species ~. , data = training.set, method = &quot;glmnet&quot;, trControl = train.control2, # change this to train.control to try k-fold CV #tuneGrid = tune.grid, preProc = c(&quot;center&quot;)) # Look at the results from model training and ROC Curves glmnet.model ## glmnet ## ## 105 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## Pre-processing: centered (4) ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 104, 104, 104, 104, 104, 104, ... ## Resampling results across tuning parameters: ## ## alpha lambda logLoss AUC prAUC Accuracy Kappa ## 0.10 0.0008702402 0.08736163 0.9971429 0.9660011 0.9619048 0.9428571 ## 0.10 0.0087024017 0.16409724 0.9959184 0.9637131 0.9523810 0.9285714 ## 0.10 0.0870240166 0.37866105 0.9752381 0.9178787 0.8952381 0.8428571 ## 0.55 0.0008702402 0.08010236 0.9971429 0.9660011 0.9619048 0.9428571 ## 0.55 0.0087024017 0.14944538 0.9961905 0.9641325 0.9523810 0.9285714 ## 0.55 0.0870240166 0.40645575 0.9778231 0.9189341 0.9142857 0.8714286 ## 1.00 0.0008702402 0.07535390 0.9975510 0.9667069 0.9619048 0.9428571 ## 1.00 0.0087024017 0.12708308 0.9953741 0.9626165 0.9523810 0.9285714 ## 1.00 0.0870240166 0.41964944 0.9790476 0.9140139 0.9142857 0.8714286 ## Mean_F1 Mean_Sensitivity Mean_Specificity Mean_Pos_Pred_Value ## 0.9618736 0.9619048 0.9809524 0.9628720 ## 0.9523712 0.9523810 0.9761905 0.9526144 ## 0.8957327 0.8952381 0.9476190 0.8969263 ## 0.9618736 0.9619048 0.9809524 0.9628720 ## 0.9523712 0.9523810 0.9761905 0.9526144 ## 0.9141280 0.9142857 0.9571429 0.9161184 ## 0.9618736 0.9619048 0.9809524 0.9628720 ## 0.9523712 0.9523810 0.9761905 0.9526144 ## 0.9142682 0.9142857 0.9571429 0.9144880 ## Mean_Neg_Pred_Value Mean_Precision Mean_Recall Mean_Detection_Rate ## 0.9812092 0.9628720 0.9619048 0.3206349 ## 0.9762537 0.9526144 0.9523810 0.3174603 ## 0.9475283 0.8969263 0.8952381 0.2984127 ## 0.9812092 0.9628720 0.9619048 0.3206349 ## 0.9762537 0.9526144 0.9523810 0.3174603 ## 0.9576774 0.9161184 0.9142857 0.3047619 ## 0.9812092 0.9628720 0.9619048 0.3206349 ## 0.9762537 0.9526144 0.9523810 0.3174603 ## 0.9572022 0.9144880 0.9142857 0.3047619 ## Mean_Balanced_Accuracy ## 0.9714286 ## 0.9642857 ## 0.9214286 ## 0.9714286 ## 0.9642857 ## 0.9357143 ## 0.9714286 ## 0.9642857 ## 0.9357143 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 0.1 and lambda = 0.0008702402. # Test the predictive ability of the model in the testing set glmnet.predict &lt;- predict(glmnet.model, testing.set) # Predict values in the testing set postResample(glmnet.predict, testing.set$Species) # the accuracy of the model ## Accuracy Kappa ## 0.9777778 0.9666667 confusionMatrix(glmnet.predict, testing.set$Species) # Lets see the breakdown of how well our model worked ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 14 0 ## virginica 0 1 15 ## ## Overall Statistics ## ## Accuracy : 0.9778 ## 95% CI : (0.8823, 0.9994) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9667 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 0.9333 1.0000 ## Specificity 1.0000 1.0000 0.9667 ## Pos Pred Value 1.0000 1.0000 0.9375 ## Neg Pred Value 1.0000 0.9677 1.0000 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3111 0.3333 ## Detection Prevalence 0.3333 0.3111 0.3556 ## Balanced Accuracy 1.0000 0.9667 0.9833 Ashby’s example helps out a lot and should be used as a model of the workflow for studying pitadtma. Imputation will be necessary to work with that dataset, so I’ll return to the DataCamp course for that. 18.2 Imputation discussion This example shows how to generate some missing values, and to use imputation overcome these. data(&quot;mtcars&quot;) set.seed(42) mtcars[sample(1:nrow(mtcars), 10), &quot;hp&quot;] &lt;- NA Y &lt;- mtcars$mpg X &lt;- mtcars[,2:4] # model &lt;- train(X,Y) #This fails, because of the NAs. Try imputation. median_model &lt;- train( X, Y, preProcess = &quot;medianImpute&quot; ) ## note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 . print(median_model) ## Random Forest ## ## 32 samples ## 3 predictor ## ## Pre-processing: median imputation (3) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 32, 32, 32, 32, 32, 32, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 2.758439 0.8106146 2.260273 ## 3 2.729423 0.8123802 2.224138 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 3. Data that is missing not at random offers special challenges. mtcars[mtcars$disp &lt; 140, &quot;hp&quot;] &lt;- NA Y &lt;- mtcars$mpg X &lt;- mtcars[,2:4] # Start with median imputation: model &lt;- train( X, Y, method = &quot;glm&quot;, preProcess = &quot;medianImpute&quot; ) print(min(model$results$RMSE)) ## [1] 3.437128 set.seed(42) model &lt;- train( X, Y, method = &quot;glm&quot;, preProcess = c(&quot;knnImpute&quot;) ) print(min(model$results$RMSE)) ## [1] 3.135958 18.3 Multiple preprocessing methods Zach Mayer offers the following cheat sheet for preprocessing: Start with median imputation (if you’re using it) Try KNN imputation if data NOT missing at random For linear models (lm, glm, glmnet) always center and scale Tree-based models (random forest, gbm) don’t need much preprocessing data(&quot;mtcars&quot;) set.seed(42) mtcars[sample(1:nrow(mtcars), 10), &quot;hp&quot;] &lt;- NA Y &lt;- mtcars$mpg X &lt;- mtcars[,2:4] # missing at random set.seed(42) model &lt;- train( X, Y, method = &quot;glm&quot;, preProcess = c(&quot;medianImpute&quot;, &quot;center&quot;, &quot;scale&quot;) ) print(min(model$results$RMSE)) ## [1] 3.079732 set.seed(42) model &lt;- train( X, Y, method = &quot;glm&quot;, preProcess = c(&quot;medianImpute&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;) ) print(min(model$results$RMSE)) # with pca applied ## [1] 3.03581 18.4 Handling low-information predictors Constant or nearly constant values mess up models, or at the very least provide little useful information. They are usually removed, at least when addressing balanced datasets. For imbalanced datasets, leaving near zero variance predictors in the dataset is probably advised. NOTE: using PCA as a preprocessing step can accomplish much the same thing as removing near zero variance predictors, so is worth trying. See above for an example using PCA. 18.5 Max Kuhn on reusing a trainControl Note that the outcome (churn/no churn) is binary and that the dataset is somewhat imbalanced. library(C50) library(modeldata) data(mlc_churn) set.seed(1) inTrainingSet &lt;- createDataPartition(mlc_churn$churn, p = 0.75, list = FALSE) churnTrain &lt;- mlc_churn[inTrainingSet,] churnTest &lt;- mlc_churn[-inTrainingSet,] glimpse(churnTrain) ## Rows: 3,751 ## Columns: 20 ## $ state &lt;fct&gt; KS, NJ, OH, MA, LA, IN, RI, IA, IA, NY, … ## $ account_length &lt;int&gt; 128, 137, 84, 121, 117, 65, 74, 168, 62,… ## $ area_code &lt;fct&gt; area_code_415, area_code_415, area_code_… ## $ international_plan &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no,… ## $ voice_mail_plan &lt;fct&gt; yes, no, no, yes, no, no, no, no, no, no… ## $ number_vmail_messages &lt;int&gt; 25, 0, 0, 24, 0, 0, 0, 0, 0, 0, 27, 0, 0… ## $ total_day_minutes &lt;dbl&gt; 265.1, 243.4, 299.4, 218.2, 184.5, 129.1… ## $ total_day_calls &lt;int&gt; 110, 114, 71, 88, 97, 137, 127, 96, 70, … ## $ total_day_charge &lt;dbl&gt; 45.07, 41.38, 50.90, 37.09, 31.37, 21.95… ## $ total_eve_minutes &lt;dbl&gt; 197.4, 121.2, 61.9, 348.5, 351.6, 228.5,… ## $ total_eve_calls &lt;int&gt; 99, 110, 88, 108, 80, 83, 148, 71, 76, 9… ## $ total_eve_charge &lt;dbl&gt; 16.78, 10.30, 5.26, 29.62, 29.89, 19.42,… ## $ total_night_minutes &lt;dbl&gt; 244.7, 162.6, 196.9, 212.6, 215.8, 208.8… ## $ total_night_calls &lt;int&gt; 91, 104, 89, 118, 90, 111, 94, 128, 99, … ## $ total_night_charge &lt;dbl&gt; 11.01, 7.32, 8.86, 9.57, 9.71, 9.40, 8.8… ## $ total_intl_minutes &lt;dbl&gt; 10.0, 12.2, 6.6, 7.5, 8.7, 12.7, 9.1, 11… ## $ total_intl_calls &lt;int&gt; 3, 5, 7, 7, 4, 6, 5, 2, 6, 9, 4, 3, 2, 4… ## $ total_intl_charge &lt;dbl&gt; 2.70, 3.29, 1.78, 2.03, 2.35, 3.43, 2.46… ## $ number_customer_service_calls &lt;int&gt; 1, 0, 2, 3, 1, 4, 0, 1, 4, 4, 1, 3, 1, 0… ## $ churn &lt;fct&gt; no, no, no, no, no, yes, no, no, no, yes… glimpse(churnTest) ## Rows: 1,249 ## Columns: 20 ## $ state &lt;fct&gt; OH, OK, AL, MO, WV, MT, VA, AZ, SC, WY, … ## $ account_length &lt;int&gt; 107, 75, 118, 147, 141, 95, 76, 130, 111… ## $ area_code &lt;fct&gt; area_code_415, area_code_415, area_code_… ## $ international_plan &lt;fct&gt; no, yes, yes, yes, yes, no, no, no, no, … ## $ voice_mail_plan &lt;fct&gt; yes, no, no, no, yes, no, yes, no, no, y… ## $ number_vmail_messages &lt;int&gt; 26, 0, 0, 0, 37, 0, 33, 0, 0, 39, 33, 0,… ## $ total_day_minutes &lt;dbl&gt; 161.6, 166.7, 223.4, 157.0, 258.6, 156.6… ## $ total_day_calls &lt;int&gt; 123, 113, 98, 79, 84, 88, 66, 112, 103, … ## $ total_day_charge &lt;dbl&gt; 27.47, 28.34, 37.98, 26.69, 43.96, 26.62… ## $ total_eve_minutes &lt;dbl&gt; 195.5, 148.3, 220.6, 103.1, 222.0, 247.6… ## $ total_eve_calls &lt;int&gt; 103, 122, 101, 94, 111, 75, 65, 99, 102,… ## $ total_eve_charge &lt;dbl&gt; 16.62, 12.61, 18.75, 8.76, 18.87, 21.05,… ## $ total_night_minutes &lt;dbl&gt; 254.4, 186.9, 203.9, 211.8, 326.4, 192.3… ## $ total_night_calls &lt;int&gt; 103, 121, 118, 96, 97, 115, 108, 78, 105… ## $ total_night_charge &lt;dbl&gt; 11.45, 8.41, 9.18, 9.53, 14.69, 8.65, 7.… ## $ total_intl_minutes &lt;dbl&gt; 13.7, 10.1, 6.3, 7.1, 11.2, 12.3, 10.0, … ## $ total_intl_calls &lt;int&gt; 3, 3, 6, 6, 5, 5, 5, 19, 6, 3, 6, 2, 5, … ## $ total_intl_charge &lt;dbl&gt; 3.70, 2.73, 1.70, 1.92, 3.02, 3.32, 2.70… ## $ number_customer_service_calls &lt;int&gt; 1, 3, 0, 0, 0, 3, 1, 0, 2, 0, 3, 3, 3, 3… ## $ churn &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, … table(churnTrain$churn) / nrow(churnTrain) ## ## yes no ## 0.1415623 0.8584377 table(churnTest$churn) / nrow(churnTest) ## ## yes no ## 0.1409127 0.8590873 So the test and training sets have approximately equal proportions of churn. Now, in order to reuse trainControl, it’s helpful to be certain that the model encounters exactly the same folds in cross validation. myFolds &lt;- createFolds(churnTrain$churn, k = 5) # Compare class distribution i &lt;- myFolds$Fold1 table(churnTrain$churn[i]) / length(i) ## ## yes no ## 0.1413333 0.8586667 myControl &lt;- trainControl( summaryFunction = twoClassSummary, classProbs = TRUE, #Important! verboseIter = TRUE, savePredictions = TRUE, index = myFolds ) Now that the folds and trainControl are established, we can go on to make some models. First a glmnet model: set.seed(42) model_glmnet &lt;- train( churn ~ ., churnTrain, metric = &quot;ROC&quot;, method = &quot;glmnet&quot;, tune.grid = expand.grid( alpha = 0:1, lambda = 0:10 / 10 ), trControl = myControl ) ## + Fold1: alpha=0.10, lambda=0.01756 ## - Fold1: alpha=0.10, lambda=0.01756 ## + Fold1: alpha=0.55, lambda=0.01756 ## - Fold1: alpha=0.55, lambda=0.01756 ## + Fold1: alpha=1.00, lambda=0.01756 ## - Fold1: alpha=1.00, lambda=0.01756 ## + Fold2: alpha=0.10, lambda=0.01756 ## - Fold2: alpha=0.10, lambda=0.01756 ## + Fold2: alpha=0.55, lambda=0.01756 ## - Fold2: alpha=0.55, lambda=0.01756 ## + Fold2: alpha=1.00, lambda=0.01756 ## - Fold2: alpha=1.00, lambda=0.01756 ## + Fold3: alpha=0.10, lambda=0.01756 ## - Fold3: alpha=0.10, lambda=0.01756 ## + Fold3: alpha=0.55, lambda=0.01756 ## - Fold3: alpha=0.55, lambda=0.01756 ## + Fold3: alpha=1.00, lambda=0.01756 ## - Fold3: alpha=1.00, lambda=0.01756 ## + Fold4: alpha=0.10, lambda=0.01756 ## - Fold4: alpha=0.10, lambda=0.01756 ## + Fold4: alpha=0.55, lambda=0.01756 ## - Fold4: alpha=0.55, lambda=0.01756 ## + Fold4: alpha=1.00, lambda=0.01756 ## - Fold4: alpha=1.00, lambda=0.01756 ## + Fold5: alpha=0.10, lambda=0.01756 ## - Fold5: alpha=0.10, lambda=0.01756 ## + Fold5: alpha=0.55, lambda=0.01756 ## - Fold5: alpha=0.55, lambda=0.01756 ## + Fold5: alpha=1.00, lambda=0.01756 ## - Fold5: alpha=1.00, lambda=0.01756 ## Aggregating results ## Selecting tuning parameters ## Fitting alpha = 1, lambda = 0.0176 on full training set plot(model_glmnet) plot(model_glmnet$finalModel) Let’s try a random forest model. set.seed(42) churnTrain$churn &lt;- factor( churnTrain$churn, levels = c(&quot;no&quot;, &quot;yes&quot;) ) model_rf &lt;- train( churn ~ ., churnTrain, metric = &quot;ROC&quot;, method = &quot;ranger&quot;, trControl = myControl ) ## + Fold1: mtry= 2, min.node.size=1, splitrule=gini ## - Fold1: mtry= 2, min.node.size=1, splitrule=gini ## + Fold1: mtry=35, min.node.size=1, splitrule=gini ## - Fold1: mtry=35, min.node.size=1, splitrule=gini ## + Fold1: mtry=69, min.node.size=1, splitrule=gini ## - Fold1: mtry=69, min.node.size=1, splitrule=gini ## + Fold1: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold1: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold1: mtry=35, min.node.size=1, splitrule=extratrees ## - Fold1: mtry=35, min.node.size=1, splitrule=extratrees ## + Fold1: mtry=69, min.node.size=1, splitrule=extratrees ## - Fold1: mtry=69, min.node.size=1, splitrule=extratrees ## + Fold2: mtry= 2, min.node.size=1, splitrule=gini ## - Fold2: mtry= 2, min.node.size=1, splitrule=gini ## + Fold2: mtry=35, min.node.size=1, splitrule=gini ## - Fold2: mtry=35, min.node.size=1, splitrule=gini ## + Fold2: mtry=69, min.node.size=1, splitrule=gini ## - Fold2: mtry=69, min.node.size=1, splitrule=gini ## + Fold2: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold2: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold2: mtry=35, min.node.size=1, splitrule=extratrees ## - Fold2: mtry=35, min.node.size=1, splitrule=extratrees ## + Fold2: mtry=69, min.node.size=1, splitrule=extratrees ## - Fold2: mtry=69, min.node.size=1, splitrule=extratrees ## + Fold3: mtry= 2, min.node.size=1, splitrule=gini ## - Fold3: mtry= 2, min.node.size=1, splitrule=gini ## + Fold3: mtry=35, min.node.size=1, splitrule=gini ## - Fold3: mtry=35, min.node.size=1, splitrule=gini ## + Fold3: mtry=69, min.node.size=1, splitrule=gini ## - Fold3: mtry=69, min.node.size=1, splitrule=gini ## + Fold3: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold3: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold3: mtry=35, min.node.size=1, splitrule=extratrees ## - Fold3: mtry=35, min.node.size=1, splitrule=extratrees ## + Fold3: mtry=69, min.node.size=1, splitrule=extratrees ## - Fold3: mtry=69, min.node.size=1, splitrule=extratrees ## + Fold4: mtry= 2, min.node.size=1, splitrule=gini ## - Fold4: mtry= 2, min.node.size=1, splitrule=gini ## + Fold4: mtry=35, min.node.size=1, splitrule=gini ## - Fold4: mtry=35, min.node.size=1, splitrule=gini ## + Fold4: mtry=69, min.node.size=1, splitrule=gini ## - Fold4: mtry=69, min.node.size=1, splitrule=gini ## + Fold4: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold4: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold4: mtry=35, min.node.size=1, splitrule=extratrees ## - Fold4: mtry=35, min.node.size=1, splitrule=extratrees ## + Fold4: mtry=69, min.node.size=1, splitrule=extratrees ## - Fold4: mtry=69, min.node.size=1, splitrule=extratrees ## + Fold5: mtry= 2, min.node.size=1, splitrule=gini ## - Fold5: mtry= 2, min.node.size=1, splitrule=gini ## + Fold5: mtry=35, min.node.size=1, splitrule=gini ## - Fold5: mtry=35, min.node.size=1, splitrule=gini ## + Fold5: mtry=69, min.node.size=1, splitrule=gini ## - Fold5: mtry=69, min.node.size=1, splitrule=gini ## + Fold5: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold5: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold5: mtry=35, min.node.size=1, splitrule=extratrees ## - Fold5: mtry=35, min.node.size=1, splitrule=extratrees ## + Fold5: mtry=69, min.node.size=1, splitrule=extratrees ## - Fold5: mtry=69, min.node.size=1, splitrule=extratrees ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 35, splitrule = extratrees, min.node.size = 1 on full training set plot(model_rf) 18.6 Comparing models Fundamental to this is making sure that the models were fit on exactly the same data! We select the model with the highest AUC and the lowest standard deviation in AUC. The resamples() function is very useful. # First, make a list model_list &lt;- list( glmnet = model_glmnet, rf = model_rf ) # Collect resamples from the CV folds resamps &lt;- resamples(model_list) resamps ## ## Call: ## resamples.default(x = model_list) ## ## Models: glmnet, rf ## Number of resamples: 5 ## Performance metrics: ROC, Sens, Spec ## Time estimates for: everything, final model fit # Summarize the results summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: glmnet, rf ## Number of resamples: 5 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## glmnet 0.7813141 0.8045259 0.8045561 0.8015931 0.8076270 0.8099425 0 ## rf 0.8991007 0.9073443 0.9096319 0.9114213 0.9187639 0.9222657 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## glmnet 0.04705882 0.09647059 0.1152941 0.09934961 0.1179245 0.1200000 0 ## rf 0.98330745 0.98486025 0.9918478 0.98913043 0.9922360 0.9934006 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## glmnet 0.9798137 0.9802019 0.9860248 0.9851708 0.9871894 0.9926242 0 ## rf 0.5082353 0.5105882 0.5270588 0.5282664 0.5294118 0.5660377 0 bwplot(resamps) bwplot(resamps, metric = &quot;ROC&quot;) # The dotplot is the most elegant way to compare the performance of multiple models dotplot(resamps, metric = &quot;ROC&quot;) densityplot(resamps, metric = &quot;ROC&quot;) xyplot(resamps, metric = &quot;ROC&quot;) "],["github.html", "Chapter 19 Working with GitHub 19.1 Basic idea 19.2 A couple invaluable websites", " Chapter 19 Working with GitHub 19.1 Basic idea GitHub will no longer take a password, but needs a personal access token. 19.2 A couple invaluable websites For creating a personal access token: https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token For deploying a personal access token: https://happygitwithr.com/credential-caching.html#credential-caching I first explored my credentials with the following: # library(gitcreds) # # gitcreds_set() According to the error when I knit this, his only works in interactive sessions, so is commented out. This shows off my github password, with the right menu selection, but not the PAT. library(credentials) ## Found git version 2.30.1 (Apple Git-130) ## Supported HTTPS credential helpers: cache, store ## Found OpenSSH_8.1p1, LibreSSL 2.7.3 ## Default SSH key: /Users/wliamlaptop/.ssh/id_rsa set_github_pat() For Bill’s Drills, I set up this PAT as directed. It’s probably not a great idea to keep it in this document, but I’ll play around with it until I understand the rules and make sure that my Github is still working. ************** Note that this resulted in the revocation of my PAT. Drat! This is readily solved by getting another PAT from GitHub. More research is necessary. See: ?set_github_pat On returning home, I encountered trouble with R. This seems to have been settled by the old copy-and-paste the error into my browser and figure out a workaround that involved a couple different methods. Now I’ve generated another PAT for my home computer, and am hoping that GitHub will be happy with me now. "],["explainingmachinelearning.html", "Chapter 20 What is machine learning? 20.1 Machine learning for MD’s 20.2 Supervised versus unsuperised learning 20.3 Classification versus regression", " Chapter 20 What is machine learning? 20.1 Machine learning for MD’s David Robinson does the nicest job explaining the difference between artificial intelligence, machine learning, and data science in this post http://varianceexplained.org/r/ds-ml-ai/. My goal with this page is to convey these ideas in a way that is relevant to my colleagues in pathology, radiology, neuroradiology, oncology, neurosurgery, and neuro-oncology. Why? Because systems that understand these ideas will use them to improve patient care. Systems that don’t, won’t. As a matter of fact, systems that remain in the dark will provide increasingly expensive, dysfunctional, inefficient, and ineffective care in the pursuit of what they believe (incorrectly) to be quaternary care–whatever the hell that is. Generally, I think that David Robinson’s descriptive (as opposed to prescriptive) approach to these ideas is a good one. Prescriptive approaches are exceedingly complex and fragile. See, for instance, the editorial in the August 2021 issue of Neurosurgery: https://pubmed.ncbi.nlm.nih.gov/34015816/. The authors trade simplicity for breadth and appear to be motivated more by intellectual property concerns than by a desire to guide colleagues through the process of data collection, analysis, and modeling. Hadley Wickham and Garrett Grolemund’s R for Data Science or Gareth James et al.’s An Introduction to Statistical Learning offer much more investigator-based approaches to the field, even if the majority of their book can be said to be outside of the field of AI/ML, in the case of R4DS (probably only the chapters on modeling can be said to fit into ML). I will adopt Robinson’s convention: But let’s back up and ask: what role does DS/ML play? Drew Conway’s classic description of data science is helpful http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram. The Conway venn diagram looks like this: One of the best references for learning about machine learning is Greener et al(4). 20.2 Supervised versus unsuperised learning 20.3 Classification versus regression References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
