[["moremodeling.html", "Chapter 18 More modeling 18.1 glmnet models 18.2 Imputation discussion 18.3 Split target from predictors 18.4 Multiple preprocessing methods 18.5 Max Kuhn on reusing a trainControl", " Chapter 18 More modeling library(tidyverse) library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift 18.1 glmnet models glmnet is an extension of glm models with built-in variable selection, which could be really nice for pituitary adenoma classification purposes. glmnet helps deal with colinearity (correlation between predictors in a model) and small samples, both of which are relevant to my pituitary adenoma data. glmnet relies on two forms of regression: Lasso regression, which penalizes the number of nonzero coefficients Ridge regression, which penalizes the absolute magnitude of the coefficients Both sorts of regression can be combined glmnet attempst to find a simple model with few nonzero coefficients or small absolute magnitude of coefficients. glmnet pairs well with random forest models, since it often yields different results. 18.1.1 Parameters for tuning glmnet models alpha(0,1): pure ridge at zero, pure lasso at one lambda(0,infinity): size of the penalty For a single value of alpha, glmnet fits all values of lambda simultaneously. This is “many models for the price of one.” This is known as the “submodel trick.” Zach Mayer uses a dataset that I can’t seem to locate, so I’ll switch the terms somewhat and hazard using iris and a three-class problem, which may be outside of the scope of this code. 18.1.2 An iris example of glmnet myControl &lt;- trainControl( method = &quot;cv&quot;, number = 10, summaryFunction = defaultSummary, classProbs = TRUE, verboseIter = TRUE ) set.seed(42) model &lt;- train( Species ~ ., iris, method = &quot;glmnet&quot;, trControl = myControl ) ## + Fold01: alpha=0.10, lambda=0.087 ## - Fold01: alpha=0.10, lambda=0.087 ## + Fold01: alpha=0.55, lambda=0.087 ## - Fold01: alpha=0.55, lambda=0.087 ## + Fold01: alpha=1.00, lambda=0.087 ## - Fold01: alpha=1.00, lambda=0.087 ## + Fold02: alpha=0.10, lambda=0.087 ## - Fold02: alpha=0.10, lambda=0.087 ## + Fold02: alpha=0.55, lambda=0.087 ## - Fold02: alpha=0.55, lambda=0.087 ## + Fold02: alpha=1.00, lambda=0.087 ## - Fold02: alpha=1.00, lambda=0.087 ## + Fold03: alpha=0.10, lambda=0.087 ## - Fold03: alpha=0.10, lambda=0.087 ## + Fold03: alpha=0.55, lambda=0.087 ## - Fold03: alpha=0.55, lambda=0.087 ## + Fold03: alpha=1.00, lambda=0.087 ## - Fold03: alpha=1.00, lambda=0.087 ## + Fold04: alpha=0.10, lambda=0.087 ## - Fold04: alpha=0.10, lambda=0.087 ## + Fold04: alpha=0.55, lambda=0.087 ## - Fold04: alpha=0.55, lambda=0.087 ## + Fold04: alpha=1.00, lambda=0.087 ## - Fold04: alpha=1.00, lambda=0.087 ## + Fold05: alpha=0.10, lambda=0.087 ## - Fold05: alpha=0.10, lambda=0.087 ## + Fold05: alpha=0.55, lambda=0.087 ## - Fold05: alpha=0.55, lambda=0.087 ## + Fold05: alpha=1.00, lambda=0.087 ## - Fold05: alpha=1.00, lambda=0.087 ## + Fold06: alpha=0.10, lambda=0.087 ## - Fold06: alpha=0.10, lambda=0.087 ## + Fold06: alpha=0.55, lambda=0.087 ## - Fold06: alpha=0.55, lambda=0.087 ## + Fold06: alpha=1.00, lambda=0.087 ## - Fold06: alpha=1.00, lambda=0.087 ## + Fold07: alpha=0.10, lambda=0.087 ## - Fold07: alpha=0.10, lambda=0.087 ## + Fold07: alpha=0.55, lambda=0.087 ## - Fold07: alpha=0.55, lambda=0.087 ## + Fold07: alpha=1.00, lambda=0.087 ## - Fold07: alpha=1.00, lambda=0.087 ## + Fold08: alpha=0.10, lambda=0.087 ## - Fold08: alpha=0.10, lambda=0.087 ## + Fold08: alpha=0.55, lambda=0.087 ## - Fold08: alpha=0.55, lambda=0.087 ## + Fold08: alpha=1.00, lambda=0.087 ## - Fold08: alpha=1.00, lambda=0.087 ## + Fold09: alpha=0.10, lambda=0.087 ## - Fold09: alpha=0.10, lambda=0.087 ## + Fold09: alpha=0.55, lambda=0.087 ## - Fold09: alpha=0.55, lambda=0.087 ## + Fold09: alpha=1.00, lambda=0.087 ## - Fold09: alpha=1.00, lambda=0.087 ## + Fold10: alpha=0.10, lambda=0.087 ## - Fold10: alpha=0.10, lambda=0.087 ## + Fold10: alpha=0.55, lambda=0.087 ## - Fold10: alpha=0.55, lambda=0.087 ## + Fold10: alpha=1.00, lambda=0.087 ## - Fold10: alpha=1.00, lambda=0.087 ## Aggregating results ## Selecting tuning parameters ## Fitting alpha = 0.1, lambda = 0.00087 on full training set plot(model) 18.1.3 Now using a tuning grid # Train glmnet with custom trainControl and tuning: model model &lt;- train( Species~., iris, tuneGrid = expand.grid( alpha=0:1, lambda=seq(0.0001, 1, length=20) ), method = &quot;glmnet&quot;, trControl = myControl ) ## + Fold01: alpha=0, lambda=1 ## - Fold01: alpha=0, lambda=1 ## + Fold01: alpha=1, lambda=1 ## - Fold01: alpha=1, lambda=1 ## + Fold02: alpha=0, lambda=1 ## - Fold02: alpha=0, lambda=1 ## + Fold02: alpha=1, lambda=1 ## - Fold02: alpha=1, lambda=1 ## + Fold03: alpha=0, lambda=1 ## - Fold03: alpha=0, lambda=1 ## + Fold03: alpha=1, lambda=1 ## - Fold03: alpha=1, lambda=1 ## + Fold04: alpha=0, lambda=1 ## - Fold04: alpha=0, lambda=1 ## + Fold04: alpha=1, lambda=1 ## - Fold04: alpha=1, lambda=1 ## + Fold05: alpha=0, lambda=1 ## - Fold05: alpha=0, lambda=1 ## + Fold05: alpha=1, lambda=1 ## - Fold05: alpha=1, lambda=1 ## + Fold06: alpha=0, lambda=1 ## - Fold06: alpha=0, lambda=1 ## + Fold06: alpha=1, lambda=1 ## - Fold06: alpha=1, lambda=1 ## + Fold07: alpha=0, lambda=1 ## - Fold07: alpha=0, lambda=1 ## + Fold07: alpha=1, lambda=1 ## - Fold07: alpha=1, lambda=1 ## + Fold08: alpha=0, lambda=1 ## - Fold08: alpha=0, lambda=1 ## + Fold08: alpha=1, lambda=1 ## - Fold08: alpha=1, lambda=1 ## + Fold09: alpha=0, lambda=1 ## - Fold09: alpha=0, lambda=1 ## + Fold09: alpha=1, lambda=1 ## - Fold09: alpha=1, lambda=1 ## + Fold10: alpha=0, lambda=1 ## - Fold10: alpha=0, lambda=1 ## + Fold10: alpha=1, lambda=1 ## - Fold10: alpha=1, lambda=1 ## Aggregating results ## Selecting tuning parameters ## Fitting alpha = 1, lambda = 1e-04 on full training set # Print model to console model ## glmnet ## ## 150 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... ## Resampling results across tuning parameters: ## ## alpha lambda Accuracy Kappa ## 0 0.00010000 0.9466667 0.92 ## 0 0.05272632 0.9400000 0.91 ## 0 0.10535263 0.9000000 0.85 ## 0 0.15797895 0.8933333 0.84 ## 0 0.21060526 0.8800000 0.82 ## 0 0.26323158 0.8866667 0.83 ## 0 0.31585789 0.8866667 0.83 ## 0 0.36848421 0.8866667 0.83 ## 0 0.42111053 0.8800000 0.82 ## 0 0.47373684 0.8866667 0.83 ## 0 0.52636316 0.8866667 0.83 ## 0 0.57898947 0.8866667 0.83 ## 0 0.63161579 0.8800000 0.82 ## 0 0.68424211 0.8733333 0.81 ## 0 0.73686842 0.8666667 0.80 ## 0 0.78949474 0.8666667 0.80 ## 0 0.84212105 0.8666667 0.80 ## 0 0.89474737 0.8666667 0.80 ## 0 0.94737368 0.8666667 0.80 ## 0 1.00000000 0.8666667 0.80 ## 1 0.00010000 0.9733333 0.96 ## 1 0.05272632 0.9400000 0.91 ## 1 0.10535263 0.9466667 0.92 ## 1 0.15797895 0.9333333 0.90 ## 1 0.21060526 0.8733333 0.81 ## 1 0.26323158 0.8333333 0.75 ## 1 0.31585789 0.7000000 0.55 ## 1 0.36848421 0.6666667 0.50 ## 1 0.42111053 0.6666667 0.50 ## 1 0.47373684 0.3333333 0.00 ## 1 0.52636316 0.3333333 0.00 ## 1 0.57898947 0.3333333 0.00 ## 1 0.63161579 0.3333333 0.00 ## 1 0.68424211 0.3333333 0.00 ## 1 0.73686842 0.3333333 0.00 ## 1 0.78949474 0.3333333 0.00 ## 1 0.84212105 0.3333333 0.00 ## 1 0.89474737 0.3333333 0.00 ## 1 0.94737368 0.3333333 0.00 ## 1 1.00000000 0.3333333 0.00 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 1 and lambda = 1e-04. # Print maximum ROC statistic # max(model[[&quot;results&quot;]][[&quot;ROC&quot;]]) # this doesn&#39;t work with 3 target classes 18.2 Imputation discussion 18.3 Split target from predictors This example shows how to generate some missing values, and to use imputation overcome these. data(&quot;mtcars&quot;) set.seed(42) mtcars[sample(1:nrow(mtcars), 10), &quot;hp&quot;] &lt;- NA Y &lt;- mtcars$mpg X &lt;- mtcars[,2:4] # model &lt;- train(X,Y) #This fails, because of the NAs. Try imputation. median_model &lt;- train( X, Y, preProcess = &quot;medianImpute&quot; ) ## note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 . print(median_model) ## Random Forest ## ## 32 samples ## 3 predictor ## ## Pre-processing: median imputation (3) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 32, 32, 32, 32, 32, 32, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 2.758439 0.8106146 2.260273 ## 3 2.729423 0.8123802 2.224138 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 3. 18.4 Multiple preprocessing methods Zach Mayer offers the following cheat sheet for preprocessing: Start with median imputation (if you’re using it) Try KNN imputation if data NOT missing at random For linear models (lm, glm, glmnet) always center and scale Tree-based models (random forest, gbm) don’t need much preprocessing data(&quot;mtcars&quot;) set.seed(42) mtcars[sample(1:nrow(mtcars), 10), &quot;hp&quot;] &lt;- NA Y &lt;- mtcars$mpg X &lt;- mtcars[,2:4] # missing at random set.seed(42) model &lt;- train( X, Y, method = &quot;glm&quot;, preProcess = c(&quot;medianImpute&quot;, &quot;center&quot;, &quot;scale&quot;) ) print(min(model$results$RMSE)) ## [1] 3.079732 set.seed(42) model &lt;- train( X, Y, method = &quot;glm&quot;, preProcess = c(&quot;medianImpute&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;) ) print(min(model$results$RMSE)) # with pca applied ## [1] 3.03581 18.5 Max Kuhn on reusing a trainControl library(C50) library(modeldata) data(mlc_churn) set.seed(1) inTrainingSet &lt;- createDataPartition(mlc_churn$churn, p = 0.75, list = FALSE) churnTrain &lt;- mlc_churn[inTrainingSet,] churnTest &lt;- mlc_churn[-inTrainingSet,] glimpse(churnTrain) ## Rows: 3,751 ## Columns: 20 ## $ state &lt;fct&gt; KS, NJ, OH, MA, LA, IN, RI, IA, IA, NY, … ## $ account_length &lt;int&gt; 128, 137, 84, 121, 117, 65, 74, 168, 62,… ## $ area_code &lt;fct&gt; area_code_415, area_code_415, area_code_… ## $ international_plan &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no,… ## $ voice_mail_plan &lt;fct&gt; yes, no, no, yes, no, no, no, no, no, no… ## $ number_vmail_messages &lt;int&gt; 25, 0, 0, 24, 0, 0, 0, 0, 0, 0, 27, 0, 0… ## $ total_day_minutes &lt;dbl&gt; 265.1, 243.4, 299.4, 218.2, 184.5, 129.1… ## $ total_day_calls &lt;int&gt; 110, 114, 71, 88, 97, 137, 127, 96, 70, … ## $ total_day_charge &lt;dbl&gt; 45.07, 41.38, 50.90, 37.09, 31.37, 21.95… ## $ total_eve_minutes &lt;dbl&gt; 197.4, 121.2, 61.9, 348.5, 351.6, 228.5,… ## $ total_eve_calls &lt;int&gt; 99, 110, 88, 108, 80, 83, 148, 71, 76, 9… ## $ total_eve_charge &lt;dbl&gt; 16.78, 10.30, 5.26, 29.62, 29.89, 19.42,… ## $ total_night_minutes &lt;dbl&gt; 244.7, 162.6, 196.9, 212.6, 215.8, 208.8… ## $ total_night_calls &lt;int&gt; 91, 104, 89, 118, 90, 111, 94, 128, 99, … ## $ total_night_charge &lt;dbl&gt; 11.01, 7.32, 8.86, 9.57, 9.71, 9.40, 8.8… ## $ total_intl_minutes &lt;dbl&gt; 10.0, 12.2, 6.6, 7.5, 8.7, 12.7, 9.1, 11… ## $ total_intl_calls &lt;int&gt; 3, 5, 7, 7, 4, 6, 5, 2, 6, 9, 4, 3, 2, 4… ## $ total_intl_charge &lt;dbl&gt; 2.70, 3.29, 1.78, 2.03, 2.35, 3.43, 2.46… ## $ number_customer_service_calls &lt;int&gt; 1, 0, 2, 3, 1, 4, 0, 1, 4, 4, 1, 3, 1, 0… ## $ churn &lt;fct&gt; no, no, no, no, no, yes, no, no, no, yes… glimpse(churnTest) ## Rows: 1,249 ## Columns: 20 ## $ state &lt;fct&gt; OH, OK, AL, MO, WV, MT, VA, AZ, SC, WY, … ## $ account_length &lt;int&gt; 107, 75, 118, 147, 141, 95, 76, 130, 111… ## $ area_code &lt;fct&gt; area_code_415, area_code_415, area_code_… ## $ international_plan &lt;fct&gt; no, yes, yes, yes, yes, no, no, no, no, … ## $ voice_mail_plan &lt;fct&gt; yes, no, no, no, yes, no, yes, no, no, y… ## $ number_vmail_messages &lt;int&gt; 26, 0, 0, 0, 37, 0, 33, 0, 0, 39, 33, 0,… ## $ total_day_minutes &lt;dbl&gt; 161.6, 166.7, 223.4, 157.0, 258.6, 156.6… ## $ total_day_calls &lt;int&gt; 123, 113, 98, 79, 84, 88, 66, 112, 103, … ## $ total_day_charge &lt;dbl&gt; 27.47, 28.34, 37.98, 26.69, 43.96, 26.62… ## $ total_eve_minutes &lt;dbl&gt; 195.5, 148.3, 220.6, 103.1, 222.0, 247.6… ## $ total_eve_calls &lt;int&gt; 103, 122, 101, 94, 111, 75, 65, 99, 102,… ## $ total_eve_charge &lt;dbl&gt; 16.62, 12.61, 18.75, 8.76, 18.87, 21.05,… ## $ total_night_minutes &lt;dbl&gt; 254.4, 186.9, 203.9, 211.8, 326.4, 192.3… ## $ total_night_calls &lt;int&gt; 103, 121, 118, 96, 97, 115, 108, 78, 105… ## $ total_night_charge &lt;dbl&gt; 11.45, 8.41, 9.18, 9.53, 14.69, 8.65, 7.… ## $ total_intl_minutes &lt;dbl&gt; 13.7, 10.1, 6.3, 7.1, 11.2, 12.3, 10.0, … ## $ total_intl_calls &lt;int&gt; 3, 3, 6, 6, 5, 5, 5, 19, 6, 3, 6, 2, 5, … ## $ total_intl_charge &lt;dbl&gt; 3.70, 2.73, 1.70, 1.92, 3.02, 3.32, 2.70… ## $ number_customer_service_calls &lt;int&gt; 1, 3, 0, 0, 0, 3, 1, 0, 2, 0, 3, 3, 3, 3… ## $ churn &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, … table(churnTrain$churn/nrow(churnTrain)) ## Warning in Ops.factor(churnTrain$churn, nrow(churnTrain)): &#39;/&#39; not meaningful ## for factors ## &lt; table of extent 0 &gt; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
