[["dimensionalityreduction.html", "Chapter 12 Dimensionality Reduction 12.1 Background 12.2 Experiment in showing dimensionality reduction for pathologists", " Chapter 12 Dimensionality Reduction 12.1 Background Most complex diagnosis is conducted without recourse to perfect prevalence data, or to the characteristics of available tests. Indeed, we often don’t know which variables are relevant. An expert in pathology is defined, more or less, by an intuitive understanding of prevalence and the properties of tests which can be used to classify diseases. The process by which pathologists classify disease is often poorly understood. Magical thinking and arguments from authority infuse the process, particularly at the edges of what is known–especially when disease is rare and evidence is weak. Careers in diagnostic pathology were formerly made by adding a new rare disease to the ever-growing bestiary. Beginners often stumble when trying to visualize the testing process, and how classes are established. Dimension reduction methods are useful to prioritize variables (factors), and begin the iterative process of classification-test validation-clinical testing, it is often useful to see which ones permit the clearest distinction between clusters in high dimensional spaces–like those created when numerous tests are deployed to establish a single diagnosis. Resources to understand dimensionality reduction: https://www.datacamp.com/community/tutorials/introduction-t-sne http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/ library(tidyverse) library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout 12.2 Experiment in showing dimensionality reduction for pathologists The first step is to build three separate “clouds” of data that do not overlap in three dimensions, but which do overlap in two dimensions. df &lt;- tibble( diagnostic_class = rep(&quot;A&quot;, 100), x = rnorm(100, mean = 1, sd = 0.1), y = rnorm(100, mean = 1, sd = 0.1), z = rnorm(100, mean = 1, sd = 0.1) ) df1 &lt;- tibble( diagnostic_class = rep(&quot;B&quot;, 100), x = rnorm(100, mean = 1, sd = 0.1), y = rnorm(100, mean = 2, sd = 0.1), z = rnorm(100, mean = 1, sd = 0.1) ) df2 &lt;- tibble( diagnostic_class = rep(&quot;C&quot;, 100), x = rnorm(100, mean = 1, sd = 0.1), y = rnorm(100, mean = 1, sd = 0.1), z = rnorm(100, mean = 2, sd = 0.1) ) df &lt;- df %&gt;% rbind(df1) %&gt;% rbind(df2) The following 2D and 3D plots make it clear that the diagnostic class is only obvious when one takes into account the z-axis. df %&gt;% ggplot(aes(x,y, color = diagnostic_class)) + geom_point() plot_ly(x=df$x, y=df$y, z=df$z, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, color = df$diagnostic_class) 12.2.1 Principle component analysis PCA concentrates on placing dissimilar data points far apart in a lower dimension representation. Some useful websites for principle component analysis: http://huboqiang.cn/2016/03/03/RscatterPlotPCA library(ggfortify) autoplot(prcomp(df[,2:4]), data = df, colour = &#39;diagnostic_class&#39;, size = 8) ## Warning: `select_()` was deprecated in dplyr 0.7.0. ## Please use `select()` instead. 12.2.2 t-distributed stochastic neighbor embedding library(Rtsne) df_unique &lt;- unique(df) #removes duplicates df_matrix &lt;- as.matrix(df[,2:4]) set.seed(2020) tsne_out &lt;- Rtsne(df_matrix) tsne_plot &lt;- data.frame(x = tsne_out$Y[,1], y = tsne_out$Y[,2], col = df$diagnostic_class) ggplot(tsne_plot) + geom_point(aes(x=x, y=y, color=col)) "]]
