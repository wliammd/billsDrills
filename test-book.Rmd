---
title: "Bill's Drills Book"
author: "William McDonald"
date: "`r Sys.Date()`"
output: bookdown::gitbook
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: https://github.com/wliammd/billsDrills
---
---
title: "Bill's Drills Book"
author: "William McDonald"
date: "`r Sys.Date()`"
output: bookdown::gitbook
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: https://github.com/wliammd/billsDrills
---

# Drills: Part of Every Healthy Intellectual Diet {#intro}

The goal of this book is to organize my R drills into reasonable chunks, the better to understand my strengths and weaknesses, and to plan new forays into data science.

<!--chapter:end:index.Rmd-->


# **bookdown** Tips for This Document {#bookdownplan}

Placeholder


## Basic conventions 
## Inserting pictures
## Referencing other parts of the document
## Referencing citations:
## Figures.   

<!--chapter:end:02bookdown_tips.Rmd-->


# Shape of Data {#datashape}

Placeholder


## Making data frames
## Gather, spread, pivoting in the tidyverse
## Gathering steam...
## Spread your wings
## Missing Data: Ich vemisse Dich!
### You complete(me)
### fill(ing) in the gaps
## Pivoting to something new
## Larger structures

<!--chapter:end:03shapes.Rmd-->


# By Any Other Name {#changenames}

Placeholder


## A financial example

<!--chapter:end:04names.Rmd-->


# Factor Practice {#factorpractice}

Placeholder


## The basic structure
## Not all nominal data is a factor
## Making variables into factors
## Inspecting factors
## fct_lump()
## fct_infreq() and fct_rev()
## Additional practice
## Reordering Factors
### Improving legibility: change the linetype
### fct_reorder2(): another way to improve legibility

<!--chapter:end:05factors.Rmd-->


# Subsetting {#subset}

Placeholder


## Subsetting using brackets
## Subset using brackets by omitting the rows and columns we donâ€™t want
## Subset using brackets in combination with the which() function and the %in% operator
## Subset using the subset() function
## Subset using dyplyr's filter() and select()

<!--chapter:end:06subsetting.Rmd-->


# Data Exploration {#dataexploration}

Placeholder


## Counting things. The naming of parts. 
## fct_infreq
## Weight weight, don't tell me!
## Summarize is another very useful function:
## Graphical displays
### Joyplots
## Relative versus absolute risk

<!--chapter:end:07data_explore.Rmd-->


# Sampling {#sampling}

Placeholder


## Think about throwing a bunch of dice.
## A keen way to divide up a dataset into testing and training components. 

<!--chapter:end:08sampling.Rmd-->


# Simulating data

Placeholder


## Sample() 
## replicate()
## sample() revisited
## generating fixed levels -------------------------------------------------
## generating numerical sequences
## seq_along() and seq_len(). 
## generating random data from a probability distribution 
## Normal distribution:
## Binomial distribution:
## Uniform distribution
## Sampling from multiple distributions (building in a "difference")
## The good stuff: building in a difference based on a categorical variable
## A demonstration of the Central Limit Theorem
## Overlaying normal curve on histogram
## Crossing trial

<!--chapter:end:09simulatingData.Rmd-->


# Functions

Placeholder


## A little Dungeons & Dragons (tm) to spice it up
## Geometric and harmonic means
## for loop
## case_when()
## Compare this with if_else()

<!--chapter:end:10functions.Rmd-->

# Correlation Plots {#correlation}

```{r, message=FALSE}
library(tidyverse)
```


```{r}
head(iris)
iris %>% select(-Species) %>% cor()

M <- iris %>% select(-Species) %>% cor(method = "kendall")
```

```{r}
corrplot::corrplot(M)
corrplot::corrplot(M, method = "color")
corrplot::corrplot(M, method = "color", type = "upper")
corrplot::corrplot(M, method = "color", type = "upper", order = "hclust")
corrplot::corrplot(M, method = "color", type = "upper", order = "hclust", addCoef.col = "black")
corrplot::corrplot(M, method = "color", type = "upper", order = "hclust", addCoef.col = "black", tl.col="black")
corrplot::corrplot(M, method = "color", type = "upper", order = "hclust", addCoef.col = "black", tl.col="black", tl.srt = 45)
```





<!--chapter:end:11correlation.Rmd-->


# Dimensionality Reduction {#dimensionalityreduction}

Placeholder


## Background
## Experiment in showing dimensionality reduction for pathologists
### Principle component analysis
### t-distributed stochastic neighbor embedding

<!--chapter:end:12dimension_reduction.Rmd-->


# Clustering {#clustering}

Placeholder



<!--chapter:end:13clustering.Rmd-->


# Regular expressions  {#regex}

Placeholder


## Seeking patterns

<!--chapter:end:14seeking_patterns.Rmd-->


# US_Census_Data_Kyle_Walker_Presentation_YouTube {#census}

Placeholder


## Variables
## Part 2: Wrangling Census Data with **tidyverse** Tools
### Normalizing the data with `mutate()`.
### `group_by()` and `summarize()` in census analysis
### Margin of error considerations
### Visualizing margins of error
### `get_estimates()` and how to use them
### `ggbeeswarm()` automates some jitter considerations

<!--chapter:end:15US_census_work.Rmd-->


# **K Nearest Neighbors Revisited: Cross Validation Added** Tips for This Document {#knncv}

Placeholder



<!--chapter:end:16knncv.Rmd-->

# Modeling {#modeling}

```{r include=FALSE}
library(tidyverse)
library(rpart)
```

## Modeling notes

Notes on/inspired by the Machine Learning with caret in R DataCamp course. I'd like to better understand machine learning, especially as it pertains to classification problems. Most discussions begin with regression examples, so these will also be addressed in order to build a better foundation. This is the general approach of James, Witten, Hastie and Tibshirani in An Introduction to Statistical Learning with Applications in R, too. 


A regression example from the course:

```{r}
library(caret)

model <- train(
  price~., 
  diamonds,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 10,
    verboseIter = TRUE
  )
)

# Print model to console
model
```

Note that **caret** handles the work of splitting test sets and calculating RMSE. 

Another example from the DataCamp course.

```{r}
library(MASS)

# Fit lm model using 5-fold CV: model
model <- train(
  medv~., 
  Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 5,
    verboseIter = TRUE
  )
)

# Print model to console
model
```

Cross validation can *itself* be repeated. The following is a 5-fold cross validation repeated 5 times.

```{r}
# Fit lm model using 5 x 5-fold CV: model
model <- train(
  medv ~ ., 
  Boston,
  method = "lm",
  trControl = trainControl(
    method = "repeatedcv", 
    number = 5,
    repeats = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

predict(model, Boston)
```

## Classification

### An **rpart** classification example.

```{r}
model <- rpart(
  Species ~ .,
  data = iris,
  method = "class"
)

predicted <- predict(model, iris, type = "class")
predicted
mean(predicted == iris$Species)
```

Study what's going on in **predict()**:

```{r}
?predict.rpart
```

## From the DataCamp course

```{r}
library(mlbench)
data("Sonar")

# First randomize the dataset.
rows <- sample(nrow(Sonar))
Sonar <- Sonar[rows,]

# The split it into testing and training sets. 
split <- round(nrow(Sonar)*0.60)
train <- Sonar[1:split,]
test <- Sonar[(split+1):nrow(Sonar),]

nrow(train)/nrow(Sonar)
```

Let's build a model.

```{r}
model <- glm(
  Class ~ .,
  family = binomial(link = "logit"),
  train
)

p <- predict(model, test, type = "response")
summary(p)

p_class <- as.factor(ifelse(p > 0.1, "M", "R"))

table(p_class, test[["Class"]])

confusionMatrix(p_class, test[["Class"]])
```

Receiver operator curves add a new level of complexity and usefulness. According to Zach Mayer in DataCamp, 

 `trainControl()` in **caret** can use AUC (instead of accuracy), to tune the parameters of your models. The `twoClassSummary()` convenience function allows you to do this easily.

When using `twoClassSummary()`, be sure to always include the argument `classProbs = TRUE` or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.)

```{r}
# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)

# Train glm with custom trainControl: model
model<-train(Class~., data=Sonar, method="glm", trControl=myControl)

# Print model to console
model
```

## Random forest with **caret**

For this set, we'll use the white wine quality dataset from UC Irvine. 

```{r}
wine <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep = ";")

glimpse(wine)
# Note that quality is an integer, not a factor. **caret** seems to take this in stride. 

# Fit random forest: model
model <- train(
  quality~.,
  tuneLength = 1,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

plot(model)
```

Let's try this again with a longer `tuneLength`. This explores more models and potentially finds a better model.

```{r}
model <- train(
  quality~.,
  tuneLength = 10,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

plot(model)
```

Custom tuning of grids can be used in **caret** by using `tuneGrid()`. While it is the most flexible method for fitting **caret** models and allows complete control over how the model is fit, it requires significant knowledge of the model and dramatically increases run time. 

An example provided by Zach Meyer:

```{r}
# library(mlbench)
# library(caret)
# data("Sonar")
# myGrid <- data.frame(mtry = c(2,3,4,5,10,20))
# 
# set.seed(42)
# model <- train(
#   Class~.,
#   data = Sonar,
#   method = "ranger", 
#   tuneGrid = myGrid
# )
```

This generates an error currently. Try the following frome DataCamp exercise that followed Zach's video.

```{r}
tuneGrid <- data.frame(
  .mtry = c(2, 3, 7),
  .splitrule = "variance",
  .min.node.size = 5
)

# Fit random forest: model
model <- train(
  quality ~ .,
  tuneGrid = tuneGrid,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

# Plot model
plot(model)
```

## **glmnet** and `patma` 

I'd like to explore Zach Meyer's teaching on **glmnet** models using my own data. That means dipping into `patmanDx.csv` and pulling out the relative data. 

```{r}
patma <- read_csv("data/patmanDx.csv")
df <- patma %>% 
  select(SF1Median, Pit1Median, TPITMedian, PRLMedian, GHMedian, TSHMedian, LHMedian, FSHMedian, ACTHMedian, ASUMedian, GATA3Median, manDx) %>%
  na.omit()

glimpse(df)

# Two problems: "NULL" and classes with only 1 value. These throw errors unless I make some modifications.

df <- df %>% mutate(manDx = case_when(
  manDx == "NULL" ~ "Null",
  manDx %in% c("PLUR", "UNK") ~ "PlurUnk",
  TRUE ~ manDx
)) 

glimpse(df)

table(df$manDx)
```

That seems to give me what I'm looking for. Let's get to work:

```{r}
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = defaultSummary,
  classProbs = TRUE, #Critical to have this, per Meyers.
  verboseIter = TRUE
)

set.seed(42)
model <- train(
  manDx ~ .,
  df,
  method = "glmnet",
  trControl = myControl
)

model

plot(model)
```


<!--chapter:end:17modeling.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:99references.Rmd-->

