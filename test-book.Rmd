---
title: "Bill's Drills Book"
author: "William McDonald"
date: "`r Sys.Date()`"
output: bookdown::gitbook
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: https://github.com/wliammd/billsDrills
---

# Drills: Part of Every Healthy Intellectual Diet {#intro}

The goal of this book is to organize my R drills into reasonable chunks, the better to understand my strengths and weaknesses, and to plan new forays into data science.

<!--chapter:end:index.Rmd-->

# bookdown Tips for This Document {#bookdownplan}

## Basic conventions 
  * the `_bookdown.yml` file contains a snippet that is important to inserting the word "Chapter" before the chapter number in each of the Rmd files. 
  * packages are indicated in bold, like **dplyr**
  * inline code and filenames are indicated in typerwriter face using backticks, like `_bookdown.yml`
  * `_output.yml` is modified from that used by Xie in his `bookdown-demo` [@R-bookdown]; it evokes `style.css`, `toc.css`, `preamble.tex`, which are also borrowed from Xie. 
  * chapters are set in order by using adding 01, 02, 03, ... before the name of their Rmd, like `01chpter.Rmd`. Note that they can have short descriptive phrases, since the actual chapter titles are determined by the hashtag. `index.Rmd` always comes first in the book build, and contains the `yml` front matter. 

## Referencing other parts of the document

This is a good place to practice referencing figures. Say that I want to refer the reader back to my first starwars figure. See Figure \@ref(fig:starfig-1).

I can reference other pages in a similar fashion. See Chapter \@ref(subset). Note that this works by referencing a {#label} placed in the chapter title. 

See Chapter \@ref(intro)

See Chapter \@ref(dataexploration)

Note that the {#label} uses a single run-together word. It does not tolerate spaces and this cannot be overcome by 'quoting' it. 

## Inserting pictures

Pictures can be included in the `test-book_files` subdirectories and referenced like this: 

![Tpit immunohistochemical stain. Figure A silent corticotroph. Figure B gonadotroph](_bookdown_files/pathologyImages/TpitIHC.jpg)

Some of the subdirectories throw an error in building the book, so I settled on `_bookdown_files/pathologyImages` as the location.

Also, I note that the build does not generate the caption unless the reference is on it's own line. 

Also note that some controls on image size are available. For instance, the same image can be displayed at 50% size:

![Tpit immunohistochemical stain. Figure A silent corticotroph. Figure B gonadotroph](_bookdown_files/pathologyImages/TpitIHC.jpg){#id .class width=50% height=50%}

## Referencing citations:

In order to insert citations, one needs a .bib file in the project. I've included one in this project as `book.bib`. The yml header in Chapter \@ref(intro) needs to have a $bibliography:$ and $biblio-style:$ line added. 

To insert a citation, use the **citr** Addin from RStudio. **bookdown**, for instance, is cited thusly [@R-bookdown]. Note that I need to figure out an adequate workflow of references. The convenience of Endnote in MS Word will not be available. Nonetheless, if I populate the book.bib and packages.bib files carefully, with .txt files generated in Endnote, I should be OK.

For instance, a recent dump of my Endnote library is in bookFromEndnote.txt. This can be opened in RStudio, and I can copy-and-paste references from the .txt file to my book.bib. For instance, if I have a breast paper that I want to cite here [@RN2750], I'd copy-and-paste the reference from `bookFromEndnote.txt` to `book.bib`.

Of note, Yihui Xie includes a nifty bit of code to automatically generate a bib database for R packages:

```{r}
knitr::write_bib(c(.packages(), 'bookdown', 'knitr', 'rmarkdown', 'tidyverse', 'ComplexHeatmap'), 'packages.bib')
```

References appear automatically at the end of a chapter. 

<!--chapter:end:01chpter.Rmd-->

# Data Exploration {#dataexploration}

Data exploration is one of the most important aspects of data science and forms the cornerstone of my drills. Nonetheless, I have lots of room for improvement.

I like Hadely Wickham's writing and find his approach exceptionally clear. Therefore, I'll use the **tidyverse**. 

```{r, message=FALSE}
library(tidyverse)
```

## Counting things. The naming of parts. 

```{r starfig-1, fig.cap='Starwars Figure 1'}
starwars %>% 
  filter(!is.na(species)) %>% 
  count(species = fct_lump(species, 5), sort = TRUE) %>% 
  mutate(species = fct_reorder(species, n)) %>% 
  ggplot(aes(species, n)) + 
  geom_col() + coord_flip()
```

I like stacked bars for their economy, but it's easy to over do it. Supperimposing gender onto the columns seems easy...

```{r starfig-2, fig.cap='Starwars Figure 2'}
starwars %>% 
  filter(!is.na(species)) %>% 
  count(species = fct_lump(species, 5), gender = fct_lump(gender, 2), sort = TRUE) %>% 
  mutate(species = fct_reorder(species, n)) %>% 
  ggplot(aes(species, n, fill = gender)) + 
  geom_col() + coord_flip()
```

But note that I've got a problem: the Droids, which outnumber the Gungans, are now reordered to _after_ the Gungans. This happens because the $n$ that we're counting comprises subcategories of species _and_ gender.  Only three Gungan males exist (and no females), but that is enough to tie the Droid NA category. The Droid NA category come after the Gungan category, presumably because _male_ comes before _NA_, or because NA comes last (more likely). 

Exploring this, I see that I'm getting warning messages about the implicit NA's in gender. Note that the following renders a slightly different plot. I _still_ have not fixed the order of the species. 


```{r starfig-3, fig.cap='Starwars Figure 3'}
starwars %>% 
  filter(!is.na(species)) %>% 
  count(species = fct_lump(species, 5), gender = fct_lump(gender, 2), sort = TRUE) %>% 
  mutate(gender = fct_explicit_na(gender),
         species = fct_reorder(species, n)) %>% 
  ggplot(aes(species, n, fill = gender)) + 
  geom_col() + coord_flip()
```

The trick here is to use `group_by()` and `ungroup()` wisely. 

```{r starfig-4, fig.cap='Starwars Figure 4'}
starwars %>% filter(!is.na(species)) %>% 
  mutate(species = fct_lump(species, 5)) %>% 
  group_by(species) %>% 
  mutate(typeCount = n()) %>% 
  ungroup() %>% 
  mutate(species = fct_reorder(species, typeCount)) %>% 
  ggplot()+
  geom_bar(aes(species, fill = gender))+
  coord_flip()
```

As opposed to using `count()`, which progressively narrows the information available to be used, by using `group_by()`/`mutate()`/`ungroup()` with `geom_bar()` we have all of the variables still available for plotting. 

## fct_infreq

As expected, Hadley Wickham and Garrett Grolemund solve this more simple in **R4DS**:

```{r starfig-5, fig.cap='Starwars Figure 5'}
starwars %>% filter(!is.na(species)) %>% 
  mutate(species = fct_lump(species, 5)) %>% 
  mutate(species = species %>% fct_infreq() %>% fct_rev()) %>% 
  ggplot()+
  geom_bar(aes(species, fill = gender))+
  coord_flip()

# Note that a single `mutate()` suffices to both lump factors, organize by frequency and reverse the order, as follows.
starwars %>% filter(!is.na(species)) %>% 
  mutate(species = fct_lump(species, 5) %>% fct_infreq() %>% fct_rev()) %>% 
  ggplot()+
  geom_bar(aes(species, fill = gender))+
  coord_flip()

```

## Summarize is another very useful function:

```{r}
starwars %>% 
  filter(!(is.na(species))) %>% 
  group_by(species) %>% 
  summarize(n=n(), mean = mean(height, na.rm = TRUE)) %>% 
  arrange(desc(n))
```

<!--chapter:end:02chpter.Rmd-->

# Sampling {#sampling}

## Think about throwing a bunch of dice.

```{r}
sample(1:6, size=100, replace=TRUE) 
sample(1:6, size=100, replace=TRUE) %>% table()
sample(1:6, size=100, replace=TRUE) %>% table() %>% prop.table()
```

## A keen way to divide up a dataset into testing and training components. 
```{r}
x <- 1:10
y <- 11:30

df <- data.frame(x,y)
df

set.seed(0)
train_indexes = sample(1:nrow(df), .7 * nrow(df))

train_set <- df[train_indexes,]
test_set <- df[-train_indexes,]

train_set

test_set
```



<!--chapter:end:03chpter.Rmd-->

# Factor Practice {#factorpractice}

```{r}
cups <- c("small", "medium", "large")
manyCups <- sample(cups, size = 100, replace = TRUE)
sizesCups <- factor(manyCups, levels = c("small", "medium", "large"))
sizesCups
```

<!--chapter:end:04chpter.Rmd-->

# Crossing Trial {#crossingtrial}

From David Robinson birthday paradox Rblogger at <https://www.r-bloggers.com/the-birthday-paradox-puzzle-tidy-simulation-in-r/>

```{r}
summarized <- crossing(people = seq(2, 50, 2),
                       trial = 1:100) %>%
  mutate(birthday = map(people, ~ sample(365, .x, replace = TRUE)),
         multiple = map_lgl(birthday, ~ any(duplicated(.x)))) %>%
  group_by(people) %>%
  summarize(chance = mean(multiple))

ggplot(summarized, aes(people, chance)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(y = "Probability two have the same birthday")

# Checking the work with pbirthday function
summarized %>% 
  mutate(exact = map_dbl(people, pbirthday)) %>% 
  ggplot(aes(people, chance)) +
  geom_line() +
  geom_line(aes(y = exact), lty = 2, color = "blue") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(y = "Probability two have the same birthday")
```


<!--chapter:end:05chpter.Rmd-->

# By Any Other Name {#changenames}

This deceptively simple-seeming idea gets complex quickly. The following YouTube was a nice description of the process: <https://www.youtube.com/watch?v=Okc0IL5uTnA>

```{r}
my.data <- data.frame(colOne=1:3, column2=4:6, column_3=7:9)
rownames(my.data) <- c("ant", "bee", "cat")
names(my.data)
colnames(my.data)

#make some changes
names(my.data) <- c("col_1", "col_2", "col_3")
my.data
names(my.data)[3] <- "col.3"
my.data

names(my.data)[names(my.data)=="col_2"]
my.data["col_2"]
my.data$col_2
my.data[,2]

names(my.data)[names(my.data)=="col_2"] <- "col.2"
my.data

names(my.data) <- gsub("_", ".", names(my.data))
my.data

rownames(my.data)
my.data$species <- rownames(my.data)
my.data
rownames(my.data) <- NULL
my.data
colnames(my.data) <- c("good", "better", "best", "species")
my.data

keep <- 2:ncol(my.data)
my.data[,keep]
```


<!--chapter:end:06chpter.Rmd-->

# Correlation Plots {#correlation}

```{r}
head(iris)
iris %>% select(-Species) %>% cor()

M <- iris %>% select(-Species) %>% cor(method = "kendall")
```

```{r}
corrplot::corrplot(M)
corrplot::corrplot(M, method = "color")
corrplot::corrplot(M, method = "color", type = "upper")
corrplot::corrplot(M, method = "color", type = "upper", order = "hclust")
corrplot::corrplot(M, method = "color", type = "upper", order = "hclust", addCoef.col = "black")
corrplot::corrplot(M, method = "color", type = "upper", order = "hclust", addCoef.col = "black", tl.col="black")
corrplot::corrplot(M, method = "color", type = "upper", order = "hclust", addCoef.col = "black", tl.col="black", tl.srt = 45)
```





<!--chapter:end:07chpter.Rmd-->

# if_else() and case_when(): Comparison {#ifelsecasewhen}

## case_when()

case_when() from <https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/case_when>

```{r}
x <- 1:50
y <- 51:100

df <- data.frame(x,y)
df

case_when(
  x %% 35 == 0 ~ "fizz buzz",
  x %% 5 == 0 ~ "fizz",
  x %% 7 == 0 ~ "buzz",
  TRUE ~ as.character(x)
)
```


## Compare this with if_else()
```{r}
if_else(x %% 2 == 0, "even", "odd")
```




<!--chapter:end:08chpter.Rmd-->

# Subsetting {#subset}

From <https://www.r-bloggers.com/5-ways-to-subset-a-data-frame-in-r/>

Note: since this is down for maintenance, I will turn off evaluation on these chunks: 

```{r}
education <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/robustbase/education.csv", stringsAsFactors = FALSE)

colnames(education) <- c("X","State","Region","Urban.Population","Per.Capita.Income","Minor.Population","Education.Expenditures")

glimpse(education)

```

## Subsetting using brackets

```{r}

education[c(10:21),c(2,6:7)]


```

## Subset using brackets by omitting the rows and columns we don’t want

```{r}
education[-c(1:9,22:50),-c(1,3:5)]
```

## Subset using brackets in combination with the which() function and the %in% operator

```{r}
education[which(education$Region == 2),names(education) %in% c("State","Minor.Population","Education.Expenditures")]
```

## Subset using the subset() function

```{r}
subset(education, Region == 2, select = c("State","Minor.Population","Education.Expenditures"))
```

## Subset using dyplyr's filter() and select()

```{r}
select(filter(education, Region == 2),c(State,Minor.Population:Education.Expenditures))
```




<!--chapter:end:09chpter.Rmd-->

# Simulating data

I've just begun to explore R, and I realize that many of my questions could be improved with example data. Generating this kind of data takes practice, though.

Some good websites:
<https://clayford.github.io/dwir/dwr_12_generating_data.html>

<https://cran.r-project.org/web/packages/simstudy/vignettes/simstudy.html>

<https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/>

Also, remember that R packages have a lot of great data.

```{r}
data()
```

## Sample() 

Starting with Clayford's nice (and long page):

```{r}
sample(5) #sample without replacement

# or generate a random permutation of a vector:
dat <- c(10,12,18,16,18,9)
sample(dat)

# bootstrap resampling: sampling the same number of items WITH replacement. The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. 
sample(dat, replace = TRUE) 

rm(dat)

sample(state.abb, size = 10)

# Using 1:6 and size=1, we can simulate the roll of a die:
sample(1:6, size=1)

# We can simulate the roll of a die 100 times by setting size=100 and
# replace=TRUE
sample(1:6, size=100, replace=TRUE)

# sample produces a vector, so we can manipulate it as we would any other 
# vector. For example, simulate a 100 die rolls and tally up the totals using
# table() and prop.table():
table(sample(1:6, size=100, replace=TRUE))
prop.table(table(sample(1:6, size=100, replace=TRUE)))

table(sample(state.abb, size = 1000, replace = TRUE))
prop.table(table(sample(state.abb, size = 1000, replace = TRUE)))

# using the forward-pipe operator: %>% 
library(magrittr)
sample(1:6, size=100, replace=TRUE) %>% table() %>% prop.table()

# Or simulate rolling two dice and summing the total:
sum(sample(1:6, size=2, replace=TRUE))

# same thing with %>% 
sample(6, size=2, replace=TRUE) %>% sum()

# simulate rolling two dice 100 times by updating the sample "space"
sample(2:12, size=100, replace=TRUE)

# proportion of "snake-eyes" in 1000 rolls
mean(sample(2:12, size = 1000, replace = TRUE) == 2)
```

## replicate()

We can use the replicate() function to replicate samples. The replicate()
function allows you to replicate an expression as many times as you specify.
The basix syntax is replicate(n, expr) where n is the number of replications
and expr is the expression you want to replicate.

```{r}
# Roll 2 dice and keep the largest number, 10,000 times:
rolls <- replicate(n=1e5, expr = max(sample(1:6, size=2, replace=TRUE)))
# calculate proportions:
prop.table(table(rolls))

barplot(table(rolls))

rm(rolls)
```

## sample() revisited

The sample function also has a prob argument that allows you to assign
probabilities to your items. For example to simulate the flip of a loaded
coin, with Tails having probability 0.65:

```{r}
flips <- sample(c("H","T"), 1000, replace=TRUE, prob = c(0.35,0.65))
prop.table(table(flips))

rm(flips)

```

Coins are nice, but we can also use sample to generate practical data, for
example males and females. A web site says UVa has 11,632 female
students and 10,353 male students as of Fall 2015.

```{r}
uva <- c(11632, 10353) # female, male
round(uva/sum(uva),2)
```

Note how elegantly this answers a basic question. Nice!

We can generate a fake random sample of 500 UVa students with a weighted sampling scheme like so:

```{r}
students <- sample(c("female","male"), 500, replace=TRUE, prob = c(0.53, 0.47))
prop.table(table(students))

rm(students, uva)
```

When used with subsetting brackets, sample() can be used to create training
and test sets. For example, say we want to build some sort of predictive model using our training data. We may want to use half our data to build the model and then use the other half to evaluate its performance.

```{r}
train <- sample(nrow(iris), size= nrow(iris)/2)

# train is a random sample of numbers from 1 - 365. We can treat these like row numbers.

irisTrain <- iris[train,]
irisTest <- iris[-train,]
# confirm no intersection
dplyr::intersect(irisTrain, irisTest) 

rm(train, irisTest, irisTrain)
```

## generating fixed levels -------------------------------------------------

Often generating data means creating a series of fixed levels, such as 10
males and 10 females. The rep() function can be useful for this. Below we
replicate 10 each of "M" and "F":

```{r}
rep(c("M","F"), each=10)
rep(c("M","F"), times=10)
rep(c("M","F"), length.out = 15)
# or just length, for short
rep(c("M","F"), length = 15)
# Notice that all these generated a character vector. To use as a "factor", we would need to wrap it in the factor() function.
factor(rep(c("M","F"), each=10))

# A function specifically for creating factors is the gl() function. gl = 
# "generate levels". Below we generate a factor with 2 levels of 10 each and 
# labels of "M" and "F". Notice the result is a factor.
gl(n = 2, k = 10, labels = c("M","F"))

# A more common occurence is combinations of fixed levels, say gender, 
# education, and status. A function that helps create every combination of 
# levels is expand.grid(). Below we generate every combination of the levels 
# provided for gender, education, and status. Notice the first factors vary
# fastest.
expand.grid(gender=c("M","F"), 
            education=c("HS","College","Advanced"), 
            status=c("Single","Married","Divorced","Widowed"))

# Notice that creates a data frame that we can save:
DF <- expand.grid(gender=c("M","F"), 
            education=c("HS","College","Advanced"), 
            status=c("Single","Married","Divorced","Widowed"))
class(DF)

rm(DF)
```

Or imagine an experiment where 3 people throw 3 different kinds of paper airplanes, made of 3 paper types (3x3 = 9 planes), throwing each plane 8 times.

```{r}
schedule <- expand.grid(thrower=c("Clay","Rod","Kevin"),
            paper=c("18", "20", "24"),
            design=c("a","b","c"),
            rep=1:8)

# Randomize and drop the rep column. The sample(nrow(schedule)) code scrambles the numbers 1 through 216, which I then use to randomly shuffle the schedule of throws.
k <- sample(nrow(schedule))
schedule <- schedule[k,1:3]
head(schedule, n = 10)

# output to csv file for logging "distance flown" data
write.csv(schedule, file="throwLog.csv", row.names=FALSE)

rm(k, schedule)
```

This is a great way to set up an experiment, but I'd like to **also** add data for the throw, based on interesting distributions (normal, etc.). How would I generate samples for each contestant that was based on slightly different distributions?

What sort of distribution? See this page to get a quick refresher on common distributions: <https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/>.

Note also that ?distributions gives you the distributions in {stats}. Persevere for a time.

## generating numerical sequences

```{r}
# The seq() function allows you to generate sequences of numbers:
seq(from = 0, to = 10, by = 2)

seq(0, 10, 0.2)

# Go backwards
seq(1000, 0, -100)

# The seq() function has a length.out argument that allows you to specify the
# size of the vector you want to create. It automatically calculates the
# increment. We usually just abbreviate to length
seq(1, 10, length = 30)

# The colon operator(:) also allows you to generate regular sequences in steps
# of 1.
1:10

10:-10 # reverse direction

# When used with factors, the colon operator generates an interaction factor:
f1 <- gl(n = 2, k = 3); f1

f2 <- gl(n = 3, k = 2, labels = c("a","b","c")); f2

f1:f2

rm(f1,f2)
```

The last step seems akin to perfectly shuffling two decks of cards (the decks must be of equal length). 

## seq_along() and seq_len(). 

seq_along() returns the indices of a vector while seq_len(n) returns an integer vector of 1:n.

```{r}
seq_along(100:120)
seq_along(state.abb)

seq_len(12)
```

## generating random data from a probability distribution 

A central idea in inferential statistics is that the distribution of data can often be approximated by a theoretical distribution. R provides functions for working with several well-known theoretical distributions, including the 
ability to generate data from those distributions. A common one is the rnorm() function which generates data from a Normal distribution.

In R, the functions for theoretical distributions take the form of dxxx, pxxx, qxxx and rxxx. 

- dxxx is for the probability density/mass function (dnorm)
- pxxx is for the cumulative distribution function (pnorm)
- qxxx is for the quantile function (qnorm)
- rxxx is for random variate generation (rnorm)

For random variate generation we're interested in the rxxx variety. 

## Normal distribution:

```{r}
# 10 random draws from N(100,5)
rnorm(n = 10, mean = 100, sd = 5)
```

## Binomial distribution:

```{r}
# 10 random draws from b(1,0.5)
# AKA, 10 coin flips (size is the number of trials)
rbinom(n = 10, size = 1, prob = 0.5)

# 10 random draws from b(1,0.8)
# AKA, 10 coin flips with a coin loaded Heads (or Tails) 80% of time
rbinom(n = 10, size = 1, prob = 0.8)

# 10 random draws from b(10,0.5)
# AKA, 10 results of 10 coin flips
rbinom(n = 10, size = 10, prob = 0.5)

# We can use a binomial distribution to simulate dichotmous answers such as 
# Yes/No or success/fail. Simulate a vector of responses where respondents are 65% likely to say Yes (1) versus No (0)
rbinom(n = 10, size = 1, prob = 0.65)

# could also just use sample
sample(c("Y","N"), size = 10, replace = TRUE, prob = c(.65, .35))
```

## Uniform distribution

```{r}
# 10 random draws from a uniform distribution u(0,100)
runif(10,0,100)

# A uniform distribution can be good for random sampling. Let's say we want to sample about 10% of iris data:
k <- runif(nrow(iris),0,1) # [0,1] interval is default
sbSamp <- iris[k < 0.1, ] # sample about 10% of rows
dim(sbSamp)

# dplyr does this as well without the need for runif; and it's precise in its
# sampling fraction.
sbSamp <- dplyr::sample_frac(iris, 0.1) # sample exactly 10% of rows
dim(sbSamp)

rm(sbSamp, k)
```

## Sampling from multiple distributions (building in a "difference")

The arguments to rxxx functions can take vectors! This means we can use one
function call to generate draws from multiple distributions.

```{r}
# alternating random values from N(10,4) and N(100,40)
rnorm(10, mean = c(2,100),sd = c(2,40))

# 30 random draws, 10 each from N(10,4), N(90,4) and N(400,4)
rnorm(30, mean = rep(c(10,90,400),each=10), sd = 4)

# 100 random draws, 50 each from b(5,0.5) and b(50,0.5)
rbinom(n = 100, size = rep(c(5,50),each=50), prob = 0.5)

# Combined with matrix(), one can generate "multiple" random samples from a 
# distribution. For example, draw 5 random samples of size 10 from a N(10,1):
matrix(rnorm(10*5,10,1),ncol=5)
```

Note that in the last example, we technically drew one sample of size 50 and then laid it out in a 10x5 matrix.

## The good stuff: building in a difference based on a categorical variable

Using ifelse() we can generate different data based on a TRUE/FALSE condition. Let's say we have treated and untreated subjects. I'd like to generate Normal data that differs based on the treatment.

```{r}
trtmt <- sample(c("Treated","Untreated"), size = 20, replace = TRUE)
ifelse(trtmt=="Treated", yes = rnorm(20, 10, 1), no = rnorm(20, 20, 1))
```

Notice we have to make the length of the yes/no arguments the SAME LENGTH as
the trtmt=="Treated" logical vector! What happens if we use rnorm(n=1,...)?

```{r}
# What about more than two groups?
n <- 200
trtmt <- sample(LETTERS[1:6], size = n, replace = TRUE)

# Say we want to generate differnt Normal data for each group. One way is to do a for-loop with multiple if statements:

val <- numeric(n) # empty vector
for(i in seq_along(trtmt)){
  if(trtmt[i]=="A") val[i] <- rnorm(1, 10, 2)
  else if(trtmt[i]=="B") val[i] <- rnorm(1, 20, 4) 
  else if(trtmt[i]=="C") val[i] <- rnorm(1, 30, 6) 
  else if(trtmt[i]=="D") val[i] <- rnorm(1, 40, 8) 
  else if(trtmt[i]=="E") val[i] <- rnorm(1, 50, 10) 
  else val[i] <- rnorm(1, 60, 12) 
}
val
```

A more R-like way would be to take advantage of vectorized functions. First
create a data frame with one row for each group and the mean and standard
deviations we want to use to generate the data for that group.

```{r}
dat <- data.frame(g=LETTERS[1:6],mean=seq(10,60,10),sd=seq(2,12,2))

dat
```
dat is currently a petite little dataframe of 6 rows.

Now sample the row numbers (1 - 6) WITH replacement. We can use these to
randomly sample the data frame rows. 

ASIDE: Recall that we can repeatedly call a row or element using subsetting brackets. For example, call the first row of iris 5 times:

```{r}
iris[c(1,1,1,1,1),]
```

Let's exploit that to randomly sample with replacement our data frame of
groups:

```{r}
n <- 200
k <- sample(1:6, n, replace = TRUE)
dat <- dat[k,]

str(dat)

# Now generate our data for each group using ONE call to rnorm.
dat$vals <- rnorm(n, mean=dat$mean, sd=dat$sd)
head(dat)
```

This is pretty neat. We go from one little dataframe to a larger one in a few lines of code. Mean and SD can be varied by the class, "g" in this case. 

## A demonstration of the Central Limit Theorem

The Central Limit Theorem states that the sum of a large number of independent random variables will be approximately normally distributed almost regardless of their individual distributions. We can demonstrate this using various rxxx functions.

```{r}
# sum 6 values from 6 different distributions (sample size = 6)
n <- 1e4 # simulate 1000 times
clt  <- rexp(n, rate = 1) + rbinom(n,10,0.4) + rchisq(n,df = 6) + 
  rnorm(n, 12, 12) + rpois(n, lambda = 3) + rt(n, df = 7)
hist(clt, freq=FALSE)

# overlay a normal density curve
X <- seq(min(clt),max(clt),length = 500)       # x
Y <- dnorm(X, mean = mean(clt), sd = sd(clt))  # f(x) = dnorm
lines(X,Y,type = "l", col="blue") # plot (x,y) coordinates as a "blue" line ("l")

rm(X, Y, clt)
```

Let's unpack some of this:

```{r}
clt1  <- rexp(n, rate = 1)
hist(clt1, freq=FALSE)

clt2  <- rbinom(n,10,0.4)
hist(clt2, freq=FALSE)

clt3  <- rchisq(n,df = 6)
hist(clt3, freq=FALSE)

clt4  <-rnorm(n, 12, 12) 
hist(clt4, freq=FALSE)

clt5  <- rpois(n, lambda = 3)
hist(clt5, freq=FALSE)

clt6  <- rt(n, df = 7)
hist(clt6, freq=FALSE)

# All of this base R graphing is clunky and doesn't lend itself to modification as well as ggplot() figures.

library(tidyverse)
df <- data.frame(clt1, clt2, clt3, clt4, clt5, clt6)

df %>% ggplot(aes(clt3)) +
  geom_histogram(bins = 30)

df %>% 
  ggplot(aes(clt3)) +
  geom_density()
```

## Overlaying normal curve on histogram

The following solution was on StackOverflow at <https://stackoverflow.com/questions/6967664/ggplot2-histogram-with-normal-curve>

```{r}
set.seed(1)
df1 <- data.frame(PF = 10*rnorm(1000))
ggplot(df1, aes(x = PF)) + 
    geom_histogram(aes(y =..density..),
                   breaks = seq(-40, 40, by = 5), 
                   colour = "black", 
                   fill = "white") +
stat_function(fun = dnorm, args = list(mean = mean(df1$PF), sd = sd(df1$PF)), color = "blue") 
```

From the {ggplot2} help: "This stat makes it easy to superimpose a function on top of an existing plot. The function is called with a grid of evenly spaced values along the x axis, and the results are drawn (by default) with a line."

Note how stat_function() lends itself to quick addition: simply feed the correct fun and args to the function. 

Now accomplish this for clt3

```{r}
ggplot(df, aes(x = clt3)) + 
    geom_histogram(aes(y =..density..),
                   breaks = seq(0, 30, by = 1), 
                   colour = "black", 
                   fill = "white") +
stat_function(fun = dchisq, args = list(df = 6), color = "blue") + 
  labs(title = "chi-squared distribution")
```

Note how the args in dchisq includes only the df = 6. No mean needs to be calculated (as in dnorm). 

Now accomplish this for clt1

```{r}
ggplot(df, aes(x = clt1)) + 
    geom_histogram(aes(y =..density..),
                   breaks = seq(0, 8, by = .5), 
                   colour = "black", 
                   fill = "white") +
stat_function(fun = dexp, args = list(rate = 1), color = "blue") + 
  labs(title = "exponential distribution")
```


<!--chapter:end:10SimulatingData.Rmd-->

# Functions

```{r}
library(tidyverse)
```

From R4DS and the Solution book for R4DS:

```{r}
x <- c(1:10, rep(NA,3)) 

cV <- function(x){
  sd(x, na.rm = TRUE)/mean(x, na.rm = TRUE)
}

xVar <- function(x) {
  n <- length(x[!is.na(x)])
  m <- mean(x, na.rm = TRUE)
  sq_err <- (x - m)^2
  sum(sq_err, na.rm = TRUE) / (n - 1)
}
```


<!--chapter:end:11Functions.Rmd-->

# Relative versus absolute risk: display {#relativerisk}

The following is adopted from <https://www.r-bloggers.com/lying-with-statistics-one-beer-a-day-will-kill-you/>

**personograph** allows for cute displays of individuals in big groups

```{r}
library(personograph)
library(tidyverse)
```

Start with 2000 people. Some of them will have problems without alcohol exposure, about 18, in fact. The blogger choses 2000 people to start with because $0.7*18=1$

Note that this **doesn't** stratify for any other health problems, age, socio-economic status, etc. 

```{r}
n <- 2000
probl_wo_alc <- 18 / n

data <- list(first = probl_wo_alc, second = 1-probl_wo_alc)
personograph(data,  colors = list(first = "black", second = "#efefef"),
             fig.title = "18 of 2000 people with health problems",
             draw.legend = FALSE, n.icons = n, dimensions = c(20, 100), 
             plot.width = 0.97)
```

Now we illustrate the affect of 500 mL of alcohol per day. According to the Lancet article, the relative risk of serious illness following consumption of about 25 mL ethanol (500 mL beer at 5% ABV) increases by about 7%. 

```{r}
probl_w_alc <- 1 / n
 
data_2 <- list(first = probl_wo_alc, second = probl_w_alc, third = 1-(probl_wo_alc+probl_w_alc))
personograph(data_2, colors = list(first = "black", second = "red", third = "#efefef"),
             fig.title = "About 1 additional case with half a litre of beer per day",
             draw.legend = FALSE, n.icons = n, dimensions = c(20, 100),
             plot.width = 0.97)
```


<!--chapter:end:12RelativeRisk.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:13references.Rmd-->

