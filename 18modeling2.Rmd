# More modeling {#moremodeling}

```{r}
library(tidyverse)
library(caret)
```

## `glmnet` models

`glmnet` is an extension of glm models with built-in variable selection, which could be really nice for pituitary adenoma classification purposes.

`glmnet` helps deal with colinearity (correlation between predictors in a model) and small samples, both of which are relevant to my pituitary adenoma data. 

`glmnet` relies on two forms of regression:

  * Lasso regression, which penalizes the number of nonzero coefficients
  * Ridge regression, which penalizes the absolute magnitude of the coefficients
  * Both sorts of regression can be combined
  
`glmnet` attempst to find a simple model with few nonzero coefficients or small absolute magnitude of coefficients. 

`glmnet` pairs well with random forest models, since it often yields different results. 

### Parameters for tuning `glmnet` models

  * `alpha(0,1)`: pure ridge at zero, pure lasso at one
  * `lambda(0,infinity)`: size of the penalty
  
For a single value of alpha, `glmnet` fits all values of lambda simultaneously. This is "many models for the price of one." This is known as the "submodel trick."  

Zach Mayer uses a dataset that I can't seem to locate, so I'll switch the terms somewhat and hazard using iris and a three-class problem, which may be outside of the scope of this code. 

It may be that the example below is too wonky to salvage. See <https://neurospection.netlify.app/post/machine-learning-basics-with-caret/#a-classification-example> for some nice examples of machine learning from readily available datasets.

### An iris example of `glmnet`

```{r}
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = multiClassSummary,
  classProbs = TRUE,
  verboseIter = TRUE
)

set.seed(42)
model <- train(
  Species ~ .,
  iris,
  method = "glmnet",
  trControl = myControl
)

plot(model)
```

### Now using a tuning grid

```{r}
# Train glmnet with custom trainControl and tuning: model
model <- train(
  Species~., 
  iris,
  tuneGrid = expand.grid(
    alpha=0:1,
    lambda=seq(0.0001, 1, length=20)
  ),
  method = "glmnet",
  trControl = myControl
)

# Print model to console
model

# Print maximum ROC statistic
# max(model[["results"]][["ROC"]])
# this doesn't work with 3 target classes
```


## Imputation discussion

## Split target from predictors

This example shows how to generate some missing values, and to use imputation overcome these.

```{r mtcars-median-imputation-example, cached = TRUE}
data("mtcars")
set.seed(42)
mtcars[sample(1:nrow(mtcars), 10), "hp"] <- NA
Y <- mtcars$mpg
X <- mtcars[,2:4]

# model <- train(X,Y) #This fails, because of the NAs. Try imputation.

median_model <- train(
  X,
  Y,
  preProcess = "medianImpute"
)

print(median_model)

```

## Multiple preprocessing methods

Zach Mayer offers the following cheat sheet for preprocessing:

* Start with median imputation (if you're using it)
* Try KNN imputation if data NOT missing at random
* For linear models (lm, glm, glmnet) always center and scale
* Tree-based models (random forest, gbm) don't need much preprocessing

```{r mtcars-multiple-preprocessing-example, cached = TRUE}
data("mtcars")
set.seed(42)
mtcars[sample(1:nrow(mtcars), 10), "hp"] <- NA
Y <- mtcars$mpg
X <- mtcars[,2:4] # missing at random

set.seed(42)
model <- train(
  X,
  Y,
  method = "glm",
  preProcess = c("medianImpute", "center", "scale")
)

print(min(model$results$RMSE))

set.seed(42)
model <- train(
  X,
  Y,
  method = "glm",
  preProcess = c("medianImpute", "center", "scale", "pca")
)

print(min(model$results$RMSE)) # with pca applied
```

## Max Kuhn on reusing a trainControl

```{r traincontrol-reusing-example, cached = TRUE}
library(C50)
library(modeldata)
data(mlc_churn)
set.seed(1)
inTrainingSet <- createDataPartition(mlc_churn$churn, 
                                     p = 0.75,
                                     list = FALSE)
churnTrain <- mlc_churn[inTrainingSet,]
churnTest <- mlc_churn[-inTrainingSet,]
glimpse(churnTrain)
glimpse(churnTest)
table(churnTrain$churn/nrow(churnTrain))
```

