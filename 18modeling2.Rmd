# More modeling {#moremodeling}

```{r}
library(tidyverse)
library(caret)
```

## Split target from predictors

This example shows how to generate some missing values, and to use imputation overcome these.

```{r mtcars-median-imputation-example, cached = TRUE}
data("mtcars")
set.seed(42)
mtcars[sample(1:nrow(mtcars), 10), "hp"] <- NA
Y <- mtcars$mpg
X <- mtcars[,2:4]

# model <- train(X,Y) #This fails, because of the NAs. Try imputation.

median_model <- train(
  X,
  Y,
  preProcess = "medianImpute"
)

print(median_model)

```

## Multiple preprocessing methods

Zach Mayer offers the following cheat sheet for preprocessing:

* Start with median imputation (if you're using it)
* Try KNN imputation if data NOT missing at random
* For linear models (lm, glm, glmnet) always center and scale
* Tree-based models (random forest, gbm) don't need much preprocessing

```{r mtcars-multiple-preprocessing-example, cached = TRUE}
data("mtcars")
set.seed(42)
mtcars[sample(1:nrow(mtcars), 10), "hp"] <- NA
Y <- mtcars$mpg
X <- mtcars[,2:4] # missing at random

set.seed(42)
model <- train(
  X,
  Y,
  method = "glm",
  preProcess = c("medianImpute", "center", "scale")
)

print(min(model$results$RMSE))

set.seed(42)
model <- train(
  X,
  Y,
  method = "glm",
  preProcess = c("medianImpute", "center", "scale", "pca")
)

print(min(model$results$RMSE)) # with pca applied
```

## Max Kuhn on reusing a trainControl

```{r traincontrol-reusing-example, cached = TRUE}
library(C50)
library(modeldata)
data(mlc_churn)
set.seed(1)
inTrainingSet <- createDataPartition(mlc_churn$churn, 
                                     p = 0.75,
                                     list = FALSE)
churnTrain <- mlc_churn[inTrainingSet,]
churnTest <- mlc_churn[-inTrainingSet,]
glimpse(churnTrain)
glimpse(churnTest)
table(churnTrain$churn/nrow(churnTrain))
```

