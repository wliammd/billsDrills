# More modeling {#moremodeling}

```{r include=FALSE}
library(tidyverse)
library(rpart)
library(caret)
```

## **glmnet** and `patma` 

I'd like to explore Zach Meyer's teaching on **glmnet** models using my own data. That means dipping into `patmanDx.csv` and pulling out the relative data. 

```{r}
patma <- read_csv("data/patmanDx.csv")
df <- patma %>% select(SF1Median, Pit1Median, TPITMedian, PRLMedian, GHMedian, TSHMedian, LHMedian, FSHMedian, ACTHMedian, ASUMedian, GATA3Median, manDx) %>% na.omit()

glimpse(df)

df <- df %>% mutate(manDx = case_when(
  manDx == "NULL" ~ "Null",
  manDx %in% c("PLUR", "UNK") ~ "PlurUnk",
  TRUE ~ manDx
)) 

glimpse(df)

table(df$manDx)
```

That seems to give me what I'm looking for. Let's get to work:

```{r warning=FALSE}
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = defaultSummary,
  classProbs = TRUE, #Critical to have this, per Meyers.
  verboseIter = TRUE
)

set.seed(42)
model <- train(
  manDx ~ .,
  df,
  method = "glmnet",
  trControl = myControl
)

model

plot(model)
```

## Split target from predictors

This example shows how to generate some missing values, and to use imputation overcome these.

```{r}
data("mtcars")
set.seed(42)
mtcars[sample(1:nrow(mtcars), 10), "hp"] <- NA
Y <- mtcars$mpg
X <- mtcars[,2:4]

# model <- train(X,Y) #This fails, because of the NAs. Try imputation.

median_model <- train(
  X,
  Y,
  preProcess = "medianImpute"
)

print(median_model)

```

